{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\"\"\"\n",
    "Core dependencies and libraries for the ExplanationGAN implementation.\n",
    "\n",
    "This module imports required packages for:\n",
    "- File operations (os, shutil)\n",
    "- Numerical and data processing (numpy, pandas)\n",
    "- Deep learning with PyTorch (torch, nn, optim)\n",
    "- Data loading and batching (Dataset, DataLoader)\n",
    "- Image processing and transformations (transforms, PIL)\n",
    "- Pre-trained models (models)\n",
    "- Visualization (matplotlib)\n",
    "- Data preprocessing (LabelEncoder)\n",
    "\n",
    "Dependencies:\n",
    "    os: Operating system interfaces\n",
    "    numpy: Numerical computing\n",
    "    pandas: Data manipulation and analysis\n",
    "    torch: PyTorch deep learning framework\n",
    "    PIL: Python Imaging Library\n",
    "    sklearn: Machine learning utilities\n",
    "    matplotlib: Plotting library\n",
    "    shutil: High-level file operations\n",
    "    torchvision: Computer vision utilities\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HAM10000Dataset(Dataset):\n",
    "    \"\"\"A PyTorch Dataset class for the HAM10000 skin lesion dataset.\n",
    "    This class handles loading and preprocessing of the HAM10000 dataset, which contains\n",
    "    dermatoscopic images of various skin lesions across 7 diagnostic categories.\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing image metadata and labels.\n",
    "        img_dirs (list): List of directory paths containing the image files.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            Defaults to None.\n",
    "        device (str, optional): Device to store the tensors on ('cuda' or 'cpu').\n",
    "            Defaults to 'cuda'.\n",
    "    Attributes:\n",
    "        data (pandas.DataFrame): The loaded CSV data containing image metadata.\n",
    "        img_dirs (list): List of directories containing image files.\n",
    "        transform (callable): Transform to be applied to images.\n",
    "        device (str): Device for tensor storage.\n",
    "        label_encoder (LabelEncoder): Scikit-learn label encoder for categorical labels.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - image (Tensor): The processed image\n",
    "            - label (int): The encoded label\n",
    "    Raises:\n",
    "        FileNotFoundError: If an image file cannot be found in any of the provided directories.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, img_dirs, transform=None, device='cuda'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['encoded_label'] = self.label_encoder.fit_transform(self.data['dx'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                label = self.data.iloc[idx]['encoded_label']\n",
    "                return image, label\n",
    "        raise FileNotFoundError(f\"Image {img_name} not found in directories {self.img_dirs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedSLEBlock(nn.Module):\n",
    "    \"\"\"Enhanced Selective Local Enhancement (SLE) Block for feature modulation.\n",
    "    This module implements an enhanced version of the SLE block that separately processes\n",
    "    content and style information for more effective feature modulation. It combines\n",
    "    content-based attention with style-based modulation for improved feature enhancement.\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "        out_channels (int): Number of output channels.\n",
    "    Attributes:\n",
    "        global_pool: Global average pooling layer for content pathway.\n",
    "        content_fc1: First convolutional layer for content processing.\n",
    "        content_fc2: Second convolutional layer for content processing.\n",
    "        style_modulation: Sequential module for style feature processing.\n",
    "        gamma: Learnable parameter for style modulation scaling.\n",
    "        beta: Learnable parameter for style modulation bias.\n",
    "    Forward Args:\n",
    "        x (torch.Tensor): Input tensor for content pathway.\n",
    "        skip_x (torch.Tensor): Skip connection tensor for style pathway.\n",
    "    Returns:\n",
    "        torch.Tensor: Modulated feature map combining content and style information.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EnhancedSLEBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Content branch - adjusted channel dimensions\n",
    "        self.content_fc1 = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.content_fc2 = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        \n",
    "        # Style branch\n",
    "        self.style_modulation = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, skip_x):\n",
    "        # Content pathway\n",
    "        content = self.global_pool(x)\n",
    "        content = F.relu(self.content_fc1(content))\n",
    "        content = self.content_fc2(content)\n",
    "        content = torch.sigmoid(content)\n",
    "        \n",
    "        # Style pathway\n",
    "        style = self.style_modulation(skip_x)\n",
    "        \n",
    "        # Combine content and style\n",
    "        output = skip_x * content  # Content modulation\n",
    "        output = output + self.gamma * style + self.beta  # Style modulation\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedFASTGANGenerator(nn.Module):\n",
    "    \"\"\"Enhanced FASTGAN Generator module.\n",
    "    This module implements an enhanced version of the FASTGAN generator architecture with Skip-Layer\n",
    "    Excitation (SLE) blocks. It generates images through a series of transposed convolution operations\n",
    "    with batch normalization and ReLU activations.\n",
    "    Args:\n",
    "        latent_dim (int, optional): Dimension of the input latent vector. Defaults to 256.\n",
    "        ngf (int, optional): Number of generator filters in the first layer. Defaults to 32.\n",
    "        output_size (int, optional): Size of the output image. Defaults to 64.\n",
    "    Architecture:\n",
    "        - Initial block: Transposed convolution from latent space\n",
    "        - 4 upsampling layers with decreasing number of channels\n",
    "        - 2 Skip-Layer Excitation blocks between layers\n",
    "        - Final Tanh activation\n",
    "    Returns:\n",
    "        torch.Tensor: Generated image of shape (batch_size, 3, output_size, output_size)\n",
    "    Note:\n",
    "        The architecture follows a progressive growing pattern where the number of\n",
    "        channels is halved at each layer while the spatial dimensions are doubled.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=256, ngf=32, output_size=64):\n",
    "        super(EnhancedFASTGANGenerator, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.initial = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.sle1 = EnhancedSLEBlock(ngf * 8, ngf * 4)  # Changed input and output channels\n",
    "        self.sle2 = EnhancedSLEBlock(ngf * 4, ngf * 2)  # Changed input and output channels\n",
    "\n",
    "    def forward(self, z):\n",
    "        x0 = self.initial(z)\n",
    "        x1 = self.layer1(x0)\n",
    "        x1_sle = self.sle1(x0, x1)\n",
    "        x2 = self.layer2(x1_sle)\n",
    "        x2_sle = self.sle2(x1_sle, x2)\n",
    "        x3 = self.layer3(x2_sle)\n",
    "        x4 = self.layer4(x3)\n",
    "        return x4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedFASTGANDiscriminator(nn.Module):\n",
    "    \"\"\"Enhanced FASTGAN Discriminator Network.\n",
    "    This discriminator network is based on the FASTGAN architecture with additional\n",
    "    self-supervision capabilities through an integrated decoder. It consists of three\n",
    "    main components: a shared feature extractor, a discriminator head for real/fake\n",
    "    classification, and a decoder for self-supervised learning.\n",
    "    Args:\n",
    "        ndf (int, optional): Number of discriminator filters in the first conv layer.\n",
    "            Defaults to 64.\n",
    "        input_size (int, optional): Size of the input images. Defaults to 64.\n",
    "    Attributes:\n",
    "        features: Shared convolutional feature extractor.\n",
    "        discriminator: Classification head for real/fake prediction.\n",
    "        decoder: Decoder network for self-supervised reconstruction.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - validity (torch.Tensor): Prediction score for real/fake classification.\n",
    "            - reconstruction (torch.Tensor): Reconstructed image from the features.\n",
    "    Example:\n",
    "        >>> discriminator = EnhancedFASTGANDiscriminator(ndf=64, input_size=64)\n",
    "        >>> validity, reconstruction = discriminator(images)\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf=64, input_size=64):\n",
    "        super(EnhancedFASTGANDiscriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # Discriminator head\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(ndf * 8, 1, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Decoder for self-supervision\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ndf * 8, ndf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ndf * 4, ndf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ndf * 2, ndf, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ndf, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        validity = self.discriminator(features)\n",
    "        reconstruction = self.decoder(features)\n",
    "        return validity, reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SyntheticImageClassifier:\n",
    "    \"\"\"A class that classifies synthetic images using two different CNN architectures.\n",
    "    This classifier uses both EfficientNetV2 and ShuffleNetV2 pre-trained models to classify images.\n",
    "    Images are classified by both models and predictions are compared to ensure agreement.\n",
    "    Attributes:\n",
    "        device (str): The device to run the models on ('cuda' or 'cpu')\n",
    "        efficientnet (torch.nn.Module): EfficientNetV2 model instance\n",
    "        shufflenet (torch.nn.Module): ShuffleNetV2 model instance\n",
    "        transform (torchvision.transforms): Transformation pipeline for input images\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes for classification\n",
    "        device (str, optional): Device to run the models on. Defaults to 'cuda'\n",
    "    Methods:\n",
    "        classify_synthetic_images(synthetic_images): Classifies synthetic images using both models\n",
    "            and returns a mask indicating where both models agree on the classification\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, device='cuda'):\n",
    "        self.device = device\n",
    "        \n",
    "        # EfficientNetV2\n",
    "        self.efficientnet = models.efficientnet_v2_s(pretrained=True)\n",
    "        self.efficientnet.classifier[1] = nn.Linear(self.efficientnet.classifier[1].in_features, num_classes)\n",
    "        self.efficientnet = self.efficientnet.to(device)\n",
    "        \n",
    "        # ShuffleNetV2\n",
    "        self.shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "        self.shufflenet.fc = nn.Linear(self.shufflenet.fc.in_features, num_classes)\n",
    "        self.shufflenet = self.shufflenet.to(device)\n",
    "        \n",
    "        # Transformation for input images\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def classify_synthetic_images(self, synthetic_images):\n",
    "        resized_images = F.interpolate(synthetic_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        normalized_images = (resized_images - resized_images.min()) / (resized_images.max() - resized_images.min())\n",
    "        normalized_images = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(normalized_images)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            efficientnet_preds = self.efficientnet(normalized_images)\n",
    "            shufflenet_preds = self.shufflenet(normalized_images)\n",
    "        \n",
    "        efficientnet_classes = torch.argmax(efficientnet_preds, dim=1)\n",
    "        shufflenet_classes = torch.argmax(shufflenet_preds, dim=1)\n",
    "        \n",
    "        agreed_classification_mask = (efficientnet_classes == shufflenet_classes)\n",
    "        \n",
    "        return agreed_classification_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Performs a single training step for both Generator and Discriminator in a GAN with enhanced training features.\n",
    "                    This function implements one training iteration including:\n",
    "                    - Training the discriminator with real and fake images using hinge loss\n",
    "                    - Self-supervision through reconstruction loss\n",
    "                    - Training the generator with hinge loss\n",
    "                    Args:\n",
    "                        real_imgs (torch.Tensor): Batch of real images\n",
    "                        generator (nn.Module): The generator model\n",
    "                        discriminator (nn.Module): The discriminator model\n",
    "                        g_optimizer (torch.optim.Optimizer): Optimizer for the generator\n",
    "                        d_optimizer (torch.optim.Optimizer): Optimizer for the discriminator\n",
    "                        device (torch.device): Device to run the computation on\n",
    "                        lambda_rec (float, optional): Weight for reconstruction loss. Defaults to 10.0\n",
    "                    Returns:\n",
    "                        tuple:                   \n",
    "                            - dict: Dictionary containing various loss values:\n",
    "                                - 'd_loss': Total discriminator loss\n",
    "                                - 'd_loss_adv': Adversarial component of discriminator loss\n",
    "                                - 'd_loss_rec': Reconstruction component of discriminator loss\n",
    "                                - 'g_loss': Generator loss\n",
    "                            - torch.Tensor: Batch of generated fake images\n",
    "                    \"\"\"\n",
    "def enhanced_train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, \n",
    "                       device, lambda_rec=10.0):\n",
    "    batch_size = real_imgs.size(0)\n",
    "    \n",
    "    # Train Discriminator\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    # Real images\n",
    "    real_validity, real_reconstruction = discriminator(real_imgs)\n",
    "    \n",
    "    # Generate fake images\n",
    "    z = torch.randn(batch_size, 256, 1, 1, device=device)\n",
    "    fake_imgs = generator(z)\n",
    "    fake_validity, _ = discriminator(fake_imgs.detach())\n",
    "    \n",
    "    # Hinge loss\n",
    "    d_loss_real = torch.mean(F.relu(1.0 - real_validity))\n",
    "    d_loss_fake = torch.mean(F.relu(1.0 + fake_validity))\n",
    "    d_loss_adv = d_loss_real + d_loss_fake\n",
    "    \n",
    "    # Reconstruction loss for self-supervision\n",
    "    d_loss_rec = F.mse_loss(real_reconstruction, real_imgs)\n",
    "    \n",
    "    # Total discriminator loss\n",
    "    d_loss = d_loss_adv + lambda_rec * d_loss_rec\n",
    "    \n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # Train Generator\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    fake_validity, _ = discriminator(fake_imgs)\n",
    "    g_loss = -torch.mean(fake_validity)  # Hinge loss for generator\n",
    "    \n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return {\n",
    "        'd_loss': d_loss.item(),\n",
    "        'd_loss_adv': d_loss_adv.item(),\n",
    "        'd_loss_rec': d_loss_rec.item(),\n",
    "        'g_loss': g_loss.item()\n",
    "    }, fake_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_data_distribution_comparison(original_csv, synthetic_images_dir):\n",
    "    \"\"\"\n",
    "    Plots and compares the distribution of samples between original and synthetic datasets.\n",
    "    This function creates a bar plot comparing the number of samples per class in the\n",
    "    original HAM10000 dataset versus the synthetically generated images. The comparison\n",
    "    is visualized using a side-by-side bar chart and saved as a PNG file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_csv : str\n",
    "        Path to the CSV file containing metadata of the original HAM10000 dataset.\n",
    "        Must include a 'dx' column with class labels.\n",
    "    synthetic_images_dir : str\n",
    "        Path to the directory containing synthetic images organized in subdirectories\n",
    "        by class name.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves the generated plot as 'dataset_distribution_comparison.png' in the\n",
    "        current directory.\n",
    "    Notes\n",
    "    -----\n",
    "    - The function expects synthetic images to be stored in subdirectories named\n",
    "      after their respective classes\n",
    "    - Supported image formats are .png and .jpg\n",
    "    - Plot includes numerical annotations above each bar showing exact counts\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(original_csv)\n",
    "    original_class_counts = metadata['dx'].value_counts()\n",
    "    synthetic_class_counts = {}\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(metadata['dx'])\n",
    "    \n",
    "    for class_name in label_encoder.classes_:\n",
    "        class_dir = os.path.join(synthetic_images_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            synthetic_class_counts[class_name] = len([f for f in os.listdir(class_dir) \n",
    "                                                    if f.endswith(('.png', '.jpg'))])\n",
    "        else:\n",
    "            synthetic_class_counts[class_name] = 0\n",
    "    \n",
    "    synthetic_class_counts = pd.Series(synthetic_class_counts)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    x = np.arange(len(original_class_counts))\n",
    "    width = 0.4\n",
    "    \n",
    "    plt.bar(x - width/2, original_class_counts.values, width, label='Original Dataset', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width/2, synthetic_class_counts.values, width, label='Synthetic Images', color='orange', alpha=0.7)\n",
    "    \n",
    "    plt.title('Comparison of Original HAM10000 Dataset and Synthetic Images', fontsize=16)\n",
    "    plt.xlabel('Skin Lesion Type', fontsize=14)\n",
    "    plt.ylabel('Number of Samples', fontsize=14)\n",
    "    plt.xticks(x, original_class_counts.index, rotation=90, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    for i, (orig, synth) in enumerate(zip(original_class_counts.values, synthetic_class_counts.values)):\n",
    "        plt.text(i - width/2, orig + 50, str(int(orig)), ha='center', va='bottom', fontsize=8)\n",
    "        plt.text(i + width/2, synth + 50, str(int(synth)), ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dataset_distribution_comparison.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def copy_original_images_by_class(csv_file, img_dirs, output_base_dir='synthetic_images'):\n",
    "    \"\"\"\n",
    "    Copies and organizes image files into class-specific directories based on metadata from a CSV file.\n",
    "    This function reads image metadata from a CSV file and copies images from source directories\n",
    "    to a structured output directory, organizing them by their class (diagnosis type).\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing image metadata with 'dx' (diagnosis)\n",
    "            and 'image_id' columns.\n",
    "        img_dirs (list): List of directory paths containing the source images.\n",
    "        output_base_dir (str, optional): Base directory where images will be copied and organized.\n",
    "            Defaults to 'synthetic_images'.\n",
    "    Each class gets its own subdirectory under output_base_dir, and images are copied\n",
    "    maintaining their original filenames. The function ensures each image is copied only once,\n",
    "    even if it appears in multiple source directories.\n",
    "    Prints:\n",
    "        - Confirmation message when copying is complete\n",
    "        - Total number of unique images copied\n",
    "    Note:\n",
    "        - Creates directories if they don't exist\n",
    "        - Expects images to be in .jpg format\n",
    "        - Uses image_id from CSV with '.jpg' extension to find source files\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(csv_file)\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    copied_images = set()\n",
    "    \n",
    "    for class_name in metadata['dx'].unique():\n",
    "        class_output_dir = os.path.join(output_base_dir, class_name)\n",
    "        os.makedirs(class_output_dir, exist_ok=True)\n",
    "        \n",
    "        class_metadata = metadata[metadata['dx'] == class_name]\n",
    "        \n",
    "        for _, row in class_metadata.iterrows():\n",
    "            img_filename = row['image_id'] + '.jpg'\n",
    "            \n",
    "            for img_dir in img_dirs:\n",
    "                img_path = os.path.join(img_dir, img_filename)\n",
    "                \n",
    "                if os.path.exists(img_path):\n",
    "                    dest_path = os.path.join(class_output_dir, img_filename)\n",
    "                    \n",
    "                    if img_path not in copied_images:\n",
    "                        shutil.copy2(img_path, dest_path)\n",
    "                        copied_images.add(img_path)\n",
    "                    break\n",
    "    \n",
    "    print(f\"Original images copied to {output_base_dir}\")\n",
    "    print(f\"Total unique images copied: {len(copied_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                        Trains an enhanced FastGAN model with both generator and discriminator networks.\n",
    "                        Parameters:\n",
    "                            generator (nn.Module): Generator neural network model\n",
    "                            discriminator (nn.Module): Discriminator neural network model \n",
    "                            dataloader (DataLoader): DataLoader containing the training data\n",
    "                            num_epochs (int): Number of training epochs\n",
    "                            device (str): Device to run training on ('cuda' or 'cpu'), defaults to 'cuda'\n",
    "                            lambda_rec (float): Weight for reconstruction loss, defaults to 10.0\n",
    "                            save_interval (int): Number of batches between saving sample images, defaults to 100\n",
    "                        Returns:\n",
    "                            tuple: Trained (generator, discriminator) models\n",
    "                        The function performs the following:\n",
    "                        - Sets up Adam optimizers for both networks\n",
    "                        - Creates a directory for saving training progress images\n",
    "                        - For each epoch:\n",
    "                            - Processes batches from the dataloader\n",
    "                            - Performs training step using enhanced_train_step()\n",
    "                            - Prints loss metrics every 100 batches\n",
    "                            - Saves sample generated images at specified intervals\n",
    "                        \"\"\"\n",
    "def train_enhanced_fastgan(generator, discriminator, dataloader, num_epochs, device='cuda', lambda_rec=10.0, save_interval=100):\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    os.makedirs('training_progress', exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_imgs, _) in enumerate(dataloader):\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            \n",
    "            losses, fake_imgs = enhanced_train_step(\n",
    "                real_imgs, generator, discriminator,\n",
    "                g_optimizer, d_optimizer, device, lambda_rec\n",
    "            )\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch [{epoch}/{num_epochs}], Batch [{i}], '\n",
    "                      f'D_loss: {losses[\"d_loss\"]:.4f}, '\n",
    "                      f'D_adv: {losses[\"d_loss_adv\"]:.4f}, '\n",
    "                      f'D_rec: {losses[\"d_loss_rec\"]:.4f}, '\n",
    "                      f'G_loss: {losses[\"g_loss\"]:.4f}')\n",
    "                \n",
    "                # Save sample images\n",
    "                if i % save_interval == 0:\n",
    "                    save_image(fake_imgs[:16] * 0.5 + 0.5,\n",
    "                             f'training_progress/epoch_{epoch}_batch_{i}.png',\n",
    "                             nrow=4, normalize=False)\n",
    "    \n",
    "    return generator, discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "                            Generates and saves synthetic images using a trained generator and classifier.\n",
    "                            This function generates synthetic images for each class using a trained generator,\n",
    "                            filters them using a classifier, and saves the valid images to specified directories.\n",
    "                            Args:\n",
    "                                generator (torch.nn.Module): Trained generator model\n",
    "                                classifier (torch.nn.Module): Trained classifier model used to filter generated images\n",
    "                                num_classes (int): Number of classes to generate images for\n",
    "                                num_images_per_class (int): Number of images to generate per class\n",
    "                                device (str, optional): Device to run the models on. Defaults to 'cuda'\n",
    "                                batch_size (int, optional): Batch size for image generation. Defaults to 64\n",
    "                                output_dir (str, optional): Base directory to save generated images. Defaults to 'synthetic_images'\n",
    "                            Returns:\n",
    "                                None\n",
    "                            Notes:\n",
    "                                - Creates directories for each class under output_dir\n",
    "                                - Generated images are saved as PNG files\n",
    "                                - Images are normalized from [-1,1] to [0,1] range before saving\n",
    "                                - Progress is printed during generation\n",
    "                            \"\"\"\n",
    "def generate_synthetic_images(generator, classifier, num_classes, num_images_per_class,device='cuda', batch_size=64, output_dir='synthetic_images'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for class_idx in range(num_classes):\n",
    "            class_dir = os.path.join(output_dir, f'class_{class_idx}')\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            \n",
    "            num_generated = 0\n",
    "            while num_generated < num_images_per_class:\n",
    "                # Generate images\n",
    "                z = torch.randn(batch_size, 256, 1, 1, device=device)\n",
    "                fake_imgs = generator(z)\n",
    "                \n",
    "                # Filter images using classifier\n",
    "                valid_mask = classifier.classify_synthetic_images(fake_imgs)\n",
    "                valid_images = fake_imgs[valid_mask]\n",
    "                \n",
    "                # Save valid images\n",
    "                for idx, img in enumerate(valid_images):\n",
    "                    if num_generated >= num_images_per_class:\n",
    "                        break\n",
    "                    save_image(img * 0.5 + 0.5,\n",
    "                             os.path.join(class_dir, f'synthetic_{num_generated}.png'))\n",
    "                    num_generated += 1\n",
    "                \n",
    "                print(f'Class {class_idx}: Generated {num_generated}/{num_images_per_class} images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for training and evaluating an Enhanced FASTGAN model on the HAM10000 skin cancer dataset.\n",
    "    This function performs the following key operations:\n",
    "    1. Sets up the device (CPU/GPU) and random seeds for reproducibility\n",
    "    2. Loads and preprocesses the HAM10000 dataset\n",
    "    3. Initializes the Generator, Discriminator, and Classifier models\n",
    "    4. Trains the GAN model\n",
    "    5. Generates synthetic images for each class\n",
    "    6. Plots distribution comparison between real and synthetic data\n",
    "    The function uses an Enhanced FASTGAN architecture specifically adapted for\n",
    "    generating synthetic skin cancer images across multiple classes.\n",
    "    Dependencies:\n",
    "        - torch\n",
    "        - numpy\n",
    "        - torchvision\n",
    "        - Custom modules (HAM10000Dataset, EnhancedFASTGANGenerator, \n",
    "          EnhancedFASTGANDiscriminator, SyntheticImageClassifier)\n",
    "    Returns:\n",
    "        None\n",
    "    Example:\n",
    "        >>> main()\n",
    "        Using device: cuda\n",
    "        Number of classes: 7\n",
    "        Starting training...\n",
    "        Training and generation complete!\n",
    "    \"\"\"\n",
    "    # Set device and random seeds\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Dataset parameters\n",
    "    csv_file = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n",
    "    img_dirs = ['/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2']\n",
    "    \n",
    "    # Data preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Initialize dataset and dataloader\n",
    "    dataset = HAM10000Dataset(csv_file, img_dirs, transform=transform, device=device)\n",
    "    num_classes = len(dataset.label_encoder.classes_)\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(\"Class labels:\", dataset.label_encoder.classes_)\n",
    "    \n",
    "    batch_size = 64\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # Initialize models\n",
    "    generator = EnhancedFASTGANGenerator(latent_dim=256, output_size=64).to(device)\n",
    "    discriminator = EnhancedFASTGANDiscriminator(input_size=64).to(device)\n",
    "    classifier = SyntheticImageClassifier(num_classes=num_classes, device=device)\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs = 100\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Train the model\n",
    "    generator, discriminator = train_enhanced_fastgan(\n",
    "        generator, discriminator, dataloader, \n",
    "        num_epochs=num_epochs, device=device\n",
    "    )\n",
    "    \n",
    "    # Generate synthetic images for each class\n",
    "    print(\"Generating synthetic images...\")\n",
    "    generate_synthetic_images(\n",
    "        generator, classifier, \n",
    "        num_classes=num_classes,\n",
    "        num_images_per_class=1000,  # Adjust as needed\n",
    "        device=device,\n",
    "        output_dir='synthetic_images'\n",
    "    )\n",
    "    \n",
    "    # Plot distribution comparison\n",
    "    plot_data_distribution_comparison(csv_file, 'synthetic_images')\n",
    "    \n",
    "    print(\"Training and generation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Architecture Components:\n",
    "- Generator: Uses single convolution layer per resolution with restricted channels\n",
    "- Skip-Layer Excitation (SLE) module for gradient flow enhancement\n",
    "- Self-supervised discriminator with small decoders\n",
    "- Two CNN classifiers (EfficientNetV2 and ShuffleNetV2) for synthetic image filtering\n",
    "\n",
    "2. Key Features:\n",
    "- SLE module: \n",
    "  - Implements channel-wise multiplications\n",
    "  - Creates skip-connections between distant resolutions\n",
    "  - Handles content/style attribute disentanglement\n",
    "\n",
    "3. Training Process:\n",
    "- Uses hinge version of adversarial loss\n",
    "- Iterative training between discriminator and generator\n",
    "- Discriminator regularization through auto-encoding\n",
    "- Two-stage filtering:\n",
    "  1. Generate synthetic images from original medical data\n",
    "  2. Filter generations through dual CNN validation\n",
    "\n",
    "4. Data Flow:\n",
    "```\n",
    "Original Images → FASTGAN Generator → Synthetic Images → Dual CNN Filtering → Filtered Dataset + Original Images → Few-shot Classifier\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Enhanced FASTGAN Architecture for Synthetic Medical Image Generation \n",
    "\n",
    "Abstract:\n",
    "This implementation presents an enhanced FASTGAN (Fast and Adaptive Synthetic Training GAN) architecture specifically designed for generating synthetic medical images from the HAM10000 skin lesion dataset. The system incorporates a novel dual-classifier validation mechanism and self-supervised learning components to ensure high-quality, class-consistent synthetic image generation.\n",
    "\n",
    "1. Architecture Overview\n",
    "\n",
    "The system consists of three primary components:\n",
    "\n",
    "a) Generator Architecture\n",
    "- Implements an enhanced FASTGAN generator with a latent dimension of 256\n",
    "- Utilizes transposed convolutions for upsampling\n",
    "- Incorporates Enhanced Skip-Layer Excitation (SLE) blocks for improved feature propagation\n",
    "- Employs batch normalization and ReLU activation functions\n",
    "\n",
    "b) Discriminator Architecture\n",
    "- Features a shared feature extractor backbone\n",
    "- Implements dual-head architecture:\n",
    "  * Classification head for real/fake discrimination\n",
    "  * Decoder head for self-supervised reconstruction\n",
    "- Utilizes hinge loss for adversarial training\n",
    "\n",
    "c) Synthetic Image Classifier\n",
    "- Employs ensemble approach with EfficientNetV2 and ShuffleNetV2\n",
    "- Provides validation through classification agreement\n",
    "- Performs image normalization and size standardization\n",
    "\n",
    "2. Dataset Implementation\n",
    "\n",
    "The HAM10000Dataset class implements a custom PyTorch Dataset with the following features:\n",
    "- Supports multiple image directory sources\n",
    "- Performs label encoding for categorical diagnoses\n",
    "- Implements on-the-fly image transformation\n",
    "- Handles missing image scenarios with appropriate error handling\n",
    "\n",
    "3. Training Methodology\n",
    "\n",
    "The training process incorporates several key components:\n",
    "\n",
    "a) Enhanced Training Step\n",
    "```python\n",
    "d_loss = d_loss_adv + lambda_rec * d_loss_rec\n",
    "```\n",
    "- Combines adversarial and reconstruction losses for the discriminator\n",
    "- Implements hinge loss for improved stability\n",
    "- Uses self-supervised reconstruction for feature learning\n",
    "\n",
    "b) Generator Training\n",
    "- Utilizes adversarial hinge loss for generator optimization\n",
    "- Implements adaptive learning rates via Adam optimizer\n",
    "- Includes periodic progress visualization\n",
    "\n",
    "4. Synthetic Image Generation Process\n",
    "\n",
    "The generation pipeline includes:\n",
    "- Batch-wise image generation\n",
    "- Multi-classifier validation filtering\n",
    "- Class-specific output organization\n",
    "- Distribution matching with original dataset\n",
    "\n",
    "5. Quality Control Mechanisms\n",
    "\n",
    "Several quality control measures are implemented:\n",
    "- Dual-model classification verification\n",
    "- Image normalization and standardization\n",
    "- Distribution comparison visualization\n",
    "- Automated storage and organization of synthetic images\n",
    "\n",
    "6. Key Innovations\n",
    "\n",
    "The implementation includes several novel components:\n",
    "- Enhanced SLE blocks for improved feature propagation\n",
    "- Dual-classifier validation mechanism\n",
    "- Self-supervised reconstruction in discriminator\n",
    "- Distribution-aware generation process\n",
    "\n",
    "7. Experimental Setup\n",
    "\n",
    "The main function orchestrates the following workflow:\n",
    "- Device and seed initialization\n",
    "- Dataset preparation and loading\n",
    "- Model initialization and training\n",
    "- Synthetic image generation and validation\n",
    "- Distribution analysis and visualization\n",
    "\n",
    "8. Performance Monitoring\n",
    "\n",
    "The system includes comprehensive monitoring:\n",
    "- Loss tracking for generator and discriminator\n",
    "- Image quality assessment through periodic sampling\n",
    "- Class distribution visualization\n",
    "- Generation success rate tracking\n",
    "\n",
    "Conclusion:\n",
    "This implementation provides a robust framework for generating synthetic medical images with built-in validation mechanisms. The architecture's emphasis on quality control and class consistency makes it particularly suitable for medical image synthesis applications.\n",
    "\n",
    "Future Work:\n",
    "Potential improvements could include:\n",
    "- Implementation of additional validation metrics\n",
    "- Integration of domain-specific medical image quality assessments\n",
    "- Extension to support multi-modal medical imaging data\n",
    "- Enhancement of the classification validation mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It integrates FASTGAN to generate additional medical images from available training data. Employing two CNNs, the approach identifes syn\u0002thetic images prone to misclassifcation, removing them from the dataset. This system\u0002atic elimination aims to improve the accuracy and reliability of the classifcation task, showcasing the potential of FSL techniques in medical image classifcation.\n",
    "This study employs FASTGAN [4] for generating synthetic images in the training data\u0002set. FASTGAN employs a single convolution layer per resolution with restricted channels, \n",
    "yielding a smaller and quicker-to-train model. Moreover, it integrates the Skip-Layer Exci\u0002tation (SLE) module, enhancing gradient fow by modifying skip-connections for efcient \n",
    "gradient signal propagation across resolutions. SLE utilizes channel-wise multiplications \n",
    "and extends skip-connections between distant resolutions to improve gradient fow with\u0002out notable computational overhead. While resembling the Squeeze-and-Excitation mod\u0002ule, SLE operates between distant feature-maps, aiding gradient fow and channel-wise \n",
    "feature recalibration essential for disentangling content and style attributes in generated \n",
    "images. FASTGAN also incorporates a self-supervised discriminator trained with small \n",
    "decoders, enhancing image feature extraction via auto-encoding. This approach improves \n",
    "the discriminator’s capacity to extract comprehensive representations from inputs, thereby \n",
    "enhancing model robustness and synthesis quality. This approach maintains a pure GAN \n",
    "framework, using auto-encoding solely for discriminator regularization. Additionally, the \n",
    "method employs the hinge version of the adversarial loss for iterative training of the dis\u0002criminator and generator. Overall, these methodological advancements contribute to more \n",
    "efcient and efective GAN training, which has implications for various image synthesis \n",
    "tasks.\n",
    "Figure 3 illustrates the integration of the GAN with the few-shot classifer network. This \n",
    "paper initially undertakes the generation of synthetic medical images by using original \n",
    "images. Considering that some synthetic medical images may not be completely realistic \n",
    "or accurately categorized, the possibility of inaccuracies necessitates manual examination \n",
    "by domain experts for correction. To circumvent the need for expert intervention, the study \n",
    "employs two CNNs, namely EfcientNetV2 and ShufeNetV2, trained on the original \n",
    "medical images for the accurate classifcation of synthetic images. Consequently, only the synthetic images correctly classifed by these CNNs are utilized. These mutually classifed \n",
    "images, along with the original medical images, form the training set for the Few-Shot \n",
    "Learning (FSL) classifer. This approach ensures the selection of high-quality synthetic \n",
    "images for inclusion in the training dataset, thereby enhancing the performance and reli\u0002ability of the FSL classifer. By leveraging the capabilities of pretrained CNNs, the study \n",
    "establishes a mechanism to automate the classifcation process. It mitigates the need for \n",
    "manual intervention and facilitates the seamless integration of synthetic images into the \n",
    "training pipeline."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
