{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\"\"\"\n",
    "Initial imports for a GAN (Generative Adversarial Network) implementation.\n",
    "\n",
    "This module imports necessary libraries and frameworks for implementing a GAN:\n",
    "\n",
    "Core Libraries:\n",
    "    - os: Operating system interface\n",
    "    - numpy: Numerical computing\n",
    "    - pandas: Data manipulation and analysis\n",
    "\n",
    "PyTorch Components:\n",
    "    - torch: Main PyTorch library\n",
    "    - torch.nn: Neural network modules\n",
    "    - torch.nn.functional: Neural network functions\n",
    "    - torch.optim: Optimization algorithms\n",
    "    - torch.utils.data: Data loading utilities\n",
    "\n",
    "Computer Vision Tools:\n",
    "    - torchvision.transforms: Image transformation utilities\n",
    "    - torchvision.models: Pre-trained models\n",
    "    - torchvision.utils: Image handling utilities\n",
    "    - PIL: Python Imaging Library\n",
    "\n",
    "Data Processing & Visualization:\n",
    "    - sklearn.preprocessing: Data preprocessing tools\n",
    "    - matplotlib.pyplot: Plotting library\n",
    "\n",
    "The module sets up the foundation for building and training a GAN model\n",
    "with image processing capabilities.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAM10000Dataset(Dataset):\n",
    "    \"\"\"HAM10000Dataset class for loading and preprocessing the HAM10000 skin lesion dataset.\n",
    "    This class inherits from torch.utils.data.Dataset and provides functionality to load\n",
    "    and prepare the HAM10000 dataset for training deep learning models.\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing image metadata and labels.\n",
    "        img_dirs (list): List of directories containing the image files.\n",
    "        transform (callable, optional): Optional transform to be applied on an image.\n",
    "            Defaults to None.\n",
    "        device (str, optional): Device to load the data to ('cuda' or 'cpu').\n",
    "            Defaults to 'cuda'.\n",
    "    Attributes:\n",
    "        data (DataFrame): Pandas DataFrame containing the dataset metadata.\n",
    "        img_dirs (list): List of image directory paths.\n",
    "        transform (callable): Transform to be applied to images.\n",
    "        device (str): Device for data loading.\n",
    "        label_encoder (LabelEncoder): Scikit-learn label encoder for class labels.\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - image (Tensor): The processed image\n",
    "            - label (int): The encoded label\n",
    "    Raises:\n",
    "        FileNotFoundError: If an image file is not found in any of the provided directories.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, img_dirs, transform=None, device='cuda'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['encoded_label'] = self.label_encoder.fit_transform(self.data['dx'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                label = self.data.iloc[idx]['encoded_label']\n",
    "                return image, label\n",
    "        raise FileNotFoundError(f\"Image {img_name} not found in directories {self.img_dirs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution_comparison(original_csv, synthetic_images_dir):\n",
    "    \"\"\"\n",
    "    Compare the data distribution of the original HAM10000 dataset \n",
    "    with the distribution after adding synthetic images\n",
    "    \n",
    "    Args:\n",
    "        original_csv (str): Path to the original metadata CSV file\n",
    "        synthetic_images_dir (str): Path to the directory containing synthetic images\n",
    "\n",
    "    \n",
    "    Compare the data distribution between the original HAM10000 dataset and synthetic images.\n",
    "    This function creates a side-by-side bar plot comparing the class distribution in the original\n",
    "    HAM10000 dataset with the distribution of synthetically generated images. It also prints\n",
    "    detailed statistics about the distribution and augmentation ratios.\n",
    "        original_csv (str): Path to the CSV file containing original HAM10000 metadata.\n",
    "        synthetic_images_dir (str): Path to the directory containing synthetic images organized\n",
    "            in subdirectories by class.\n",
    "    Returns:\n",
    "        None. Displays a plot and prints distribution statistics to console.\n",
    "    The function:\n",
    "    - Creates a bar plot comparing original vs synthetic image counts per class\n",
    "    - Adds count labels on top of each bar\n",
    "    - Prints detailed distribution percentages for both original and synthetic data\n",
    "    - Calculates and displays augmentation ratios per class\n",
    "    Example:\n",
    "        >>> plot_data_distribution_comparison('metadata.csv', 'synthetic_images/')\n",
    "    Compare the data distribution of the original HAM10000 dataset \n",
    "    with the distribution after adding synthetic images\n",
    "    \n",
    "    Args:\n",
    "        original_csv (str): Path to the original metadata CSV file\n",
    "        synthetic_images_dir (str): Path to the directory containing synthetic images\n",
    "    \n",
    "    \"\"\"\n",
    "    # Read the original metadata\n",
    "    metadata = pd.read_csv(original_csv)\n",
    "    \n",
    "    # Count original class distribution\n",
    "    original_class_counts = metadata['dx'].value_counts()\n",
    "    \n",
    "    # Prepare synthetic image class counts\n",
    "    synthetic_class_counts = {}\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(metadata['dx'])\n",
    "    \n",
    "    # Count synthetic images per class\n",
    "    for class_name in label_encoder.classes_:\n",
    "        class_dir = os.path.join(synthetic_images_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            synthetic_class_counts[class_name] = len([f for f in os.listdir(class_dir) \n",
    "                                                      if f.endswith(('.png', '.jpg'))])\n",
    "        else:\n",
    "            synthetic_class_counts[class_name] = 0\n",
    "    \n",
    "    # Convert to Series for consistent plotting\n",
    "    synthetic_class_counts = pd.Series(synthetic_class_counts)\n",
    "    \n",
    "    # Prepare the plot\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create a side-by-side bar plot\n",
    "    x = np.arange(len(original_class_counts))\n",
    "    width = 0.4\n",
    "    \n",
    "    plt.bar(x - width/2, original_class_counts.values, width, label='Original Dataset', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width/2, synthetic_class_counts.values, width, label='Synthetic Images', color='orange', alpha=0.7)\n",
    "    \n",
    "    plt.title('Comparison of Original HAM10000 Dataset and Synthetic Images', fontsize=16)\n",
    "    plt.xlabel('Skin Lesion Type', fontsize=14)\n",
    "    plt.ylabel('Number of Samples', fontsize=14)\n",
    "    plt.xticks(x, original_class_counts.index, rotation=90, ha='right')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add count labels on top of each bar\n",
    "    for i, (orig, synth) in enumerate(zip(original_class_counts.values, synthetic_class_counts.values)):\n",
    "        plt.text(i - width/2, orig + 50, str(int(orig)), ha='center', va='bottom', fontsize=8)\n",
    "        plt.text(i + width/2, synth + 50, str(int(synth)), ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('dataset_distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show('dataset_distribution_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Print detailed comparison\n",
    "    print(\"\\nOriginal Dataset Distribution:\")\n",
    "    total_original = len(metadata)\n",
    "    for cls, count in original_class_counts.items():\n",
    "        percentage = (count / total_original) * 100\n",
    "        print(f\"{cls}: {count} samples ({percentage:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nSynthetic Images Distribution:\")\n",
    "    total_synthetic = synthetic_class_counts.sum()\n",
    "    for cls, count in synthetic_class_counts.items():\n",
    "        percentage = (count / total_synthetic) * 100 if total_synthetic > 0 else 0\n",
    "        print(f\"{cls}: {count} synthetic images ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Calculate and print augmentation ratio\n",
    "    print(\"\\nAugmentation Ratio:\")\n",
    "    for cls in original_class_counts.index:\n",
    "        orig_count = original_class_counts.get(cls, 0)\n",
    "        synth_count = synthetic_class_counts.get(cls, 0)\n",
    "        augmentation_ratio = synth_count / orig_count if orig_count > 0 else 0\n",
    "        print(f\"{cls}: {augmentation_ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLEBlock(nn.Module):\n",
    "    \"\"\"Spatial Light-weight Enhancement Block (SLE).\n",
    "\n",
    "    This block implements a lightweight channel attention mechanism that enhances relevant\n",
    "    features by focusing on important channels. It uses global average pooling followed by\n",
    "    a bottleneck architecture with two fully connected layers.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input channels.\n",
    "\n",
    "    Attributes:\n",
    "        global_pool (nn.AdaptiveAvgPool2d): Global average pooling layer.\n",
    "        fc1 (nn.Conv2d): First fully connected layer implemented as 1x1 convolution.\n",
    "        fc2 (nn.Conv2d): Second fully connected layer implemented as 1x1 convolution.\n",
    "        sigmoid (nn.Sigmoid): Sigmoid activation function.\n",
    "\n",
    "    Input:\n",
    "        x (torch.Tensor): Input feature map for generating attention weights.\n",
    "        y (torch.Tensor): Feature map to be enhanced.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Enhanced feature map (y * attention_weights).\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(SLEBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // 2, 1)\n",
    "        self.fc2 = nn.Conv2d(in_channels // 2, in_channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.global_pool(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return y * x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FASTGANGenerator(nn.Module):\n",
    "    \"\"\"FASTGAN Generator implementation.\n",
    "\n",
    "    This class implements the Generator part of FASTGAN architecture, which transforms\n",
    "    a latent vector into an image using transposed convolutions and skip-layer\n",
    "    excitation blocks.\n",
    "\n",
    "    Args:\n",
    "        latent_dim (int, optional): Dimension of the input noise vector. Defaults to 256.\n",
    "        ngf (int, optional): Number of generator filters in the first conv layer. Defaults to 64.\n",
    "        output_size (int, optional): Size of the output image. Defaults to 64.\n",
    "\n",
    "    Attributes:\n",
    "        output_size (int): Size of the output image.\n",
    "        initial (nn.Sequential): Initial upsampling block.\n",
    "        layer1-4 (nn.Sequential): Upsampling layers with transposed convolutions.\n",
    "        sle1-2 (SLEBlock): Skip-Layer Excitation blocks for feature refinement.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Generated image of shape (batch_size, 3, output_size, output_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=256, ngf=64, output_size=64):\n",
    "        super(FASTGANGenerator, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, ngf * 16, 4, 1, 0),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.sle1 = SLEBlock(ngf * 8)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.sle2 = SLEBlock(ngf * 4)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 2, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.initial(z)\n",
    "        x = self.layer1(x)\n",
    "        x = self.sle1(x, x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.sle2(x, x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "class FASTGANDiscriminator(nn.Module):\n",
    "    def __init__(self, ndf=64, input_size=64):\n",
    "        super(FASTGANDiscriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(ndf * 8, 1, 1),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfSupervisedDiscriminator(nn.Module):\n",
    "    \"\"\"A self-supervised discriminator neural network module for GANs.\n",
    "    This discriminator combines traditional GAN discrimination with self-supervised learning\n",
    "    by incorporating an additional reconstruction task. It consists of a main discriminator\n",
    "    network that processes images through multiple convolutional layers, and a decoder\n",
    "    network that attempts to reconstruct the input from the learned features.\n",
    "    Args:\n",
    "        ndf (int, optional): Number of discriminator filters in first conv layer. Default: 64\n",
    "    Architecture:\n",
    "        Main discriminator:\n",
    "        - Series of Conv2d layers that downsample from 16x16 to 1x1\n",
    "        - Each conv layer followed by BatchNorm and LeakyReLU\n",
    "        - Final Sigmoid activation for binary classification\n",
    "        Decoder (self-supervised):\n",
    "        - Series of ConvTranspose2d layers that upsample from 1x1 to 16x16\n",
    "        - Each layer followed by BatchNorm and ReLU\n",
    "        - Final Tanh activation for image reconstruction\n",
    "    Returns:\n",
    "        tuple: Contains:\n",
    "            - validity (Tensor): Discrimination score between 0 and 1\n",
    "            - reconstruction (Tensor): Reconstructed version of input image\n",
    "    \"\"\"\n",
    "    def __init__(self, ndf=64):\n",
    "        super(SelfSupervisedDiscriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1),          # 16x16 -> 8x8\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),   # 8x8 -> 4x4\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),  # 4x4 -> 2x2\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),  # 2x2 -> 1x1\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 1, 1, 0), # 1x1 -> 1x1\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ndf * 16, 1, 1, 1, 0),      # 1x1 -> 1x1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Small decoders for self-supervised learning\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ndf * 16, ndf * 8, 4, 1, 0),   # 1x1 -> 2x2\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ndf * 8, ndf * 4, 4, 2, 1),    # 2x2 -> 4x4\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ndf * 4, ndf * 2, 4, 2, 1),    # 4x4 -> 8x8\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ndf * 2, 3, 4, 2, 1),          # 8x8 -> 16x16\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.main[:-1](x)            # Extract features before the final layer\n",
    "        validity = self.main[-1](features)      # Compute validity score\n",
    "        reconstruction = self.decoder1(features) # Reconstruct the image\n",
    "        \n",
    "        # Debugging Statements\n",
    "        print(f\"Input Image Size: {x.size()}\")\n",
    "        print(f\"Reconstructed Image Size: {reconstruction.size()}\")\n",
    "        \n",
    "        return validity, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, \n",
    "               device, autoencoder_loss):\n",
    "    \"\"\"Train one step of GAN.\n",
    "            This function performs one training step for both the generator and discriminator\n",
    "            in a Generative Adversarial Network (GAN).\n",
    "            Args:\n",
    "                real_imgs (torch.Tensor): Batch of real images from the dataset\n",
    "                generator (nn.Module): The generator model\n",
    "                discriminator (nn.Module): The discriminator model\n",
    "                g_optimizer (torch.optim.Optimizer): Optimizer for the generator\n",
    "                d_optimizer (torch.optim.Optimizer): Optimizer for the discriminator\n",
    "                device (torch.device): Device to run the computations on ('cuda' or 'cpu')\n",
    "                autoencoder_loss: Currently unused parameter for potential autoencoder loss\n",
    "            Returns:\n",
    "                tuple:\n",
    "                    - float: Discriminator loss for the current batch\n",
    "                    - float: Generator loss for the current batch\n",
    "                    - torch.Tensor: Generated fake images\n",
    "            Note:\n",
    "                The function first trains the discriminator to better distinguish between real\n",
    "                and fake images, then trains the generator to produce more realistic images\n",
    "                that can fool the discriminator.\n",
    "            \"\"\"\n",
    "    batch_size = real_imgs.size(0)\n",
    "    \n",
    "    # Train Discriminator\n",
    "    d_optimizer.zero_grad()\n",
    "    \n",
    "    real_validity = discriminator(real_imgs)\n",
    "    \n",
    "    z = torch.randn(batch_size, 256, 1, 1, device=device)\n",
    "    fake_imgs = generator(z)\n",
    "    fake_validity = discriminator(fake_imgs.detach())\n",
    "    \n",
    "    d_loss = (F.binary_cross_entropy(real_validity, torch.ones_like(real_validity)) +\n",
    "              F.binary_cross_entropy(fake_validity, torch.zeros_like(fake_validity)))\n",
    "    \n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # Train Generator\n",
    "    g_optimizer.zero_grad()\n",
    "    \n",
    "    fake_validity = discriminator(fake_imgs)\n",
    "    g_loss = F.binary_cross_entropy(fake_validity, torch.ones_like(fake_validity))\n",
    "    \n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    return d_loss.item(), g_loss.item(), fake_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fastgan(generator, discriminator, dataloader, num_epochs, device='cuda'):\n",
    "    \"\"\"\n",
    "    Trains a FastGAN model using a generator and discriminator.\n",
    "    This function implements the training loop for a FastGAN architecture, performing\n",
    "    alternating updates to the generator and discriminator networks using Adam optimization.\n",
    "    Args:\n",
    "        generator: The generator neural network model\n",
    "        discriminator: The discriminator neural network model\n",
    "        dataloader: PyTorch DataLoader containing the training data\n",
    "        num_epochs (int): Number of epochs to train for\n",
    "        device (str, optional): Device to run the training on. Defaults to 'cuda'.\n",
    "    Returns:\n",
    "        None. The function updates the models in-place and saves generated images periodically.\n",
    "    Example:\n",
    "        >>> train_fastgan(generator, discriminator, train_loader, num_epochs=100)\n",
    "    \"\"\"\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    autoencoder_loss = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_imgs, _) in enumerate(dataloader):\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            \n",
    "            d_loss, g_loss, fake_imgs = train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, device, autoencoder_loss)\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch [{epoch}/{num_epochs}], '\n",
    "                      f'D_loss: {d_loss:.4f}, G_loss: {g_loss:.4f}')\n",
    "                \n",
    "                # Optional: Save some generated images\n",
    "                if i % 500 == 0:\n",
    "                    save_image(fake_imgs[:16] * 0.5 + 0.5, \n",
    "                               f'generated_images_epoch_{epoch}_batch_{i}.png', \n",
    "                               normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveGrowingManager:\n",
    "    \"\"\"Progressive Growing Manager for GANs.\n",
    "    This class manages the progressive growing of GANs by controlling image size transitions\n",
    "    and blending between different resolutions using an alpha parameter.\n",
    "    Args:\n",
    "        start_size (int): Initial size of generated images. Defaults to 16.\n",
    "        target_size (int): Final target size for generated images. Defaults to 64.\n",
    "        n_steps (int): Number of progressive growing steps. Defaults to 3.\n",
    "    Attributes:\n",
    "        current_size (int): Current size of generated images.\n",
    "        target_size (int): Target size to reach through progressive growing.\n",
    "        n_steps (int): Total number of growing steps.\n",
    "        alpha (float): Blending factor between resolutions (0.0 to 1.0).\n",
    "    Methods:\n",
    "        step(): Advances the progressive growing process by updating alpha and size.\n",
    "        get_size(): Returns the current image size.\n",
    "    Example:\n",
    "        pg_manager = ProgressiveGrowingManager(start_size=16, target_size=128, n_steps=4)\n",
    "        current_size = pg_manager.get_size()  # Returns 16\n",
    "        pg_manager.step()  # Updates alpha value\n",
    "    \"\"\"\n",
    "    def __init__(self, start_size=16, target_size=64, n_steps=3):\n",
    "        self.current_size = start_size\n",
    "        self.target_size = target_size\n",
    "        self.n_steps = n_steps\n",
    "        self.alpha = 0.0\n",
    "        \n",
    "    def step(self):\n",
    "        self.alpha = min(1.0, self.alpha + 0.1)\n",
    "        if self.alpha >= 1.0 and self.current_size < self.target_size:\n",
    "            self.current_size = min(self.current_size * 2, self.target_size)\n",
    "            self.alpha = 0.0\n",
    "            \n",
    "    def get_size(self):\n",
    "        return self.current_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticImageClassifier:\n",
    "    \"\"\"\n",
    "    A class that combines EfficientNetV2 and ShuffleNetV2 models for synthetic image classification.\n",
    "    This classifier uses two pre-trained models (EfficientNetV2 and ShuffleNetV2) to classify synthetic images.\n",
    "    The models are modified to output the specified number of classes and can be loaded with custom pretrained weights.\n",
    "    Images are classified only when both models agree on the classification.\n",
    "    Attributes:\n",
    "        device (str): Device to run the models on ('cuda' or 'cpu')\n",
    "        efficientnet (torch.nn.Module): EfficientNetV2 model instance\n",
    "        shufflenet (torch.nn.Module): ShuffleNetV2 model instance\n",
    "        transform (torchvision.transforms.Compose): Image transformation pipeline for preprocessing\n",
    "        num_classes (int): Number of output classes for classification\n",
    "        device (str, optional): Device to run models on. Defaults to 'cuda'.\n",
    "    Example:\n",
    "        >>> classifier = SyntheticImageClassifier(num_classes=10)\n",
    "        >>> classifier.load_pretrained_weights('efficientnet.pth', 'shufflenet.pth')\n",
    "        >>> mask = classifier.classify_synthetic_images(synthetic_images)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, device='cuda'):\n",
    "        self.device = device\n",
    "        \n",
    "        # EfficientNetV2\n",
    "        self.efficientnet = models.efficientnet_v2_s(pretrained=True)\n",
    "        self.efficientnet.classifier[1] = nn.Linear(self.efficientnet.classifier[1].in_features, num_classes)\n",
    "        self.efficientnet = self.efficientnet.to(device)\n",
    "        \n",
    "        # ShuffleNetV2\n",
    "        self.shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "        self.shufflenet.fc = nn.Linear(self.shufflenet.fc.in_features, num_classes)\n",
    "        self.shufflenet = self.shufflenet.to(device)\n",
    "        \n",
    "        # Transformation for input images\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def load_pretrained_weights(self, efficientnet_path, shufflenet_path):\n",
    "        \"\"\"\n",
    "        Load pretrained weights for both models\n",
    "        \n",
    "        Args:\n",
    "            efficientnet_path (str): Path to EfficientNetV2 weights\n",
    "            shufflenet_path (str): Path to ShuffleNetV2 weights\n",
    "        \"\"\"\n",
    "        self.efficientnet.load_state_dict(torch.load(efficientnet_path))\n",
    "        self.shufflenet.load_state_dict(torch.load(shufflenet_path))\n",
    "        \n",
    "        # Set models to evaluation mode\n",
    "        self.efficientnet.eval()\n",
    "        self.shufflenet.eval()\n",
    "    \n",
    "    def classify_synthetic_images(self, synthetic_images):\n",
    "        \"\"\"\n",
    "        Classify synthetic images using both models\n",
    "        \n",
    "        Args:\n",
    "            synthetic_images (torch.Tensor): Tensor of synthetic images\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Mask of correctly classified images\n",
    "        \"\"\"\n",
    "        # Resize and normalize synthetic images for classification\n",
    "        resized_images = F.interpolate(synthetic_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        normalized_images = (resized_images - resized_images.min()) / (resized_images.max() - resized_images.min())\n",
    "        normalized_images = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(normalized_images)\n",
    "        \n",
    "        # Get predictions from both models\n",
    "        with torch.no_grad():\n",
    "            efficientnet_preds = self.efficientnet(normalized_images)\n",
    "            shufflenet_preds = self.shufflenet(normalized_images)\n",
    "        \n",
    "        # Get class predictions\n",
    "        efficientnet_classes = torch.argmax(efficientnet_preds, dim=1)\n",
    "        shufflenet_classes = torch.argmax(shufflenet_preds, dim=1)\n",
    "        \n",
    "        # Create mask where both models agree\n",
    "        agreed_classification_mask = (efficientnet_classes == shufflenet_classes)\n",
    "        \n",
    "        return agreed_classification_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_original_images_by_class(csv_file, img_dirs, output_base_dir='synthetic_images'):\n",
    "    \"\"\"\n",
    "    Copy original images to synthetic images folder, organized by class.\n",
    "    This function reads a metadata CSV file containing image information and copies the original\n",
    "    images to a new directory structure organized by class. Each image is copied only once to\n",
    "    avoid duplicates.\n",
    "        csv_file (str): Path to the metadata CSV file containing image information with 'dx' \n",
    "            (diagnosis/class) and 'image_id' columns.\n",
    "        img_dirs (list): List of directory paths where original images are stored. The function\n",
    "            will search these directories in order until it finds each image.\n",
    "        output_base_dir (str, optional): Base directory where the class-organized images will\n",
    "            be copied to. Defaults to 'synthetic_images'.\n",
    "    Returns:\n",
    "        None\n",
    "    Side Effects:\n",
    "        - Creates output_base_dir if it doesn't exist\n",
    "        - Creates subdirectories for each unique class in the metadata\n",
    "        - Copies images to their respective class directories\n",
    "        - Prints summary of copied images and their destination\n",
    "    Example:\n",
    "        >>> csv_file = 'metadata.csv'\n",
    "        >>> img_dirs = ['images/folder1', 'images/folder2']\n",
    "        >>> copy_original_images_by_class(csv_file, img_dirs, 'output_directory')\n",
    "    Returns:\n",
    "        None\n",
    "    Side Effects:\n",
    "        - Creates output_base_dir if it doesn't exist\n",
    "        - Creates subdirectories for each unique class in the metadata\n",
    "        - Copies images to their respective class directories\n",
    "        - Prints summary of copied images and their destination\n",
    "    Example:\n",
    "        >>> csv_file = 'metadata.csv'\n",
    "        >>> img_dirs = ['images/folder1', 'images/folder2']\n",
    "        >>> copy_original_images_by_class(csv_file, img_dirs, 'output_directory')\n",
    "    Copy original images to synthetic images folder, organized by class.\n",
    "    This function reads a metadata CSV file containing image information and copies the original\n",
    "    images to a new directory structure organized by class. Each image is copied only once to\n",
    "    avoid duplicates.\n",
    "        csv_file (str): Path to the metadata CSV file containing image information with 'dx' \n",
    "            (diagnosis/class) and 'image_id' columns.\n",
    "        img_dirs (list): List of directory paths where original images are stored. The function\n",
    "            will search these directories in order until it finds each image.\n",
    "        output_base_dir (str, optional): Base directory where the class-organized images will\n",
    "            be copied to. Defaults to 'synthetic_images'.\n",
    "    \n",
    "    Copy original images to synthetic images folder, organized by class\n",
    "    \n",
    "    Args:\n",
    "        csv_file (str): Path to the metadata CSV file\n",
    "        img_dirs (list): List of directories containing original images\n",
    "        output_base_dir (str): Base directory for synthetic images\n",
    "    \"\"\"\n",
    "    # Read the metadata\n",
    "    metadata = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Ensure the output base directory exists\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    \n",
    "    # Track copied images to avoid duplicates\n",
    "    copied_images = set()\n",
    "    \n",
    "    # Iterate through unique classes\n",
    "    for class_name in metadata['dx'].unique():\n",
    "        # Create class-specific directory\n",
    "        class_output_dir = os.path.join(output_base_dir, class_name)\n",
    "        os.makedirs(class_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Filter metadata for current class\n",
    "        class_metadata = metadata[metadata['dx'] == class_name]\n",
    "        \n",
    "        # Copy images for this class\n",
    "        for _, row in class_metadata.iterrows():\n",
    "            img_filename = row['image_id'] + '.jpg'\n",
    "            \n",
    "            # Search for the image in provided directories\n",
    "            for img_dir in img_dirs:\n",
    "                img_path = os.path.join(img_dir, img_filename)\n",
    "                \n",
    "                if os.path.exists(img_path):\n",
    "                    # Destination path\n",
    "                    dest_path = os.path.join(class_output_dir, img_filename)\n",
    "                    \n",
    "                    # Copy only if not already copied\n",
    "                    if img_path not in copied_images:\n",
    "                        shutil.copy2(img_path, dest_path)\n",
    "                        copied_images.add(img_path)\n",
    "                    break\n",
    "    \n",
    "    print(f\"Original images copied to {output_base_dir}\")\n",
    "    print(f\"Total unique images copied: {len(copied_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for training and generating synthetic medical images using a FASTGAN architecture.\n",
    "    This function performs the following steps:\n",
    "    1. Sets up the device (CPU/GPU) and seeds for reproducibility\n",
    "    2. Initializes data transformations and loads the HAM10000 skin cancer dataset\n",
    "    3. Creates and trains a FASTGAN model (generator and discriminator)\n",
    "    4. Generates synthetic images for underrepresented classes\n",
    "    5. Filters synthetic images using a classifier\n",
    "    6. Saves the generated images and plots data distribution comparisons\n",
    "    Returns:\n",
    "        None\n",
    "    Dependencies:\n",
    "        - torch: PyTorch library for deep learning\n",
    "        - torchvision: PyTorch computer vision library\n",
    "        - numpy: Numerical computing library\n",
    "        - os: Operating system interface\n",
    "    Note:\n",
    "        - The function is designed to work with the HAM10000 skin cancer dataset\n",
    "        - Synthetic images are generated for all classes except 'nv' and 'vasc'\n",
    "        - The function requires a GPU for optimal performance\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    csv_file = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n",
    "    img_dirs = ['/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2']\n",
    "    \n",
    "    # Call the function to plot the data distribution\n",
    "    \n",
    "    copy_original_images_by_class(csv_file, img_dirs)\n",
    "    dataset = HAM10000Dataset(csv_file, img_dirs, transform=transform, device=device)\n",
    "    \n",
    "    num_classes = len(dataset.label_encoder.classes_)\n",
    "    print(\"Unique Classes:\", dataset.label_encoder.classes_)\n",
    "    print(\"Number of Classes:\", num_classes)\n",
    "    \n",
    "    batch_size = 64\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    latent_dim = 256\n",
    "    generator = FASTGANGenerator(latent_dim, output_size=64).to(device)\n",
    "    discriminator = FASTGANDiscriminator(input_size=64).to(device)\n",
    "    \n",
    "    num_epochs = 150\n",
    "    \n",
    "    train_fastgan(generator, discriminator, data_loader, num_epochs, device)\n",
    "    \n",
    "    os.makedirs('synthetic_images', exist_ok=True)\n",
    "    \n",
    "    # Initialize the Synthetic Image Classifier\n",
    "    classifier = SyntheticImageClassifier(num_classes=num_classes, device=device)\n",
    "    \n",
    "    # Note: In a real scenario, you would load pretrained weights\n",
    "    # classifier.load_pretrained_weights('path/to/efficientnet_weights.pth', 'path/to/shufflenet_weights.pth')\n",
    "    \n",
    "    synthetic_images_by_class = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for class_idx in range(num_classes):\n",
    "            # Get the class name\n",
    "            class_name = dataset.label_encoder.inverse_transform([class_idx])[0]\n",
    "            \n",
    "            # Skip generating 1000 images for 'nv' and 'vasc' classes\n",
    "            if class_name in ['nv', 'vasc']:\n",
    "                continue\n",
    "            \n",
    "            # Number of images to generate\n",
    "            num_images_to_generate = 1000\n",
    "            \n",
    "            # Calculate number of batches needed\n",
    "            num_batches = (num_images_to_generate + batch_size - 1) // batch_size\n",
    "            \n",
    "            valid_synthetic_images_list = []\n",
    "            \n",
    "            for _ in range(num_batches):\n",
    "                # Generate a batch of images\n",
    "                z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "                synthetic_images = generator(z)\n",
    "                \n",
    "                # Classify synthetic images\n",
    "                valid_image_mask = classifier.classify_synthetic_images(synthetic_images)\n",
    "                \n",
    "                # Filter synthetic images based on classification\n",
    "                valid_synthetic_images = synthetic_images[valid_image_mask]\n",
    "                \n",
    "                valid_synthetic_images_list.append(valid_synthetic_images)\n",
    "                \n",
    "                # Break if we have enough images\n",
    "                if len(torch.cat(valid_synthetic_images_list)) >= num_images_to_generate:\n",
    "                    break\n",
    "            \n",
    "            # Concatenate and trim to exact number of images\n",
    "            valid_synthetic_images = torch.cat(valid_synthetic_images_list)[:num_images_to_generate]\n",
    "            \n",
    "            synthetic_images_by_class[class_idx] = valid_synthetic_images.cpu()\n",
    "            \n",
    "            class_dir = os.path.join('synthetic_images', class_name)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            \n",
    "            #for i, img in enumerate(valid_synthetic_images):\n",
    "            #    save_path = os.path.join(class_dir, f'synthetic_image_{i}.png')\n",
    "            #    save_image((img * 0.5 + 0.5), save_path)\n",
    "    \n",
    "    print(\"Synthetic image generation, classification, and filtering complete!\")\n",
    "    plot_data_distribution_comparison(csv_file,'/kaggle/working/synthetic_images')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain Step 2 of this code, which focuses on the FastGAN architecture and training components. I'll break down the key classes and functions:\n",
    "\n",
    "1. **SLEBlock (Skip-Layer Excitation Block)**\n",
    "```python\n",
    "class SLEBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(SLEBlock, self).__init__()\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_channels, in_channels // 2, 1)\n",
    "        self.fc2 = nn.Conv2d(in_channels // 2, in_channels, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "```\n",
    "This block implements attention mechanism in the generator:\n",
    "- Uses global average pooling to capture channel-wise statistics\n",
    "- Has two 1x1 convolutions that act as fully connected layers\n",
    "- Applies sigmoid activation to generate attention weights\n",
    "- The output modulates the skip connection features\n",
    "\n",
    "2. **FASTGANGenerator**\n",
    "```python\n",
    "class FASTGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=256, ngf=64, output_size=64)\n",
    "```\n",
    "The generator has a progressive architecture:\n",
    "- Takes 256-dimensional noise vector as input\n",
    "- Uses transposed convolutions to progressively upscale the image\n",
    "- Incorporates SLE blocks after certain layers for better feature refinement\n",
    "- The output size is 64x64x3 (RGB image)\n",
    "- Uses BatchNorm and ReLU activations throughout\n",
    "- Final Tanh activation to normalize output to [-1, 1]\n",
    "\n",
    "3. **FASTGANDiscriminator**\n",
    "```python\n",
    "class FASTGANDiscriminator(nn.Module):\n",
    "    def __init__(self, ndf=64, input_size=64)\n",
    "```\n",
    "The discriminator:\n",
    "- Takes 64x64x3 images as input\n",
    "- Uses regular convolutions to progressively downsample\n",
    "- Includes BatchNorm and LeakyReLU activations\n",
    "- Ends with adaptive average pooling and final convolution\n",
    "- Outputs a single value through sigmoid for real/fake classification\n",
    "\n",
    "4. **Training Functions**\n",
    "\n",
    "`train_step()` handles a single training iteration:\n",
    "```python\n",
    "def train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, device, autoencoder_loss)\n",
    "```\n",
    "- First trains discriminator:\n",
    "  - Gets predictions for real images\n",
    "  - Generates fake images\n",
    "  - Calculates discriminator loss using binary cross entropy\n",
    "  - Updates discriminator weights\n",
    "\n",
    "- Then trains generator:\n",
    "  - Generates fake images\n",
    "  - Gets discriminator predictions\n",
    "  - Calculates generator loss\n",
    "  - Updates generator weights\n",
    "\n",
    "`train_fastgan()` manages the overall training process:\n",
    "```python\n",
    "def train_fastgan(generator, discriminator, dataloader, num_epochs, device='cuda')\n",
    "```\n",
    "- Sets up Adam optimizers for both networks\n",
    "- Runs training for specified number of epochs\n",
    "- Prints progress every 100 batches\n",
    "- Saves sample generated images every 500 batches\n",
    "\n",
    "5. **Image Classification Verification**\n",
    "\n",
    "```python\n",
    "class SyntheticImageClassifier:\n",
    "    def __init__(self, num_classes, device='cuda')\n",
    "```\n",
    "This class uses two pre-trained models (EfficientNetV2 and ShuffleNetV2) to verify the quality of generated images:\n",
    "- Both models are modified for the specific number of classes\n",
    "- Images are only kept if both models agree on the classification\n",
    "- Helps filter out poor quality or ambiguous generated images\n",
    "\n",
    "The whole system follows a progressive training approach where:\n",
    "1. The generator creates images from random noise\n",
    "2. The discriminator learns to distinguish real from fake\n",
    "3. Generated images are verified by classification models\n",
    "4. Only high-quality, confidently classified images are kept\n",
    "\n",
    "This architecture is specifically designed for fast training while maintaining good image quality, making it suitable for medical image synthesis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the core GAN algorithm architecture and configuration from the provided information:\n",
    "\n",
    "1. Architecture Components:\n",
    "- Generator: Uses single convolution layer per resolution with restricted channels\n",
    "- Skip-Layer Excitation (SLE) module for gradient flow enhancement\n",
    "- Self-supervised discriminator with small decoders\n",
    "- Two CNN classifiers (EfficientNetV2 and ShuffleNetV2) for synthetic image filtering\n",
    "\n",
    "2. Key Features:\n",
    "- SLE module: \n",
    "  - Implements channel-wise multiplications\n",
    "  - Creates skip-connections between distant resolutions\n",
    "  - Handles content/style attribute disentanglement\n",
    "\n",
    "3. Training Process:\n",
    "- Uses hinge version of adversarial loss\n",
    "- Iterative training between discriminator and generator\n",
    "- Discriminator regularization through auto-encoding\n",
    "- Two-stage filtering:\n",
    "  1. Generate synthetic images from original medical data\n",
    "  2. Filter generations through dual CNN validation\n",
    "\n",
    "4. Data Flow:\n",
    "```\n",
    "Original Images → FASTGAN Generator → Synthetic Images → Dual CNN Filtering → Filtered Dataset + Original Images → Few-shot Classifier\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
