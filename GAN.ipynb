{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport shutil\n\nclass HAM10000Dataset(Dataset):\n    def __init__(self, csv_file, img_dirs, transform=None, device='cuda'):\n        self.data = pd.read_csv(csv_file)\n        self.img_dirs = img_dirs\n        self.transform = transform\n        self.device = device\n        \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        self.data['encoded_label'] = self.label_encoder.fit_transform(self.data['dx'])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n        for img_dir in self.img_dirs:\n            img_path = os.path.join(img_dir, img_name)\n            if os.path.exists(img_path):\n                image = Image.open(img_path).convert('RGB')\n                if self.transform:\n                    image = self.transform(image)\n                label = self.data.iloc[idx]['encoded_label']\n                return image, label\n        raise FileNotFoundError(f\"Image {img_name} not found in directories {self.img_dirs}\")\n\ndef plot_data_distribution_comparison(original_csv, synthetic_images_dir):\n    \"\"\"\n    Compare the data distribution of the original HAM10000 dataset \n    with the distribution after adding synthetic images\n    \n    Args:\n        original_csv (str): Path to the original metadata CSV file\n        synthetic_images_dir (str): Path to the directory containing synthetic images\n    \"\"\"\n    # Read the original metadata\n    metadata = pd.read_csv(original_csv)\n    \n    # Count original class distribution\n    original_class_counts = metadata['dx'].value_counts()\n    \n    # Prepare synthetic image class counts\n    synthetic_class_counts = {}\n    label_encoder = LabelEncoder()\n    label_encoder.fit(metadata['dx'])\n    \n    # Count synthetic images per class\n    for class_name in label_encoder.classes_:\n        class_dir = os.path.join(synthetic_images_dir, class_name)\n        if os.path.exists(class_dir):\n            synthetic_class_counts[class_name] = len([f for f in os.listdir(class_dir) \n                                                      if f.endswith(('.png', '.jpg'))])\n        else:\n            synthetic_class_counts[class_name] = 0\n    \n    # Convert to Series for consistent plotting\n    synthetic_class_counts = pd.Series(synthetic_class_counts)\n    \n    # Prepare the plot\n    plt.figure(figsize=(15, 6))\n    \n    # Create a side-by-side bar plot\n    x = np.arange(len(original_class_counts))\n    width = 0.4\n    \n    plt.bar(x - width/2, original_class_counts.values, width, label='Original Dataset', color='blue', alpha=0.7)\n    plt.bar(x + width/2, synthetic_class_counts.values, width, label='Synthetic Images', color='orange', alpha=0.7)\n    \n    plt.title('Comparison of Original HAM10000 Dataset and Synthetic Images', fontsize=16)\n    plt.xlabel('Skin Lesion Type', fontsize=14)\n    plt.ylabel('Number of Samples', fontsize=14)\n    plt.xticks(x, original_class_counts.index, rotation=90, ha='right')\n    plt.legend()\n    \n    # Add count labels on top of each bar\n    for i, (orig, synth) in enumerate(zip(original_class_counts.values, synthetic_class_counts.values)):\n        plt.text(i - width/2, orig + 50, str(int(orig)), ha='center', va='bottom', fontsize=8)\n        plt.text(i + width/2, synth + 50, str(int(synth)), ha='center', va='bottom', fontsize=8)\n    \n    plt.tight_layout()\n    #plt.savefig('dataset_distribution_comparison.png', dpi=300, bbox_inches='tight')\n    plt.show('dataset_distribution_comparison.png')\n    plt.close()\n\n    # Print detailed comparison\n    print(\"\\nOriginal Dataset Distribution:\")\n    total_original = len(metadata)\n    for cls, count in original_class_counts.items():\n        percentage = (count / total_original) * 100\n        print(f\"{cls}: {count} samples ({percentage:.2f}%)\")\n    \n    print(\"\\nSynthetic Images Distribution:\")\n    total_synthetic = synthetic_class_counts.sum()\n    for cls, count in synthetic_class_counts.items():\n        percentage = (count / total_synthetic) * 100 if total_synthetic > 0 else 0\n        print(f\"{cls}: {count} synthetic images ({percentage:.2f}%)\")\n    \n    # Calculate and print augmentation ratio\n    print(\"\\nAugmentation Ratio:\")\n    for cls in original_class_counts.index:\n        orig_count = original_class_counts.get(cls, 0)\n        synth_count = synthetic_class_counts.get(cls, 0)\n        augmentation_ratio = synth_count / orig_count if orig_count > 0 else 0\n        print(f\"{cls}: {augmentation_ratio:.2f}x\")\n\nclass SLEBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(SLEBlock, self).__init__()\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(in_channels, in_channels // 2, 1)\n        self.fc2 = nn.Conv2d(in_channels // 2, in_channels, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, y):\n        x = self.global_pool(x)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return y * x\n        \nclass FASTGANGenerator(nn.Module):\n    def __init__(self, latent_dim=256, ngf=64, output_size=64):\n        super(FASTGANGenerator, self).__init__()\n        self.output_size = output_size\n        self.initial = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, ngf * 16, 4, 1, 0),\n            nn.BatchNorm2d(ngf * 16),\n            nn.ReLU(True)\n        )\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True)\n        )\n        self.sle1 = SLEBlock(ngf * 8)\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True)\n        )\n        self.sle2 = SLEBlock(ngf * 4)\n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True)\n        )\n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 2, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        x = self.initial(z)\n        x = self.layer1(x)\n        x = self.sle1(x, x)\n        x = self.layer2(x)\n        x = self.sle2(x, x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\nclass FASTGANDiscriminator(nn.Module):\n    def __init__(self, ndf=64, input_size=64):\n        super(FASTGANDiscriminator, self).__init__()\n        self.input_size = input_size\n        self.main = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(ndf * 8, 1, 1),\n            nn.Flatten(),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.main(x)\n\nclass SelfSupervisedDiscriminator(nn.Module):\n    def __init__(self, ndf=64):\n        super(SelfSupervisedDiscriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1),          # 16x16 -> 8x8\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),   # 8x8 -> 4x4\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),  # 4x4 -> 2x2\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),  # 2x2 -> 1x1\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 8, ndf * 16, 1, 1, 0), # 1x1 -> 1x1\n            nn.BatchNorm2d(ndf * 16),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 16, 1, 1, 1, 0),      # 1x1 -> 1x1\n            nn.Sigmoid()\n        )\n        \n        # Small decoders for self-supervised learning\n        self.decoder1 = nn.Sequential(\n            nn.ConvTranspose2d(ndf * 16, ndf * 8, 4, 1, 0),   # 1x1 -> 2x2\n            nn.BatchNorm2d(ndf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf * 8, ndf * 4, 4, 2, 1),    # 2x2 -> 4x4\n            nn.BatchNorm2d(ndf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf * 4, ndf * 2, 4, 2, 1),    # 4x4 -> 8x8\n            nn.BatchNorm2d(ndf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf * 2, 3, 4, 2, 1),          # 8x8 -> 16x16\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        features = self.main[:-1](x)            # Extract features before the final layer\n        validity = self.main[-1](features)      # Compute validity score\n        reconstruction = self.decoder1(features) # Reconstruct the image\n        \n        # Debugging Statements\n        print(f\"Input Image Size: {x.size()}\")\n        print(f\"Reconstructed Image Size: {reconstruction.size()}\")\n        \n        return validity, reconstruction\n\ndef train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, \n               device, autoencoder_loss):\n    batch_size = real_imgs.size(0)\n    \n    # Train Discriminator\n    d_optimizer.zero_grad()\n    \n    real_validity = discriminator(real_imgs)\n    \n    z = torch.randn(batch_size, 256, 1, 1, device=device)\n    fake_imgs = generator(z)\n    fake_validity = discriminator(fake_imgs.detach())\n    \n    d_loss = (F.binary_cross_entropy(real_validity, torch.ones_like(real_validity)) +\n              F.binary_cross_entropy(fake_validity, torch.zeros_like(fake_validity)))\n    \n    d_loss.backward()\n    d_optimizer.step()\n    \n    # Train Generator\n    g_optimizer.zero_grad()\n    \n    fake_validity = discriminator(fake_imgs)\n    g_loss = F.binary_cross_entropy(fake_validity, torch.ones_like(fake_validity))\n    \n    g_loss.backward()\n    g_optimizer.step()\n    \n    return d_loss.item(), g_loss.item(), fake_imgs\n\ndef train_fastgan(generator, discriminator, dataloader, num_epochs, device='cuda'):\n    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    autoencoder_loss = nn.MSELoss()\n    \n    for epoch in range(num_epochs):\n        for i, (real_imgs, _) in enumerate(dataloader):\n            real_imgs = real_imgs.to(device)\n            \n            d_loss, g_loss, fake_imgs = train_step(real_imgs, generator, discriminator, \n                                                   g_optimizer, d_optimizer, \n                                                   device, autoencoder_loss)\n            \n            if i % 100 == 0:\n                print(f'Epoch [{epoch}/{num_epochs}], '\n                      f'D_loss: {d_loss:.4f}, G_loss: {g_loss:.4f}')\n                \n                # Optional: Save some generated images\n                if i % 500 == 0:\n                    save_image(fake_imgs[:16] * 0.5 + 0.5, \n                               f'generated_images_epoch_{epoch}_batch_{i}.png', \n                               normalize=False)\n\nclass ProgressiveGrowingManager:\n    def __init__(self, start_size=16, target_size=64, n_steps=3):\n        self.current_size = start_size\n        self.target_size = target_size\n        self.n_steps = n_steps\n        self.alpha = 0.0\n        \n    def step(self):\n        self.alpha = min(1.0, self.alpha + 0.1)\n        if self.alpha >= 1.0 and self.current_size < self.target_size:\n            self.current_size = min(self.current_size * 2, self.target_size)\n            self.alpha = 0.0\n            \n    def get_size(self):\n        return self.current_size\n\nclass SyntheticImageClassifier:\n    def __init__(self, num_classes, device='cuda'):\n        self.device = device\n        \n        # EfficientNetV2\n        self.efficientnet = models.efficientnet_v2_s(pretrained=True)\n        self.efficientnet.classifier[1] = nn.Linear(self.efficientnet.classifier[1].in_features, num_classes)\n        self.efficientnet = self.efficientnet.to(device)\n        \n        # ShuffleNetV2\n        self.shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n        self.shufflenet.fc = nn.Linear(self.shufflenet.fc.in_features, num_classes)\n        self.shufflenet = self.shufflenet.to(device)\n        \n        # Transformation for input images\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n    def load_pretrained_weights(self, efficientnet_path, shufflenet_path):\n        \"\"\"\n        Load pretrained weights for both models\n        \n        Args:\n            efficientnet_path (str): Path to EfficientNetV2 weights\n            shufflenet_path (str): Path to ShuffleNetV2 weights\n        \"\"\"\n        self.efficientnet.load_state_dict(torch.load(efficientnet_path))\n        self.shufflenet.load_state_dict(torch.load(shufflenet_path))\n        \n        # Set models to evaluation mode\n        self.efficientnet.eval()\n        self.shufflenet.eval()\n    \n    def classify_synthetic_images(self, synthetic_images):\n        \"\"\"\n        Classify synthetic images using both models\n        \n        Args:\n            synthetic_images (torch.Tensor): Tensor of synthetic images\n        \n        Returns:\n            torch.Tensor: Mask of correctly classified images\n        \"\"\"\n        # Resize and normalize synthetic images for classification\n        resized_images = F.interpolate(synthetic_images, size=(224, 224), mode='bilinear', align_corners=False)\n        normalized_images = (resized_images - resized_images.min()) / (resized_images.max() - resized_images.min())\n        normalized_images = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(normalized_images)\n        \n        # Get predictions from both models\n        with torch.no_grad():\n            efficientnet_preds = self.efficientnet(normalized_images)\n            shufflenet_preds = self.shufflenet(normalized_images)\n        \n        # Get class predictions\n        efficientnet_classes = torch.argmax(efficientnet_preds, dim=1)\n        shufflenet_classes = torch.argmax(shufflenet_preds, dim=1)\n        \n        # Create mask where both models agree\n        agreed_classification_mask = (efficientnet_classes == shufflenet_classes)\n        \n        return agreed_classification_mask\n\ndef copy_original_images_by_class(csv_file, img_dirs, output_base_dir='synthetic_images'):\n    \"\"\"\n    Copy original images to synthetic images folder, organized by class\n    \n    Args:\n        csv_file (str): Path to the metadata CSV file\n        img_dirs (list): List of directories containing original images\n        output_base_dir (str): Base directory for synthetic images\n    \"\"\"\n    # Read the metadata\n    metadata = pd.read_csv(csv_file)\n    \n    # Ensure the output base directory exists\n    os.makedirs(output_base_dir, exist_ok=True)\n    \n    # Track copied images to avoid duplicates\n    copied_images = set()\n    \n    # Iterate through unique classes\n    for class_name in metadata['dx'].unique():\n        # Create class-specific directory\n        class_output_dir = os.path.join(output_base_dir, class_name)\n        os.makedirs(class_output_dir, exist_ok=True)\n        \n        # Filter metadata for current class\n        class_metadata = metadata[metadata['dx'] == class_name]\n        \n        # Copy images for this class\n        for _, row in class_metadata.iterrows():\n            img_filename = row['image_id'] + '.jpg'\n            \n            # Search for the image in provided directories\n            for img_dir in img_dirs:\n                img_path = os.path.join(img_dir, img_filename)\n                \n                if os.path.exists(img_path):\n                    # Destination path\n                    dest_path = os.path.join(class_output_dir, img_filename)\n                    \n                    # Copy only if not already copied\n                    if img_path not in copied_images:\n                        shutil.copy2(img_path, dest_path)\n                        copied_images.add(img_path)\n                    break\n    \n    print(f\"Original images copied to {output_base_dir}\")\n    print(f\"Total unique images copied: {len(copied_images)}\")\n\ndef main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    torch.manual_seed(42)\n    np.random.seed(42)\n    \n    transform = transforms.Compose([\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    csv_file = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n    img_dirs = ['/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2']\n    \n    # Call the function to plot the data distribution\n    \n    copy_original_images_by_class(csv_file, img_dirs)\n    dataset = HAM10000Dataset(csv_file, img_dirs, transform=transform, device=device)\n    \n    num_classes = len(dataset.label_encoder.classes_)\n    print(\"Unique Classes:\", dataset.label_encoder.classes_)\n    print(\"Number of Classes:\", num_classes)\n    \n    batch_size = 64\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    \n    latent_dim = 256\n    generator = FASTGANGenerator(latent_dim, output_size=64).to(device)\n    discriminator = FASTGANDiscriminator(input_size=64).to(device)\n    \n    num_epochs = 150\n    \n    train_fastgan(generator, discriminator, data_loader, num_epochs, device)\n    \n    os.makedirs('synthetic_images', exist_ok=True)\n    \n    # Initialize the Synthetic Image Classifier\n    classifier = SyntheticImageClassifier(num_classes=num_classes, device=device)\n    \n    # Note: In a real scenario, you would load pretrained weights\n    # classifier.load_pretrained_weights('path/to/efficientnet_weights.pth', 'path/to/shufflenet_weights.pth')\n    \n    synthetic_images_by_class = {}\n    \n    with torch.no_grad():\n        for class_idx in range(num_classes):\n            # Get the class name\n            class_name = dataset.label_encoder.inverse_transform([class_idx])[0]\n            \n            # Skip generating 1000 images for 'nv' and 'vasc' classes\n            if class_name in ['nv', 'vasc']:\n                continue\n            \n            # Number of images to generate\n            num_images_to_generate = 1000\n            \n            # Calculate number of batches needed\n            num_batches = (num_images_to_generate + batch_size - 1) // batch_size\n            \n            valid_synthetic_images_list = []\n            \n            for _ in range(num_batches):\n                # Generate a batch of images\n                z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n                synthetic_images = generator(z)\n                \n                # Classify synthetic images\n                valid_image_mask = classifier.classify_synthetic_images(synthetic_images)\n                \n                # Filter synthetic images based on classification\n                valid_synthetic_images = synthetic_images[valid_image_mask]\n                \n                valid_synthetic_images_list.append(valid_synthetic_images)\n                \n                # Break if we have enough images\n                if len(torch.cat(valid_synthetic_images_list)) >= num_images_to_generate:\n                    break\n            \n            # Concatenate and trim to exact number of images\n            valid_synthetic_images = torch.cat(valid_synthetic_images_list)[:num_images_to_generate]\n            \n            synthetic_images_by_class[class_idx] = valid_synthetic_images.cpu()\n            \n            class_dir = os.path.join('synthetic_images', class_name)\n            os.makedirs(class_dir, exist_ok=True)\n            \n            #for i, img in enumerate(valid_synthetic_images):\n            #    save_path = os.path.join(class_dir, f'synthetic_image_{i}.png')\n            #    save_image((img * 0.5 + 0.5), save_path)\n    \n    print(\"Synthetic image generation, classification, and filtering complete!\")\n    plot_data_distribution_comparison(csv_file,'/kaggle/working/synthetic_images')\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}