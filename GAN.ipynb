{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport shutil\n\nclass HAM10000Dataset(Dataset):\n    def __init__(self, csv_file, img_dirs, transform=None, device='cuda'):\n        self.data = pd.read_csv(csv_file)\n        self.img_dirs = img_dirs\n        self.transform = transform\n        self.device = device\n        \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        self.data['encoded_label'] = self.label_encoder.fit_transform(self.data['dx'])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n        for img_dir in self.img_dirs:\n            img_path = os.path.join(img_dir, img_name)\n            if os.path.exists(img_path):\n                image = Image.open(img_path).convert('RGB')\n                if self.transform:\n                    image = self.transform(image)\n                label = self.data.iloc[idx]['encoded_label']\n                return image, label\n        raise FileNotFoundError(f\"Image {img_name} not found in directories {self.img_dirs}\")\n\nclass EnhancedSLEBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(EnhancedSLEBlock, self).__init__()\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Content branch - adjusted channel dimensions\n        self.content_fc1 = nn.Conv2d(in_channels, in_channels, 1)  # Changed from in_channels//2\n        self.content_fc2 = nn.Conv2d(in_channels, in_channels, 1)  # Input/output channels match\n        \n        # Style branch\n        self.style_modulation = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 1),\n            nn.InstanceNorm2d(in_channels),\n            nn.ReLU(True)\n        )\n        \n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.beta = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, skip_x):\n        # Content pathway\n        content = self.global_pool(x)\n        content = F.relu(self.content_fc1(content))\n        content = self.content_fc2(content)\n        content = torch.sigmoid(content)\n        \n        # Style pathway\n        style = self.style_modulation(skip_x)\n        \n        # Combine content and style\n        output = skip_x * content  # Content modulation\n        output = output + self.gamma * style + self.beta  # Style modulation\n        return output\n\nclass EnhancedFASTGANGenerator(nn.Module):\n    def __init__(self, latent_dim=256, ngf=64, output_size=64):\n        super(EnhancedFASTGANGenerator, self).__init__()\n        self.output_size = output_size\n        \n        self.initial = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, ngf * 16, 4, 1, 0),\n            nn.BatchNorm2d(ngf * 16),\n            nn.ReLU(True)\n        )\n        \n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True)\n        )\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True)\n        )\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 2, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n        \n        self.sle1 = EnhancedSLEBlock(ngf * 8)\n        self.sle2 = EnhancedSLEBlock(ngf * 4)\n\n    def forward(self, z):\n        x0 = self.initial(z)\n        x1 = self.layer1(x0)\n        x1_sle = self.sle1(x0, x1)\n        x2 = self.layer2(x1_sle)\n        x2_sle = self.sle2(x1_sle, x2)\n        x3 = self.layer3(x2_sle)\n        x4 = self.layer4(x3)\n        return x4\n\nclass EnhancedFASTGANDiscriminator(nn.Module):\n    def __init__(self, ndf=64, input_size=64):\n        super(EnhancedFASTGANDiscriminator, self).__init__()\n        self.input_size = input_size\n        \n        # Shared feature extractor\n        self.features = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2)\n        )\n        \n        # Discriminator head\n        self.discriminator = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(ndf * 8, 1, 1),\n            nn.Flatten(),\n            nn.Sigmoid()\n        )\n        \n        # Decoder for self-supervision\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(ndf * 8, ndf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf * 4, ndf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf * 2, ndf, 4, 2, 1),\n            nn.BatchNorm2d(ndf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        features = self.features(x)\n        validity = self.discriminator(features)\n        reconstruction = self.decoder(features)\n        return validity, reconstruction\n\nclass SyntheticImageClassifier:\n    def __init__(self, num_classes, device='cuda'):\n        self.device = device\n        \n        # EfficientNetV2\n        self.efficientnet = models.efficientnet_v2_s(pretrained=True)\n        self.efficientnet.classifier[1] = nn.Linear(self.efficientnet.classifier[1].in_features, num_classes)\n        self.efficientnet = self.efficientnet.to(device)\n        \n        # ShuffleNetV2\n        self.shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n        self.shufflenet.fc = nn.Linear(self.shufflenet.fc.in_features, num_classes)\n        self.shufflenet = self.shufflenet.to(device)\n        \n        # Transformation for input images\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def classify_synthetic_images(self, synthetic_images):\n        resized_images = F.interpolate(synthetic_images, size=(224, 224), mode='bilinear', align_corners=False)\n        normalized_images = (resized_images - resized_images.min()) / (resized_images.max() - resized_images.min())\n        normalized_images = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(normalized_images)\n        \n        with torch.no_grad():\n            efficientnet_preds = self.efficientnet(normalized_images)\n            shufflenet_preds = self.shufflenet(normalized_images)\n        \n        efficientnet_classes = torch.argmax(efficientnet_preds, dim=1)\n        shufflenet_classes = torch.argmax(shufflenet_preds, dim=1)\n        \n        agreed_classification_mask = (efficientnet_classes == shufflenet_classes)\n        \n        return agreed_classification_mask\n\ndef enhanced_train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, \n                       device, lambda_rec=10.0):\n    batch_size = real_imgs.size(0)\n    \n    # Train Discriminator\n    d_optimizer.zero_grad()\n    \n    # Real images\n    real_validity, real_reconstruction = discriminator(real_imgs)\n    \n    # Generate fake images\n    z = torch.randn(batch_size, 256, 1, 1, device=device)\n    fake_imgs = generator(z)\n    fake_validity, _ = discriminator(fake_imgs.detach())\n    \n    # Hinge loss\n    d_loss_real = torch.mean(F.relu(1.0 - real_validity))\n    d_loss_fake = torch.mean(F.relu(1.0 + fake_validity))\n    d_loss_adv = d_loss_real + d_loss_fake\n    \n    # Reconstruction loss for self-supervision\n    d_loss_rec = F.mse_loss(real_reconstruction, real_imgs)\n    \n    # Total discriminator loss\n    d_loss = d_loss_adv + lambda_rec * d_loss_rec\n    \n    d_loss.backward()\n    d_optimizer.step()\n    \n    # Train Generator\n    g_optimizer.zero_grad()\n    \n    fake_validity, _ = discriminator(fake_imgs)\n    g_loss = -torch.mean(fake_validity)  # Hinge loss for generator\n    \n    g_loss.backward()\n    g_optimizer.step()\n    \n    return {\n        'd_loss': d_loss.item(),\n        'd_loss_adv': d_loss_adv.item(),\n        'd_loss_rec': d_loss_rec.item(),\n        'g_loss': g_loss.item()\n    }, fake_imgs\n\ndef plot_data_distribution_comparison(original_csv, synthetic_images_dir):\n    metadata = pd.read_csv(original_csv)\n    original_class_counts = metadata['dx'].value_counts()\n    synthetic_class_counts = {}\n    label_encoder = LabelEncoder()\n    label_encoder.fit(metadata['dx'])\n    \n    for class_name in label_encoder.classes_:\n        class_dir = os.path.join(synthetic_images_dir, class_name)\n        if os.path.exists(class_dir):\n            synthetic_class_counts[class_name] = len([f for f in os.listdir(class_dir) \n                                                    if f.endswith(('.png', '.jpg'))])\n        else:\n            synthetic_class_counts[class_name] = 0\n    \n    synthetic_class_counts = pd.Series(synthetic_class_counts)\n    \n    plt.figure(figsize=(15, 6))\n    x = np.arange(len(original_class_counts))\n    width = 0.4\n    \n    plt.bar(x - width/2, original_class_counts.values, width, label='Original Dataset', color='blue', alpha=0.7)\n    plt.bar(x + width/2, synthetic_class_counts.values, width, label='Synthetic Images', color='orange', alpha=0.7)\n    \n    plt.title('Comparison of Original HAM10000 Dataset and Synthetic Images', fontsize=16)\n    plt.xlabel('Skin Lesion Type', fontsize=14)\n    plt.ylabel('Number of Samples', fontsize=14)\n    plt.xticks(x, original_class_counts.index, rotation=90, ha='right')\n    plt.legend()\n    \n    for i, (orig, synth) in enumerate(zip(original_class_counts.values, synthetic_class_counts.values)):\n        plt.text(i - width/2, orig + 50, str(int(orig)), ha='center', va='bottom', fontsize=8)\n        plt.text(i + width/2, synth + 50, str(int(synth)), ha='center', va='bottom', fontsize=8)\n    \n    plt.tight_layout()\n    plt.savefig('dataset_distribution_comparison.png')\n    plt.close()\n\ndef copy_original_images_by_class(csv_file, img_dirs, output_base_dir='synthetic_images'):\n    metadata = pd.read_csv(csv_file)\n    os.makedirs(output_base_dir, exist_ok=True)\n    copied_images = set()\n    \n    for class_name in metadata['dx'].unique():\n        class_output_dir = os.path.join(output_base_dir, class_name)\n        os.makedirs(class_output_dir, exist_ok=True)\n        \n        class_metadata = metadata[metadata['dx'] == class_name]\n        \n        for _, row in class_metadata.iterrows():\n            img_filename = row['image_id'] + '.jpg'\n            \n            for img_dir in img_dirs:\n                img_path = os.path.join(img_dir, img_filename)\n                \n                if os.path.exists(img_path):\n                    dest_path = os.path.join(class_output_dir, img_filename)\n                    \n                    if img_path not in copied_images:\n                        shutil.copy2(img_path, dest_path)\n                        copied_images.add(img_path)\n                    break\n    \n    print(f\"Original images copied to {output_base_dir}\")\n    print(f\"Total unique images copied: {len(copied_images)}\")\n\ndef train_enhanced_fastgan(generator, discriminator, dataloader, num_epochs, device='cuda', \n                          lambda_rec=10.0, save_interval=100):\n    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    os.makedirs('training_progress', exist_ok=True)\n    \n    for epoch in range(num_epochs):\n        for i, (real_imgs, _) in enumerate(dataloader):\n            real_imgs = real_imgs.to(device)\n            \n            losses, fake_imgs = enhanced_train_step(\n                real_imgs, generator, discriminator,\n                g_optimizer, d_optimizer, device, lambda_rec\n            )\n            \n            if i % 100 == 0:\n                print(f'Epoch [{epoch}/{num_epochs}], Batch [{i}], '\n                      f'D_loss: {losses[\"d_loss\"]:.4f}, '\n                      f'D_adv: {losses[\"d_loss_adv\"]:.4f}, '\n                      f'D_rec: {losses[\"d_loss_rec\"]:.4f}, '\n                      f'G_loss: {losses[\"g_loss\"]:.4f}')\n                \n                # Save sample images\n                if i % save_interval == 0:\n                    save_image(fake_imgs[:16] * 0.5 + 0.5,\n                             f'training_progress/epoch_{epoch}_batch_{i}.png',\n                             nrow=4, normalize=False)\n    \n    return generator, discriminator\n\ndef generate_synthetic_images(generator, classifier, num_classes, num_images_per_class,\n                            device='cuda', batch_size=64, output_dir='synthetic_images'):\n    os.makedirs(output_dir, exist_ok=True)\n    generator.eval()\n    \n    with torch.no_grad():\n        for class_idx in range(num_classes):\n            class_dir = os.path.join(output_dir, f'class_{class_idx}')\n            os.makedirs(class_dir, exist_ok=True)\n            \n            num_generated = 0\n            while num_generated < num_images_per_class:\n                # Generate images\n                z = torch.randn(batch_size, 256, 1, 1, device=device)\n                fake_imgs = generator(z)\n                \n                # Filter images using classifier\n                valid_mask = classifier.classify_synthetic_images(fake_imgs)\n                valid_images = fake_imgs[valid_mask]\n                \n                # Save valid images\n                for idx, img in enumerate(valid_images):\n                    if num_generated >= num_images_per_class:\n                        break\n                    save_image(img * 0.5 + 0.5,\n                             os.path.join(class_dir, f'synthetic_{num_generated}.png'))\n                    num_generated += 1\n                \n                print(f'Class {class_idx}: Generated {num_generated}/{num_images_per_class} images')\n\ndef main():\n    # Set device and random seeds\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    torch.manual_seed(42)\n    np.random.seed(42)\n    \n    # Dataset parameters\n    csv_file = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n    img_dirs = ['/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2']\n    \n    # Data preprocessing\n    transform = transforms.Compose([\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    # Initialize dataset and dataloader\n    dataset = HAM10000Dataset(csv_file, img_dirs, transform=transform, device=device)\n    num_classes = len(dataset.label_encoder.classes_)\n    print(f\"Number of classes: {num_classes}\")\n    print(\"Class labels:\", dataset.label_encoder.classes_)\n    \n    batch_size = 64\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    \n    # Initialize models\n    generator = EnhancedFASTGANGenerator(latent_dim=256, output_size=64).to(device)\n    discriminator = EnhancedFASTGANDiscriminator(input_size=64).to(device)\n    classifier = SyntheticImageClassifier(num_classes=num_classes, device=device)\n    \n    # Training parameters\n    num_epochs = 100\n    print(\"Starting training...\")\n    \n    # Train the model\n    generator, discriminator = train_enhanced_fastgan(\n        generator, discriminator, dataloader, \n        num_epochs=num_epochs, device=device\n    )\n    \n    # Generate synthetic images for each class\n    print(\"Generating synthetic images...\")\n    generate_synthetic_images(\n        generator, classifier, \n        num_classes=num_classes,\n        num_images_per_class=1000,  # Adjust as needed\n        device=device,\n        output_dir='synthetic_images'\n    )\n    \n    # Plot distribution comparison\n    plot_data_distribution_comparison(csv_file, 'synthetic_images')\n    \n    print(\"Training and generation complete!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T08:16:24.208489Z","iopub.execute_input":"2025-01-06T08:16:24.208769Z","iopub.status.idle":"2025-01-06T08:16:34.325098Z","shell.execute_reply.started":"2025-01-06T08:16:24.208747Z","shell.execute_reply":"2025-01-06T08:16:34.323611Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of classes: 7\nClass labels: ['akiec' 'bcc' 'bkl' 'df' 'mel' 'nv' 'vasc']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_v2_s-dd5fe13b.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_s-dd5fe13b.pth\n100%|██████████| 82.7M/82.7M [00:00<00:00, 94.6MB/s]\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth\" to /root/.cache/torch/hub/checkpoints/shufflenetv2_x1-5666bf0f80.pth\n100%|██████████| 8.79M/8.79M [00:00<00:00, 61.5MB/s]","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dd52bef97ec8>\u001b[0m in \u001b[0;36m<cell line: 440>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-dd52bef97ec8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     generator, discriminator = train_enhanced_fastgan(\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-dd52bef97ec8>\u001b[0m in \u001b[0;36mtrain_enhanced_fastgan\u001b[0;34m(generator, discriminator, dataloader, num_epochs, device, lambda_rec, save_interval)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mreal_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             losses, fake_imgs = enhanced_train_step(\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0mreal_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_rec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-dd52bef97ec8>\u001b[0m in \u001b[0;36menhanced_train_step\u001b[0;34m(real_imgs, generator, discriminator, g_optimizer, d_optimizer, device, lambda_rec)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Generate fake images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mfake_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0mfake_validity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-dd52bef97ec8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mx1_sle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msle1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_sle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mx2_sle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msle2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_sle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-dd52bef97ec8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, skip_x)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Content pathway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_fc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent_fc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 454\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    455\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 512, 1, 1], expected input[64, 1024, 1, 1] to have 512 channels, but got 1024 channels instead"],"ename":"RuntimeError","evalue":"Given groups=1, weight of size [512, 512, 1, 1], expected input[64, 1024, 1, 1] to have 512 channels, but got 1024 channels instead","output_type":"error"}],"execution_count":1}]}