{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nimport shutil\n\nclass HAM10000Dataset(Dataset):\n    def __init__(self, csv_file, img_dirs, transform=None, device='cuda'):\n        self.data = pd.read_csv(csv_file)\n        self.img_dirs = img_dirs\n        self.transform = transform\n        self.device = device\n        \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        self.data['encoded_label'] = self.label_encoder.fit_transform(self.data['dx'])\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n        for img_dir in self.img_dirs:\n            img_path = os.path.join(img_dir, img_name)\n            if os.path.exists(img_path):\n                image = Image.open(img_path).convert('RGB')\n                if self.transform:\n                    image = self.transform(image)\n                label = self.data.iloc[idx]['encoded_label']\n                return image, label\n        raise FileNotFoundError(f\"Image {img_name} not found in directories {self.img_dirs}\")\n\nclass EnhancedSLEBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(EnhancedSLEBlock, self).__init__()\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        \n        # Content branch - adjusted channel dimensions\n        self.content_fc1 = nn.Conv2d(in_channels, out_channels, 1)\n        self.content_fc2 = nn.Conv2d(out_channels, out_channels, 1)\n        \n        # Style branch\n        self.style_modulation = nn.Sequential(\n            nn.Conv2d(out_channels, out_channels, 1),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n        \n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.beta = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, skip_x):\n        # Content pathway\n        content = self.global_pool(x)\n        content = F.relu(self.content_fc1(content))\n        content = self.content_fc2(content)\n        content = torch.sigmoid(content)\n        \n        # Style pathway\n        style = self.style_modulation(skip_x)\n        \n        # Combine content and style\n        output = skip_x * content  # Content modulation\n        output = output + self.gamma * style + self.beta  # Style modulation\n        return output\n\nclass EnhancedFASTGANGenerator(nn.Module):\n    def __init__(self, latent_dim=256, ngf=32, output_size=64):\n        super(EnhancedFASTGANGenerator, self).__init__()\n        self.output_size = output_size\n        \n        self.initial = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True)\n        )\n        \n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True)\n        )\n        \n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True)\n        )\n        \n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(ngf, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n        \n        self.sle1 = EnhancedSLEBlock(ngf * 8, ngf * 4)  # Changed input and output channels\n        self.sle2 = EnhancedSLEBlock(ngf * 4, ngf * 2)  # Changed input and output channels\n\n    def forward(self, z):\n        x0 = self.initial(z)\n        x1 = self.layer1(x0)\n        x1_sle = self.sle1(x0, x1)\n        x2 = self.layer2(x1_sle)\n        x2_sle = self.sle2(x1_sle, x2)\n        x3 = self.layer3(x2_sle)\n        x4 = self.layer4(x3)\n        return x4\n\nclass EnhancedFASTGANDiscriminator(nn.Module):\n    def __init__(self, ndf=64, input_size=64):\n        super(EnhancedFASTGANDiscriminator, self).__init__()\n        self.input_size = input_size\n        \n        # Shared feature extractor\n        self.features = nn.Sequential(\n            nn.Conv2d(3, ndf, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2)\n        )\n        \n        # Discriminator head\n        self.discriminator = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(ndf * 8, 1, 1),\n            nn.Flatten(),\n            nn.Sigmoid()\n        )\n        \n        # Decoder for self-supervision\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(ndf * 8, ndf * 4, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf * 4, ndf * 2, 4, 2, 1),\n            nn.BatchNorm2d(ndf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf * 2, ndf, 4, 2, 1),\n            nn.BatchNorm2d(ndf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ndf, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        features = self.features(x)\n        validity = self.discriminator(features)\n        reconstruction = self.decoder(features)\n        return validity, reconstruction\n\nclass SyntheticImageClassifier:\n    def __init__(self, num_classes, device='cuda'):\n        self.device = device\n        \n        # EfficientNetV2\n        self.efficientnet = models.efficientnet_v2_s(pretrained=True)\n        self.efficientnet.classifier[1] = nn.Linear(self.efficientnet.classifier[1].in_features, num_classes)\n        self.efficientnet = self.efficientnet.to(device)\n        \n        # ShuffleNetV2\n        self.shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n        self.shufflenet.fc = nn.Linear(self.shufflenet.fc.in_features, num_classes)\n        self.shufflenet = self.shufflenet.to(device)\n        \n        # Transformation for input images\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    \n    def classify_synthetic_images(self, synthetic_images):\n        resized_images = F.interpolate(synthetic_images, size=(224, 224), mode='bilinear', align_corners=False)\n        normalized_images = (resized_images - resized_images.min()) / (resized_images.max() - resized_images.min())\n        normalized_images = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(normalized_images)\n        \n        with torch.no_grad():\n            efficientnet_preds = self.efficientnet(normalized_images)\n            shufflenet_preds = self.shufflenet(normalized_images)\n        \n        efficientnet_classes = torch.argmax(efficientnet_preds, dim=1)\n        shufflenet_classes = torch.argmax(shufflenet_preds, dim=1)\n        \n        agreed_classification_mask = (efficientnet_classes == shufflenet_classes)\n        \n        return agreed_classification_mask\n\ndef enhanced_train_step(real_imgs, generator, discriminator, g_optimizer, d_optimizer, \n                       device, lambda_rec=10.0):\n    batch_size = real_imgs.size(0)\n    \n    # Train Discriminator\n    d_optimizer.zero_grad()\n    \n    # Real images\n    real_validity, real_reconstruction = discriminator(real_imgs)\n    \n    # Generate fake images\n    z = torch.randn(batch_size, 256, 1, 1, device=device)\n    fake_imgs = generator(z)\n    fake_validity, _ = discriminator(fake_imgs.detach())\n    \n    # Hinge loss\n    d_loss_real = torch.mean(F.relu(1.0 - real_validity))\n    d_loss_fake = torch.mean(F.relu(1.0 + fake_validity))\n    d_loss_adv = d_loss_real + d_loss_fake\n    \n    # Reconstruction loss for self-supervision\n    d_loss_rec = F.mse_loss(real_reconstruction, real_imgs)\n    \n    # Total discriminator loss\n    d_loss = d_loss_adv + lambda_rec * d_loss_rec\n    \n    d_loss.backward()\n    d_optimizer.step()\n    \n    # Train Generator\n    g_optimizer.zero_grad()\n    \n    fake_validity, _ = discriminator(fake_imgs)\n    g_loss = -torch.mean(fake_validity)  # Hinge loss for generator\n    \n    g_loss.backward()\n    g_optimizer.step()\n    \n    return {\n        'd_loss': d_loss.item(),\n        'd_loss_adv': d_loss_adv.item(),\n        'd_loss_rec': d_loss_rec.item(),\n        'g_loss': g_loss.item()\n    }, fake_imgs\n\ndef plot_data_distribution_comparison(original_csv, synthetic_images_dir):\n    metadata = pd.read_csv(original_csv)\n    original_class_counts = metadata['dx'].value_counts()\n    synthetic_class_counts = {}\n    label_encoder = LabelEncoder()\n    label_encoder.fit(metadata['dx'])\n    \n    for class_name in label_encoder.classes_:\n        class_dir = os.path.join(synthetic_images_dir, class_name)\n        if os.path.exists(class_dir):\n            synthetic_class_counts[class_name] = len([f for f in os.listdir(class_dir) \n                                                    if f.endswith(('.png', '.jpg'))])\n        else:\n            synthetic_class_counts[class_name] = 0\n    \n    synthetic_class_counts = pd.Series(synthetic_class_counts)\n    \n    plt.figure(figsize=(15, 6))\n    x = np.arange(len(original_class_counts))\n    width = 0.4\n    \n    plt.bar(x - width/2, original_class_counts.values, width, label='Original Dataset', color='blue', alpha=0.7)\n    plt.bar(x + width/2, synthetic_class_counts.values, width, label='Synthetic Images', color='orange', alpha=0.7)\n    \n    plt.title('Comparison of Original HAM10000 Dataset and Synthetic Images', fontsize=16)\n    plt.xlabel('Skin Lesion Type', fontsize=14)\n    plt.ylabel('Number of Samples', fontsize=14)\n    plt.xticks(x, original_class_counts.index, rotation=90, ha='right')\n    plt.legend()\n    \n    for i, (orig, synth) in enumerate(zip(original_class_counts.values, synthetic_class_counts.values)):\n        plt.text(i - width/2, orig + 50, str(int(orig)), ha='center', va='bottom', fontsize=8)\n        plt.text(i + width/2, synth + 50, str(int(synth)), ha='center', va='bottom', fontsize=8)\n    \n    plt.tight_layout()\n    plt.savefig('dataset_distribution_comparison.png')\n    plt.close()\n\ndef copy_original_images_by_class(csv_file, img_dirs, output_base_dir='synthetic_images'):\n    metadata = pd.read_csv(csv_file)\n    os.makedirs(output_base_dir, exist_ok=True)\n    copied_images = set()\n    \n    for class_name in metadata['dx'].unique():\n        class_output_dir = os.path.join(output_base_dir, class_name)\n        os.makedirs(class_output_dir, exist_ok=True)\n        \n        class_metadata = metadata[metadata['dx'] == class_name]\n        \n        for _, row in class_metadata.iterrows():\n            img_filename = row['image_id'] + '.jpg'\n            \n            for img_dir in img_dirs:\n                img_path = os.path.join(img_dir, img_filename)\n                \n                if os.path.exists(img_path):\n                    dest_path = os.path.join(class_output_dir, img_filename)\n                    \n                    if img_path not in copied_images:\n                        shutil.copy2(img_path, dest_path)\n                        copied_images.add(img_path)\n                    break\n    \n    print(f\"Original images copied to {output_base_dir}\")\n    print(f\"Total unique images copied: {len(copied_images)}\")\n\ndef train_enhanced_fastgan(generator, discriminator, dataloader, num_epochs, device='cuda', \n                          lambda_rec=10.0, save_interval=100):\n    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    os.makedirs('training_progress', exist_ok=True)\n    \n    for epoch in range(num_epochs):\n        for i, (real_imgs, _) in enumerate(dataloader):\n            real_imgs = real_imgs.to(device)\n            \n            losses, fake_imgs = enhanced_train_step(\n                real_imgs, generator, discriminator,\n                g_optimizer, d_optimizer, device, lambda_rec\n            )\n            \n            if i % 100 == 0:\n                print(f'Epoch [{epoch}/{num_epochs}], Batch [{i}], '\n                      f'D_loss: {losses[\"d_loss\"]:.4f}, '\n                      f'D_adv: {losses[\"d_loss_adv\"]:.4f}, '\n                      f'D_rec: {losses[\"d_loss_rec\"]:.4f}, '\n                      f'G_loss: {losses[\"g_loss\"]:.4f}')\n                \n                # Save sample images\n                if i % save_interval == 0:\n                    save_image(fake_imgs[:16] * 0.5 + 0.5,\n                             f'training_progress/epoch_{epoch}_batch_{i}.png',\n                             nrow=4, normalize=False)\n    \n    return generator, discriminator\n\ndef generate_synthetic_images(generator, classifier, num_classes, num_images_per_class,\n                            device='cuda', batch_size=64, output_dir='synthetic_images'):\n    os.makedirs(output_dir, exist_ok=True)\n    generator.eval()\n    \n    with torch.no_grad():\n        for class_idx in range(num_classes):\n            class_dir = os.path.join(output_dir, f'class_{class_idx}')\n            os.makedirs(class_dir, exist_ok=True)\n            \n            num_generated = 0\n            while num_generated < num_images_per_class:\n                # Generate images\n                z = torch.randn(batch_size, 256, 1, 1, device=device)\n                fake_imgs = generator(z)\n                \n                # Filter images using classifier\n                valid_mask = classifier.classify_synthetic_images(fake_imgs)\n                valid_images = fake_imgs[valid_mask]\n                \n                # Save valid images\n                for idx, img in enumerate(valid_images):\n                    if num_generated >= num_images_per_class:\n                        break\n                    save_image(img * 0.5 + 0.5,\n                             os.path.join(class_dir, f'synthetic_{num_generated}.png'))\n                    num_generated += 1\n                \n                print(f'Class {class_idx}: Generated {num_generated}/{num_images_per_class} images')\n\ndef main():\n    # Set device and random seeds\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    torch.manual_seed(42)\n    np.random.seed(42)\n    \n    # Dataset parameters\n    csv_file = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n    img_dirs = ['/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2']\n    \n    # Data preprocessing\n    transform = transforms.Compose([\n        transforms.Resize((64, 64)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    # Initialize dataset and dataloader\n    dataset = HAM10000Dataset(csv_file, img_dirs, transform=transform, device=device)\n    num_classes = len(dataset.label_encoder.classes_)\n    print(f\"Number of classes: {num_classes}\")\n    print(\"Class labels:\", dataset.label_encoder.classes_)\n    \n    batch_size = 64\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    \n    # Initialize models\n    generator = EnhancedFASTGANGenerator(latent_dim=256, output_size=64).to(device)\n    discriminator = EnhancedFASTGANDiscriminator(input_size=64).to(device)\n    classifier = SyntheticImageClassifier(num_classes=num_classes, device=device)\n    \n    # Training parameters\n    num_epochs = 100\n    print(\"Starting training...\")\n    \n    # Train the model\n    generator, discriminator = train_enhanced_fastgan(\n        generator, discriminator, dataloader, \n        num_epochs=num_epochs, device=device\n    )\n    \n    # Generate synthetic images for each class\n    print(\"Generating synthetic images...\")\n    generate_synthetic_images(\n        generator, classifier, \n        num_classes=num_classes,\n        num_images_per_class=1000,  # Adjust as needed\n        device=device,\n        output_dir='synthetic_images'\n    )\n    \n    # Plot distribution comparison\n    plot_data_distribution_comparison(csv_file, 'synthetic_images')\n    \n    print(\"Training and generation complete!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T08:28:42.411765Z","iopub.execute_input":"2025-01-06T08:28:42.412127Z","iopub.status.idle":"2025-01-06T08:29:11.798458Z","shell.execute_reply.started":"2025-01-06T08:28:42.412097Z","shell.execute_reply":"2025-01-06T08:29:11.797026Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of classes: 7\nClass labels: ['akiec' 'bcc' 'bkl' 'df' 'mel' 'nv' 'vasc']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_V2_S_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X1_0_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X1_0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [0/100], Batch [0], D_loss: 6.4733, D_adv: 2.0022, D_rec: 0.4471, G_loss: -0.5366\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6bf5e695352b>\u001b[0m in \u001b[0;36m<cell line: 440>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-6bf5e695352b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     generator, discriminator = train_enhanced_fastgan(\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-6bf5e695352b>\u001b[0m in \u001b[0;36mtrain_enhanced_fastgan\u001b[0;34m(generator, discriminator, dataloader, num_epochs, device, lambda_rec, save_interval)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mreal_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3}]}