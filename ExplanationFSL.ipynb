{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTUNet++ Architecture Overview\n",
    "\n",
    "## High-Level Architecture\n",
    "1. Input Processing:\n",
    "   - Query image and support set images are processed through a CNN backbone\n",
    "   - Feature maps (Fmap âˆˆ â„aÃ—hÃ—w) are extracted for each image\n",
    "\n",
    "2. Main Components:\n",
    "   - CNN Backbone (Modified ResNet-18)\n",
    "   - Pattern Extractor Module\n",
    "   - Pairwise Matching Module (MLP-based)\n",
    "\n",
    "## Pattern Extractor Module Architecture\n",
    "1. Feature Processing:\n",
    "   - 1Ã—1 convolution layer followed by ReLU activation\n",
    "   - Dimensionality reduction from a to b\n",
    "   - Flattening: Fmap' âˆˆ â„bÃ—v (v = hÃ—w)\n",
    "   - Integration with learnable positional embedding (Pl)\n",
    "\n",
    "2. Attention Mechanism:\n",
    "   - Iterative process (R times)\n",
    "   - Uses Gated Recurrent Unit with Skip Connections (GRUsc)\n",
    "   - Pattern updates: K(r+1) = GRUsc(W(r), K(r))\n",
    "   - Attention calculation through normalization function\n",
    "\n",
    "3. Pattern Processing:\n",
    "   - Self-attention mechanism over spatial dimensions\n",
    "   - Dot-product similarity calculation\n",
    "   - Pattern refinement using GRUsc\n",
    "   - Attention map adjustment using Hadamard product\n",
    "\n",
    "## Configuration Details\n",
    "\n",
    "### CNN Backbone (ResNet-18 Modifications):\n",
    "- Removed first two downsampling layers\n",
    "- First conv layer: 7Ã—7 â†’ 3Ã—3\n",
    "- Output feature maps: 512\n",
    "- Fixed parameters during training\n",
    "\n",
    "### Pattern Extractor Module:\n",
    "- GRUsc hidden dimension: 256\n",
    "- Update iterations: 3\n",
    "- Number of patterns: 7\n",
    "- Networks gq and gM: 3 fully connected layers with ReLU\n",
    "\n",
    "### Training Configuration:\n",
    "1. Initial Phase:\n",
    "   - Learning rate: 10â»â´\n",
    "   - Rate reduction: 10Ã— at epoch 40\n",
    "   - Total epochs: 150\n",
    "\n",
    "2. Fine-tuning Phase:\n",
    "   - CNN and pattern extractor learning rate: 10â»âµ\n",
    "   - 20 iterations\n",
    "   - 500 episodes per epoch for 2-way tasks\n",
    "   - Other components: Initial learning rate 10â»â´\n",
    "   - Rate reduction: 10Ã— at epoch 10\n",
    "\n",
    "### Implementation Details:\n",
    "- Framework: PyTorch\n",
    "- Optimizer: AdaBelief\n",
    "- Input image size: 80Ã—80\n",
    "- Data augmentation: Random flipping and affine transformations\n",
    "- Evaluation: 2000 episodes of 2-way classification\n",
    "- Support images: N = 5 or 10\n",
    "- Query images: 15 per class\n",
    "\n",
    "## Training Process Flow\n",
    "1. Task-based training of backbone CNN\n",
    "2. Independent training of attention module\n",
    "3. Training of few-shot classifier\n",
    "4. Model selection based on validation performance (2,000 episodes)\n",
    "\n",
    "## Mathematical Formulations\n",
    "1. Feature Extraction:\n",
    "   - Fmap = fðœ™(x) âˆˆ â„aÃ—hÃ—w\n",
    "\n",
    "2. Pattern Attention:\n",
    "   - Att = fpe(Fmap) âˆˆ â„uÃ—v\n",
    "\n",
    "3. Similarity Scoring:\n",
    "   - score(Oq, Om) = Ïƒ(fðœƒ([Oq, Om]))\n",
    "\n",
    "4. Classification:\n",
    "   - m* = argmax_m score(Oq, Om)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flowchart TD\n",
    "    subgraph Input\n",
    "        QI[Query Image]\n",
    "        SS[Support Set Images]\n",
    "    end\n",
    "\n",
    "    subgraph CNN[\"CNN Backbone (ResNet-18)\"]\n",
    "        F1[Feature Extraction]\n",
    "    end\n",
    "\n",
    "    subgraph PE[\"Pattern Extractor Module\"]\n",
    "        C1[1x1 Conv + ReLU]\n",
    "        FT[Flatten Operation]\n",
    "        PE1[Positional Embedding]\n",
    "        AT1[Self-Attention]\n",
    "        GRU[GRU with Skip Connections]\n",
    "        AGG[Attention Aggregation]\n",
    "        AP[Average Pooling]\n",
    "    end\n",
    "\n",
    "    subgraph PM[\"Pairwise Matching\"]\n",
    "        CON[Feature Concatenation]\n",
    "        MLP[Multi-Layer Perceptron]\n",
    "        SC[Similarity Score]\n",
    "    end\n",
    "\n",
    "    %% Main flow\n",
    "    QI --> F1\n",
    "    SS --> F1\n",
    "    F1 --> |Fmap âˆˆ â„aÃ—hÃ—w| C1\n",
    "    C1 --> |Reduced Dim| FT\n",
    "    FT --> |Fmap' âˆˆ â„bÃ—v| PE1\n",
    "    PE1 --> AT1\n",
    "    AT1 --> |Att'| GRU\n",
    "    GRU --> |K(r+1)| AT1\n",
    "    GRU --> |Final Attention| AGG\n",
    "    AGG --> |Att''| AP\n",
    "    AP --> |O| CON\n",
    "    CON --> MLP\n",
    "    MLP --> |score| SC\n",
    "\n",
    "    %% Iterative loop\n",
    "    AT1 --> |R iterations| AT1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTUNet++ Data Flow Process\n",
    "\n",
    "## 1. Input Processing\n",
    "- **Query Image (xq)**: Single image for classification\n",
    "- **Support Set (Ds)**: Set of labeled images for comparison\n",
    "  - M classes with N images per class\n",
    "  - Total: MÃ—N support images\n",
    "\n",
    "## 2. Feature Extraction (CNN Stage)\n",
    "1. **Input â†’ Feature Maps**\n",
    "   - CNN processes both query and support images\n",
    "   - Output: Fmap = fðœ™(x) âˆˆ â„aÃ—hÃ—w\n",
    "   - Uses modified ResNet-18 backbone\n",
    "\n",
    "## 3. Pattern Extractor Flow\n",
    "1. **Dimensionality Reduction**\n",
    "   - Input: Fmap âˆˆ â„aÃ—hÃ—w\n",
    "   - 1Ã—1 convolution + ReLU\n",
    "   - Output: Reduced dimension from a to b\n",
    "\n",
    "2. **Spatial Processing**\n",
    "   - Flatten operation: Fmap' âˆˆ â„bÃ—v (v = hÃ—w)\n",
    "   - Add positional embedding: Fmap' = Fmap' + Pl\n",
    "\n",
    "3. **Attention Mechanism (Iterative Process)**\n",
    "   - Input: Flattened features\n",
    "   - Pattern Generation:\n",
    "     1. Calculate similarity: gq(K(r)) gM(Fmap')\n",
    "     2. Apply normalization: Att(r) = ðœš(Att'(r))\n",
    "     3. Update patterns: K(r+1) = GRUsc(W(r), K(r))\n",
    "   - Iterations: R times\n",
    "   - Output: Final attention maps\n",
    "\n",
    "4. **Feature Aggregation**\n",
    "   - Aggregate attention: Att'' = 1/u Ã— Att(r)\n",
    "   - Average pooling: O = 1/(hÃ—w) Ã— âˆ‘Att''ij Ã— Fmapij\n",
    "\n",
    "## 4. Pairwise Matching Flow\n",
    "1. **Feature Processing**\n",
    "   - Query features: Oq\n",
    "   - Support features: Om (averaged if N > 1)\n",
    "\n",
    "2. **Similarity Computation**\n",
    "   - Concatenate features: [Oq, Om]\n",
    "   - MLP processing: fðœƒ([Oq, Om])\n",
    "   - Output: similarity score\n",
    "\n",
    "3. **Classification**\n",
    "   - Compare scores across all support classes\n",
    "   - Select class with highest similarity score\n",
    "   - Final output: predicted class m*\n",
    "\n",
    "## Data Dimensions at Key Points\n",
    "1. Initial Features: â„aÃ—hÃ—w\n",
    "2. Reduced Features: â„bÃ—v\n",
    "3. Attention Maps: â„uÃ—v\n",
    "4. Final Features: â„b\n",
    "5. Similarity Scores: â„M (M = number of classes)\n",
    "\n",
    "## Key Transformations\n",
    "1. **Spatial â†’ Pattern Space**\n",
    "   - Feature maps â†’ Pattern attention\n",
    "   - Dimension: (aÃ—hÃ—w) â†’ (uÃ—v)\n",
    "\n",
    "2. **Pattern â†’ Classification Space**\n",
    "   - Pattern features â†’ Similarity scores\n",
    "   - Dimension: (b) â†’ (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision tqdm pillow\n",
    "!pip install adabelief-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from adabelief_pytorch import AdaBelief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modified first conv with stride 2\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        \n",
    "        # Modify layer1 and layer2 to remove downsampling\n",
    "        self.layer1 = self._modify_layer(resnet.layer1, stride=1)\n",
    "        self.layer2 = self._modify_layer(resnet.layer2, stride=1)\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def _modify_layer(self, layer, stride):\n",
    "        for block in layer:\n",
    "            block.conv1.stride = (stride, stride)\n",
    "            block.conv2.stride = (1, 1)\n",
    "            if block.downsample is not None:\n",
    "                block.downsample[0].stride = (stride, stride)\n",
    "        return layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        return x  # Output: [B, 512, H/16, W/16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim)\n",
    "        self.skip_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        h_new = self.gru(x, h)\n",
    "        skip = self.skip_proj(x)\n",
    "        return h_new + skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatternExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=512, hidden_dim=256, num_patterns=7, num_iterations=3):\n",
    "        super(PatternExtractor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patterns = num_patterns\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        # Initial feature processing\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, hidden_dim, 1, 1))\n",
    "        \n",
    "        # Attention gating parameter\n",
    "        self.attention_gate = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # GRU with skip connections\n",
    "        self.grusc = GRUWithSkipConnection(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Pattern networks\n",
    "        self.pattern_init = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_patterns * hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Attention networks\n",
    "        self.query_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.key_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initial feature processing\n",
    "        x = self.conv1x1(x)  # [B, hidden_dim, H, W]\n",
    "        x = x + self.positional_embedding\n",
    "        h, w = x.shape[-2:]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, H*W, hidden_dim]\n",
    "        \n",
    "        # Initialize patterns\n",
    "        patterns = self.pattern_init(x.mean(1))  # [B, num_patterns * hidden_dim]\n",
    "        patterns = patterns.view(batch_size, self.num_patterns, self.hidden_dim)\n",
    "        \n",
    "        # Iterative pattern refinement\n",
    "        h_state = torch.zeros(batch_size * self.num_patterns, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            # Self-attention\n",
    "            q = self.query_net(patterns.reshape(-1, self.hidden_dim))\n",
    "            k = self.key_net(x.reshape(-1, self.hidden_dim))\n",
    "            \n",
    "            # Compute attention scores\n",
    "            attn = torch.matmul(q.view(batch_size, self.num_patterns, -1),\n",
    "                              k.view(batch_size, -1, self.hidden_dim).transpose(1, 2))\n",
    "            attn = F.softmax(attn / (self.hidden_dim ** 0.5), dim=-1)\n",
    "            \n",
    "            # Apply attention gating\n",
    "            attn = attn * torch.sigmoid(self.attention_gate)\n",
    "            \n",
    "            # Update patterns\n",
    "            context = torch.bmm(attn, x)  # [B, num_patterns, hidden_dim]\n",
    "            \n",
    "            # GRU update\n",
    "            context_flat = context.reshape(-1, self.hidden_dim)\n",
    "            h_state = self.grusc(context_flat, h_state)\n",
    "            patterns = h_state.view(batch_size, self.num_patterns, self.hidden_dim)\n",
    "        \n",
    "        return patterns, attn.view(batch_size, self.num_patterns, h, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PairwiseMatchingModule(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(PairwiseMatchingModule, self).__init__()\n",
    "        self.matching_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query_features, support_features):\n",
    "        # Ensure both inputs are 2D\n",
    "        if query_features.dim() > 2:\n",
    "            query_features = query_features.mean(1)  # Average across any extra dimensions\n",
    "        if support_features.dim() > 2:\n",
    "            support_features = support_features.mean(1)  # Average across any extra dimensions\n",
    "            \n",
    "        batch_size = query_features.size(0)\n",
    "        \n",
    "        # Reshape for pairwise comparison\n",
    "        query_expanded = query_features.unsqueeze(1)  # [B, 1, H]\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([\n",
    "            query_expanded.expand(-1, support_features.size(0), -1),  # [B, S, H]\n",
    "            support_features.unsqueeze(0).expand(batch_size, -1, -1)  # [B, S, H]\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Get scores\n",
    "        scores = self.matching_net(combined.view(-1, combined.size(-1)))\n",
    "        return scores.view(batch_size, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class MTUNetPlusPlus(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(MTUNetPlusPlus, self).__init__()\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        self.pattern_extractor = PatternExtractor(in_channels=512, hidden_dim=hidden_dim)\n",
    "        self.matching_module = PairwiseMatchingModule(hidden_dim)\n",
    "        \n",
    "    def forward(self, query_img, support_imgs=None, return_features=False):\n",
    "        query_features = self.backbone(query_img)\n",
    "        query_patterns, query_attn = self.pattern_extractor(query_features)\n",
    "        \n",
    "        if support_imgs is None or return_features:\n",
    "            return query_patterns, query_attn\n",
    "        \n",
    "        support_features = self.backbone(support_imgs)\n",
    "        support_patterns, _ = self.pattern_extractor(support_features)\n",
    "        \n",
    "        # Ensure patterns are properly averaged before matching\n",
    "        query_features = query_patterns.mean(1)   # Average across patterns\n",
    "        support_features = support_patterns.mean(1)  # Average across patterns\n",
    "        \n",
    "        scores = self.matching_module(query_features, support_features)\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, device,\n",
    "                 checkpoint_dir='checkpoints'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        self.best_accuracy = 0.0\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    # Modify train_episode method\n",
    "    def train_episode(self, support_images, support_labels, query_images, query_labels):\n",
    "        self.model.train()\n",
    "        \n",
    "        # Reshape tensors to correct dimensions\n",
    "        support_images = support_images.squeeze(0)  # Remove extra batch dimension\n",
    "        query_images = query_images.squeeze(0)\n",
    "        support_labels = support_labels.squeeze(0)\n",
    "        query_labels = query_labels.squeeze(0)\n",
    "        \n",
    "        # Move to device\n",
    "        support_images = support_images.to(self.device)\n",
    "        support_labels = support_labels.to(self.device)\n",
    "        query_images = query_images.to(self.device)\n",
    "        query_labels = query_labels.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        scores = self.model(query_images, support_images)\n",
    "        loss = F.cross_entropy(scores, query_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        predictions = scores.max(1)[1]\n",
    "        accuracy = (predictions == query_labels).float().mean()\n",
    "        \n",
    "        return loss.item(), accuracy.item()\n",
    "        \n",
    "    \n",
    "    def validate_episode(self, support_images, support_labels, query_images, query_labels):\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            support_images = support_images.to(self.device)\n",
    "            support_labels = support_labels.to(self.device)\n",
    "            query_images = query_images.to(self.device)\n",
    "            query_labels = query_labels.to(self.device)\n",
    "            \n",
    "            scores = self.model(query_images, support_images)\n",
    "            loss = F.cross_entropy(scores, query_labels)\n",
    "            \n",
    "            predictions = scores.max(1)[1]\n",
    "            accuracy = (predictions == query_labels).float().mean()\n",
    "        \n",
    "        return loss.item(), accuracy.item()\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "        for batch_idx, (support_imgs, support_labels, query_imgs, query_labels) in pbar:\n",
    "            loss, accuracy = self.train_episode(support_imgs, support_labels, query_imgs, query_labels)\n",
    "            \n",
    "            total_loss += loss\n",
    "            total_accuracy += accuracy\n",
    "            \n",
    "            pbar.set_description(f'Epoch {epoch} | Loss: {loss:.4f} | Acc: {accuracy:.4f}')\n",
    "        \n",
    "        return total_loss / len(self.train_loader), total_accuracy / len(self.train_loader)\n",
    "    \n",
    "    def validate(self):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for support_imgs, support_labels, query_imgs, query_labels in tqdm(self.val_loader):\n",
    "            loss, accuracy = self.validate_episode(support_imgs, support_labels, query_imgs, query_labels)\n",
    "            total_loss += loss\n",
    "            total_accuracy += accuracy\n",
    "        \n",
    "        return total_loss / len(self.val_loader), total_accuracy / len(self.val_loader)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, accuracy):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        \n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_epoch = epoch\n",
    "            best_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "            torch.save(checkpoint, best_path)\n",
    "    \n",
    "    def _adjust_learning_rates(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if any(p in self.model.backbone.parameters() for p in param_group['params']):\n",
    "                param_group['lr'] = 1e-5\n",
    "            else:\n",
    "                param_group['lr'] = 1e-4\n",
    "    \n",
    "    def train_phase(self, num_epochs, phase_name=\"Training\"):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n{phase_name} - Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "            \n",
    "            val_loss, val_acc = self.validate()\n",
    "            print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            self.save_checkpoint(epoch, val_acc)\n",
    "            \n",
    "            if phase_name == \"Initial Training\" and epoch == 40:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.1\n",
    "       \n",
    "    def train_full(self):\n",
    "        \"\"\"Complete two-phase training process\"\"\"\n",
    "        # Phase 1: Initial training\n",
    "        print(\"Starting Phase 1: Initial Training\")\n",
    "        self.train_phase(num_epochs=150, phase_name=\"Initial Training\")\n",
    "        \n",
    "        # Phase 2: Fine-tuning\n",
    "        print(\"\\nStarting Phase 2: Fine-tuning\")\n",
    "        self._adjust_learning_rates()\n",
    "        self.train_phase(num_epochs=20, phase_name=\"Fine-tuning\")\n",
    "        \n",
    "        print(f\"\\nTraining completed! Best accuracy: {self.best_accuracy:.4f} at epoch {self.best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def split_classes(root_dir, val_split=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split classes into training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Path to the data directory\n",
    "        val_split: Proportion of classes to use for validation\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        train_classes: List of class names for training\n",
    "        val_classes: List of class names for validation\n",
    "    \"\"\"\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # Get all class folders\n",
    "    classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "    \n",
    "    # Randomly shuffle classes\n",
    "    random.shuffle(classes)\n",
    "    \n",
    "    # Split classes\n",
    "    split_idx = int(len(classes) * (1 - val_split))\n",
    "    train_classes = classes[:split_idx]\n",
    "    val_classes = classes[split_idx:]\n",
    "    \n",
    "    return train_classes, val_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EpisodeDataset(Dataset):\n",
    "    def __init__(self, root_dir, allowed_classes, transform=None, n_way=2, n_support=5, n_query=15):\n",
    "        \"\"\"\n",
    "        Dataset class for few-shot learning episodes\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Root directory containing class folders\n",
    "            allowed_classes: List of class names this dataset can use\n",
    "            transform: Image transformations\n",
    "            n_way: Number of classes per episode\n",
    "            n_support: Number of support examples per class\n",
    "            n_query: Number of query examples per class\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        \n",
    "        # Only use the allowed classes\n",
    "        self.classes = allowed_classes\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        # Build image paths dictionary\n",
    "        self.images_by_class = {}\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(root_dir, cls)\n",
    "            self.images_by_class[cls] = [\n",
    "                os.path.join(class_path, img) \n",
    "                for img in os.listdir(class_path) \n",
    "                if img.endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000  # Number of episodes per epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Rest of the __getitem__ method remains the same\n",
    "        episode_classes = random.sample(self.classes, self.n_way)\n",
    "        \n",
    "        support_images = []\n",
    "        support_labels = []\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "        \n",
    "        for label, cls in enumerate(episode_classes):\n",
    "            class_images = self.images_by_class[cls]\n",
    "            selected_images = random.sample(class_images, self.n_support + self.n_query)\n",
    "            \n",
    "            for img_path in selected_images[:self.n_support]:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                support_images.append(image)\n",
    "                support_labels.append(label)\n",
    "            \n",
    "            for img_path in selected_images[self.n_support:]:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                query_images.append(image)\n",
    "                query_labels.append(label)\n",
    "        \n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "        \n",
    "        return support_images, support_labels, query_images, query_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((80, 80)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Split classes into train and validation sets\n",
    "    data_path = '/kaggle/input/ham10000-and-gan/synthetic_images'  # Update this to your actual data path\n",
    "    train_classes, val_classes = split_classes(data_path)\n",
    "    \n",
    "    print(f\"Number of training classes: {len(train_classes)}\")\n",
    "    print(f\"Number of validation classes: {len(val_classes)}\")\n",
    "    \n",
    "    # Create datasets with respective class splits\n",
    "    train_dataset = EpisodeDataset(\n",
    "        root_dir=data_path,\n",
    "        allowed_classes=train_classes,\n",
    "        transform=transform,\n",
    "        n_way=2,\n",
    "        n_support=5,\n",
    "        n_query=15\n",
    "    )\n",
    "    \n",
    "    val_dataset = EpisodeDataset(\n",
    "        root_dir=data_path,\n",
    "        allowed_classes=val_classes,\n",
    "        transform=transform,\n",
    "        n_way=2,\n",
    "        n_support=5,\n",
    "        n_query=15\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Rest of the main function remains the same\n",
    "    model = MTUNetPlusPlus(hidden_dim=256).to(device)\n",
    "    optimizer = AdaBelief(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    trainer.train_full()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
