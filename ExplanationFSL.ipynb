{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTUNet++ Architecture Overview\n",
    "\n",
    "## High-Level Architecture\n",
    "1. Input Processing:\n",
    "   - Query image and support set images are processed through a CNN backbone\n",
    "   - Feature maps (Fmap ‚àà ‚Ñùa√óh√ów) are extracted for each image\n",
    "\n",
    "2. Main Components:\n",
    "   - CNN Backbone (Modified ResNet-18)\n",
    "   - Pattern Extractor Module\n",
    "   - Pairwise Matching Module (MLP-based)\n",
    "\n",
    "## Pattern Extractor Module Architecture\n",
    "1. Feature Processing:\n",
    "   - 1√ó1 convolution layer followed by ReLU activation\n",
    "   - Dimensionality reduction from a to b\n",
    "   - Flattening: Fmap' ‚àà ‚Ñùb√óv (v = h√ów)\n",
    "   - Integration with learnable positional embedding (Pl)\n",
    "\n",
    "2. Attention Mechanism:\n",
    "   - Iterative process (R times)\n",
    "   - Uses Gated Recurrent Unit with Skip Connections (GRUsc)\n",
    "   - Pattern updates: K(r+1) = GRUsc(W(r), K(r))\n",
    "   - Attention calculation through normalization function\n",
    "\n",
    "3. Pattern Processing:\n",
    "   - Self-attention mechanism over spatial dimensions\n",
    "   - Dot-product similarity calculation\n",
    "   - Pattern refinement using GRUsc\n",
    "   - Attention map adjustment using Hadamard product\n",
    "\n",
    "## Configuration Details\n",
    "\n",
    "### CNN Backbone (ResNet-18 Modifications):\n",
    "- Removed first two downsampling layers\n",
    "- First conv layer: 7√ó7 ‚Üí 3√ó3\n",
    "- Output feature maps: 512\n",
    "- Fixed parameters during training\n",
    "\n",
    "### Pattern Extractor Module:\n",
    "- GRUsc hidden dimension: 256\n",
    "- Update iterations: 3\n",
    "- Number of patterns: 7\n",
    "- Networks gq and gM: 3 fully connected layers with ReLU\n",
    "\n",
    "### Training Configuration:\n",
    "1. Initial Phase:\n",
    "   - Learning rate: 10‚Åª‚Å¥\n",
    "   - Rate reduction: 10√ó at epoch 40\n",
    "   - Total epochs: 150\n",
    "\n",
    "2. Fine-tuning Phase:\n",
    "   - CNN and pattern extractor learning rate: 10‚Åª‚Åµ\n",
    "   - 20 iterations\n",
    "   - 500 episodes per epoch for 2-way tasks\n",
    "   - Other components: Initial learning rate 10‚Åª‚Å¥\n",
    "   - Rate reduction: 10√ó at epoch 10\n",
    "\n",
    "### Implementation Details:\n",
    "- Framework: PyTorch\n",
    "- Optimizer: AdaBelief\n",
    "- Input image size: 80√ó80\n",
    "- Data augmentation: Random flipping and affine transformations\n",
    "- Evaluation: 2000 episodes of 2-way classification\n",
    "- Support images: N = 5 or 10\n",
    "- Query images: 15 per class\n",
    "\n",
    "## Training Process Flow\n",
    "1. Task-based training of backbone CNN\n",
    "2. Independent training of attention module\n",
    "3. Training of few-shot classifier\n",
    "4. Model selection based on validation performance (2,000 episodes)\n",
    "\n",
    "## Mathematical Formulations\n",
    "1. Feature Extraction:\n",
    "   - Fmap = fùúô(x) ‚àà ‚Ñùa√óh√ów\n",
    "\n",
    "2. Pattern Attention:\n",
    "   - Att = fpe(Fmap) ‚àà ‚Ñùu√óv\n",
    "\n",
    "3. Similarity Scoring:\n",
    "   - score(Oq, Om) = œÉ(fùúÉ([Oq, Om]))\n",
    "\n",
    "4. Classification:\n",
    "   - m* = argmax_m score(Oq, Om)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flowchart TD\n",
    "    subgraph Input\n",
    "        QI[Query Image]\n",
    "        SS[Support Set Images]\n",
    "    end\n",
    "\n",
    "    subgraph CNN[\"CNN Backbone (ResNet-18)\"]\n",
    "        F1[Feature Extraction]\n",
    "    end\n",
    "\n",
    "    subgraph PE[\"Pattern Extractor Module\"]\n",
    "        C1[1x1 Conv + ReLU]\n",
    "        FT[Flatten Operation]\n",
    "        PE1[Positional Embedding]\n",
    "        AT1[Self-Attention]\n",
    "        GRU[GRU with Skip Connections]\n",
    "        AGG[Attention Aggregation]\n",
    "        AP[Average Pooling]\n",
    "    end\n",
    "\n",
    "    subgraph PM[\"Pairwise Matching\"]\n",
    "        CON[Feature Concatenation]\n",
    "        MLP[Multi-Layer Perceptron]\n",
    "        SC[Similarity Score]\n",
    "    end\n",
    "\n",
    "    %% Main flow\n",
    "    QI --> F1\n",
    "    SS --> F1\n",
    "    F1 --> |Fmap ‚àà ‚Ñùa√óh√ów| C1\n",
    "    C1 --> |Reduced Dim| FT\n",
    "    FT --> |Fmap' ‚àà ‚Ñùb√óv| PE1\n",
    "    PE1 --> AT1\n",
    "    AT1 --> |Att'| GRU\n",
    "    GRU --> |K(r+1)| AT1\n",
    "    GRU --> |Final Attention| AGG\n",
    "    AGG --> |Att''| AP\n",
    "    AP --> |O| CON\n",
    "    CON --> MLP\n",
    "    MLP --> |score| SC\n",
    "\n",
    "    %% Iterative loop\n",
    "    AT1 --> |R iterations| AT1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTUNet++ Data Flow Process\n",
    "\n",
    "## 1. Input Processing\n",
    "- **Query Image (xq)**: Single image for classification\n",
    "- **Support Set (Ds)**: Set of labeled images for comparison\n",
    "  - M classes with N images per class\n",
    "  - Total: M√óN support images\n",
    "\n",
    "## 2. Feature Extraction (CNN Stage)\n",
    "1. **Input ‚Üí Feature Maps**\n",
    "   - CNN processes both query and support images\n",
    "   - Output: Fmap = fùúô(x) ‚àà ‚Ñùa√óh√ów\n",
    "   - Uses modified ResNet-18 backbone\n",
    "\n",
    "## 3. Pattern Extractor Flow\n",
    "1. **Dimensionality Reduction**\n",
    "   - Input: Fmap ‚àà ‚Ñùa√óh√ów\n",
    "   - 1√ó1 convolution + ReLU\n",
    "   - Output: Reduced dimension from a to b\n",
    "\n",
    "2. **Spatial Processing**\n",
    "   - Flatten operation: Fmap' ‚àà ‚Ñùb√óv (v = h√ów)\n",
    "   - Add positional embedding: Fmap' = Fmap' + Pl\n",
    "\n",
    "3. **Attention Mechanism (Iterative Process)**\n",
    "   - Input: Flattened features\n",
    "   - Pattern Generation:\n",
    "     1. Calculate similarity: gq(K(r)) gM(Fmap')\n",
    "     2. Apply normalization: Att(r) = ùúö(Att'(r))\n",
    "     3. Update patterns: K(r+1) = GRUsc(W(r), K(r))\n",
    "   - Iterations: R times\n",
    "   - Output: Final attention maps\n",
    "\n",
    "4. **Feature Aggregation**\n",
    "   - Aggregate attention: Att'' = 1/u √ó Att(r)\n",
    "   - Average pooling: O = 1/(h√ów) √ó ‚àëAtt''ij √ó Fmapij\n",
    "\n",
    "## 4. Pairwise Matching Flow\n",
    "1. **Feature Processing**\n",
    "   - Query features: Oq\n",
    "   - Support features: Om (averaged if N > 1)\n",
    "\n",
    "2. **Similarity Computation**\n",
    "   - Concatenate features: [Oq, Om]\n",
    "   - MLP processing: fùúÉ([Oq, Om])\n",
    "   - Output: similarity score\n",
    "\n",
    "3. **Classification**\n",
    "   - Compare scores across all support classes\n",
    "   - Select class with highest similarity score\n",
    "   - Final output: predicted class m*\n",
    "\n",
    "## Data Dimensions at Key Points\n",
    "1. Initial Features: ‚Ñùa√óh√ów\n",
    "2. Reduced Features: ‚Ñùb√óv\n",
    "3. Attention Maps: ‚Ñùu√óv\n",
    "4. Final Features: ‚Ñùb\n",
    "5. Similarity Scores: ‚ÑùM (M = number of classes)\n",
    "\n",
    "## Key Transformations\n",
    "1. **Spatial ‚Üí Pattern Space**\n",
    "   - Feature maps ‚Üí Pattern attention\n",
    "   - Dimension: (a√óh√ów) ‚Üí (u√óv)\n",
    "\n",
    "2. **Pattern ‚Üí Classification Space**\n",
    "   - Pattern features ‚Üí Similarity scores\n",
    "   - Dimension: (b) ‚Üí (M)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
