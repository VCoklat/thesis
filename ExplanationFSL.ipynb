{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTUNet++ Architecture Overview\n",
    "\n",
    "## High-Level Architecture\n",
    "1. Input Processing:\n",
    "   - Query image and support set images are processed through a CNN backbone\n",
    "   - Feature maps (Fmap âˆˆ â„aÃ—hÃ—w) are extracted for each image\n",
    "\n",
    "2. Main Components:\n",
    "   - CNN Backbone (Modified ResNet-18)\n",
    "   - Pattern Extractor Module\n",
    "   - Pairwise Matching Module (MLP-based)\n",
    "\n",
    "## Pattern Extractor Module Architecture\n",
    "1. Feature Processing:\n",
    "   - 1Ã—1 convolution layer followed by ReLU activation\n",
    "   - Dimensionality reduction from a to b\n",
    "   - Flattening: Fmap' âˆˆ â„bÃ—v (v = hÃ—w)\n",
    "   - Integration with learnable positional embedding (Pl)\n",
    "\n",
    "2. Attention Mechanism:\n",
    "   - Iterative process (R times)\n",
    "   - Uses Gated Recurrent Unit with Skip Connections (GRUsc)\n",
    "   - Pattern updates: K(r+1) = GRUsc(W(r), K(r))\n",
    "   - Attention calculation through normalization function\n",
    "\n",
    "3. Pattern Processing:\n",
    "   - Self-attention mechanism over spatial dimensions\n",
    "   - Dot-product similarity calculation\n",
    "   - Pattern refinement using GRUsc\n",
    "   - Attention map adjustment using Hadamard product\n",
    "\n",
    "## Configuration Details\n",
    "\n",
    "### CNN Backbone (ResNet-18 Modifications):\n",
    "- Removed first two downsampling layers\n",
    "- First conv layer: 7Ã—7 â†’ 3Ã—3\n",
    "- Output feature maps: 512\n",
    "- Fixed parameters during training\n",
    "\n",
    "### Pattern Extractor Module:\n",
    "- GRUsc hidden dimension: 256\n",
    "- Update iterations: 3\n",
    "- Number of patterns: 7\n",
    "- Networks gq and gM: 3 fully connected layers with ReLU\n",
    "\n",
    "### Training Configuration:\n",
    "1. Initial Phase:\n",
    "   - Learning rate: 10â»â´\n",
    "   - Rate reduction: 10Ã— at epoch 40\n",
    "   - Total epochs: 150\n",
    "\n",
    "2. Fine-tuning Phase:\n",
    "   - CNN and pattern extractor learning rate: 10â»âµ\n",
    "   - 20 iterations\n",
    "   - 500 episodes per epoch for 2-way tasks\n",
    "   - Other components: Initial learning rate 10â»â´\n",
    "   - Rate reduction: 10Ã— at epoch 10\n",
    "\n",
    "### Implementation Details:\n",
    "- Framework: PyTorch\n",
    "- Optimizer: AdaBelief\n",
    "- Input image size: 80Ã—80\n",
    "- Data augmentation: Random flipping and affine transformations\n",
    "- Evaluation: 2000 episodes of 2-way classification\n",
    "- Support images: N = 5 or 10\n",
    "- Query images: 15 per class\n",
    "\n",
    "## Training Process Flow\n",
    "1. Task-based training of backbone CNN\n",
    "2. Independent training of attention module\n",
    "3. Training of few-shot classifier\n",
    "4. Model selection based on validation performance (2,000 episodes)\n",
    "\n",
    "## Mathematical Formulations\n",
    "1. Feature Extraction:\n",
    "   - Fmap = fðœ™(x) âˆˆ â„aÃ—hÃ—w\n",
    "\n",
    "2. Pattern Attention:\n",
    "   - Att = fpe(Fmap) âˆˆ â„uÃ—v\n",
    "\n",
    "3. Similarity Scoring:\n",
    "   - score(Oq, Om) = Ïƒ(fðœƒ([Oq, Om]))\n",
    "\n",
    "4. Classification:\n",
    "   - m* = argmax_m score(Oq, Om)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flowchart TD\n",
    "    subgraph Input\n",
    "        QI[Query Image]\n",
    "        SS[Support Set Images]\n",
    "    end\n",
    "\n",
    "    subgraph CNN[\"CNN Backbone (ResNet-18)\"]\n",
    "        F1[Feature Extraction]\n",
    "    end\n",
    "\n",
    "    subgraph PE[\"Pattern Extractor Module\"]\n",
    "        C1[1x1 Conv + ReLU]\n",
    "        FT[Flatten Operation]\n",
    "        PE1[Positional Embedding]\n",
    "        AT1[Self-Attention]\n",
    "        GRU[GRU with Skip Connections]\n",
    "        AGG[Attention Aggregation]\n",
    "        AP[Average Pooling]\n",
    "    end\n",
    "\n",
    "    subgraph PM[\"Pairwise Matching\"]\n",
    "        CON[Feature Concatenation]\n",
    "        MLP[Multi-Layer Perceptron]\n",
    "        SC[Similarity Score]\n",
    "    end\n",
    "\n",
    "    %% Main flow\n",
    "    QI --> F1\n",
    "    SS --> F1\n",
    "    F1 --> |Fmap âˆˆ â„aÃ—hÃ—w| C1\n",
    "    C1 --> |Reduced Dim| FT\n",
    "    FT --> |Fmap' âˆˆ â„bÃ—v| PE1\n",
    "    PE1 --> AT1\n",
    "    AT1 --> |Att'| GRU\n",
    "    GRU --> |K(r+1)| AT1\n",
    "    GRU --> |Final Attention| AGG\n",
    "    AGG --> |Att''| AP\n",
    "    AP --> |O| CON\n",
    "    CON --> MLP\n",
    "    MLP --> |score| SC\n",
    "\n",
    "    %% Iterative loop\n",
    "    AT1 --> |R iterations| AT1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTUNet++ Data Flow Process\n",
    "\n",
    "## 1. Input Processing\n",
    "- **Query Image (xq)**: Single image for classification\n",
    "- **Support Set (Ds)**: Set of labeled images for comparison\n",
    "  - M classes with N images per class\n",
    "  - Total: MÃ—N support images\n",
    "\n",
    "## 2. Feature Extraction (CNN Stage)\n",
    "1. **Input â†’ Feature Maps**\n",
    "   - CNN processes both query and support images\n",
    "   - Output: Fmap = fðœ™(x) âˆˆ â„aÃ—hÃ—w\n",
    "   - Uses modified ResNet-18 backbone\n",
    "\n",
    "## 3. Pattern Extractor Flow\n",
    "1. **Dimensionality Reduction**\n",
    "   - Input: Fmap âˆˆ â„aÃ—hÃ—w\n",
    "   - 1Ã—1 convolution + ReLU\n",
    "   - Output: Reduced dimension from a to b\n",
    "\n",
    "2. **Spatial Processing**\n",
    "   - Flatten operation: Fmap' âˆˆ â„bÃ—v (v = hÃ—w)\n",
    "   - Add positional embedding: Fmap' = Fmap' + Pl\n",
    "\n",
    "3. **Attention Mechanism (Iterative Process)**\n",
    "   - Input: Flattened features\n",
    "   - Pattern Generation:\n",
    "     1. Calculate similarity: gq(K(r)) gM(Fmap')\n",
    "     2. Apply normalization: Att(r) = ðœš(Att'(r))\n",
    "     3. Update patterns: K(r+1) = GRUsc(W(r), K(r))\n",
    "   - Iterations: R times\n",
    "   - Output: Final attention maps\n",
    "\n",
    "4. **Feature Aggregation**\n",
    "   - Aggregate attention: Att'' = 1/u Ã— Att(r)\n",
    "   - Average pooling: O = 1/(hÃ—w) Ã— âˆ‘Att''ij Ã— Fmapij\n",
    "\n",
    "## 4. Pairwise Matching Flow\n",
    "1. **Feature Processing**\n",
    "   - Query features: Oq\n",
    "   - Support features: Om (averaged if N > 1)\n",
    "\n",
    "2. **Similarity Computation**\n",
    "   - Concatenate features: [Oq, Om]\n",
    "   - MLP processing: fðœƒ([Oq, Om])\n",
    "   - Output: similarity score\n",
    "\n",
    "3. **Classification**\n",
    "   - Compare scores across all support classes\n",
    "   - Select class with highest similarity score\n",
    "   - Final output: predicted class m*\n",
    "\n",
    "## Data Dimensions at Key Points\n",
    "1. Initial Features: â„aÃ—hÃ—w\n",
    "2. Reduced Features: â„bÃ—v\n",
    "3. Attention Maps: â„uÃ—v\n",
    "4. Final Features: â„b\n",
    "5. Similarity Scores: â„M (M = number of classes)\n",
    "\n",
    "## Key Transformations\n",
    "1. **Spatial â†’ Pattern Space**\n",
    "   - Feature maps â†’ Pattern attention\n",
    "   - Dimension: (aÃ—hÃ—w) â†’ (uÃ—v)\n",
    "\n",
    "2. **Pattern â†’ Classification Space**\n",
    "   - Pattern features â†’ Similarity scores\n",
    "   - Dimension: (b) â†’ (M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This code installs the necessary Python packages for the project.\n",
    "# - torch: A deep learning framework for building and training neural networks.\n",
    "# - torchvision: A package that provides datasets, model architectures, and image transformations for computer vision.\n",
    "# - tqdm: A library for adding progress bars to loops.\n",
    "# - pillow: A library for image processing.\n",
    "# - adabelief-pytorch: An optimizer that combines the benefits of Adam and RMSProp optimizers.\n",
    "!pip install torch torchvision tqdm pillow\n",
    "!pip install adabelief-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script sets up the necessary imports for a PyTorch-based deep learning project. \n",
    "It includes the following libraries and modules:\n",
    "\n",
    "- torch: The main PyTorch library.\n",
    "- torch.nn: A sub-library containing neural network layers and functions.\n",
    "- torch.nn.functional: A sub-library containing functional interfaces for neural network layers.\n",
    "- torchvision.models: Pre-trained models provided by the torchvision library.\n",
    "- torchvision.transforms: Common image transformations provided by the torchvision library.\n",
    "- torch.utils.data: Utilities for data handling, including Dataset and DataLoader classes.\n",
    "- numpy: A library for numerical operations.\n",
    "- PIL (Python Imaging Library): A library for opening, manipulating, and saving images.\n",
    "- os: A module for interacting with the operating system.\n",
    "- random: A module for generating random numbers.\n",
    "- tqdm: A library for creating progress bars.\n",
    "- adabelief_pytorch: The AdaBelief optimizer for PyTorch.\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from adabelief_pytorch import AdaBelief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ModifiedResNet18 is a custom neural network model based on ResNet-18 with the following modifications:\n",
    "- The first convolutional layer is modified to have a kernel size of 3 and a stride of 2.\n",
    "- The first two residual layers (layer1 and layer2) are modified to remove downsampling by setting their stride to 1.\n",
    "- All parameters are frozen to prevent them from being updated during training.\n",
    "\n",
    "Attributes:\n",
    "    conv1 (nn.Conv2d): Modified first convolutional layer.\n",
    "    bn1 (nn.BatchNorm2d): Batch normalization layer from the original ResNet-18.\n",
    "    relu (nn.ReLU): ReLU activation function from the original ResNet-18.\n",
    "    layer1 (nn.Sequential): Modified first residual layer with no downsampling.\n",
    "    layer2 (nn.Sequential): Modified second residual layer with no downsampling.\n",
    "    layer3 (nn.Sequential): Third residual layer from the original ResNet-18.\n",
    "    layer4 (nn.Sequential): Fourth residual layer from the original ResNet-18.\n",
    "\n",
    "Methods:\n",
    "    _modify_layer(layer, stride):\n",
    "        Modifies the given residual layer to set the stride of the first convolutional layer and the downsample layer (if present) to the specified stride.\n",
    "        \n",
    "    forward(x):\n",
    "        Defines the forward pass of the network.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, 3, H, W].\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [B, 512, H/16, W/16].\n",
    "\"\"\"\n",
    "\n",
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modified first conv with stride 2\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        \n",
    "        # Modify layer1 and layer2 to remove downsampling\n",
    "        self.layer1 = self._modify_layer(resnet.layer1, stride=1)\n",
    "        self.layer2 = self._modify_layer(resnet.layer2, stride=1)\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def _modify_layer(self, layer, stride):\n",
    "        for block in layer:\n",
    "            block.conv1.stride = (stride, stride)\n",
    "            block.conv2.stride = (1, 1)\n",
    "            if block.downsample is not None:\n",
    "                block.downsample[0].stride = (stride, stride)\n",
    "        return layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        return x  # Output: [B, 512, H/16, W/16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GRUWithSkipConnection is a custom neural network module that combines a GRU cell with a skip connection.\n",
    "\n",
    "Attributes:\n",
    "    gru (nn.GRUCell): A GRU cell that processes the input sequence.\n",
    "    skip_proj (nn.Linear): A linear layer that projects the input to the hidden dimension for the skip connection.\n",
    "\n",
    "Methods:\n",
    "    __init__(input_dim, hidden_dim):\n",
    "        Initializes the GRUWithSkipConnection module with the specified input and hidden dimensions.\n",
    "    \n",
    "    forward(x, h):\n",
    "        Performs a forward pass through the GRU cell and adds a skip connection.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The input tensor of shape (batch_size, input_dim).\n",
    "            h (Tensor): The hidden state tensor of shape (batch_size, hidden_dim).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor of shape (batch_size, hidden_dim) after applying the GRU cell and skip connection.\n",
    "\"\"\"\n",
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim)\n",
    "        self.skip_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        h_new = self.gru(x, h)\n",
    "        skip = self.skip_proj(x)\n",
    "        return h_new + skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PatternExtractor is a neural network module designed to extract patterns from input feature maps using iterative self-attention and GRU-based refinement.\n",
    "\n",
    "Attributes:\n",
    "    hidden_dim (int): The dimensionality of the hidden layers.\n",
    "    num_patterns (int): The number of patterns to extract.\n",
    "    num_iterations (int): The number of iterations for pattern refinement.\n",
    "    conv1x1 (nn.Conv2d): A 1x1 convolutional layer for initial feature processing.\n",
    "    positional_embedding (nn.Parameter): A learnable positional embedding added to the input features.\n",
    "    attention_gate (nn.Parameter): A learnable parameter for attention gating.\n",
    "    grusc (GRUWithSkipConnection): A GRU module with skip connections for pattern refinement.\n",
    "    pattern_init (nn.Sequential): A network for initializing patterns.\n",
    "    query_net (nn.Sequential): A network for generating query vectors for self-attention.\n",
    "    key_net (nn.Sequential): A network for generating key vectors for self-attention.\n",
    "\n",
    "Methods:\n",
    "    forward(x):\n",
    "        Forward pass of the PatternExtractor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input feature map of shape [B, in_channels, H, W].\n",
    "        \n",
    "        Returns:\n",
    "            patterns (torch.Tensor): Extracted patterns of shape [B, num_patterns, hidden_dim].\n",
    "            attn (torch.Tensor): Attention maps of shape [B, num_patterns, H, W].\n",
    "\"\"\"\n",
    "\n",
    "class PatternExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=512, hidden_dim=256, num_patterns=7, num_iterations=3):\n",
    "        super(PatternExtractor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patterns = num_patterns\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        # Initial feature processing\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, hidden_dim, 1, 1))\n",
    "        \n",
    "        # Attention gating parameter\n",
    "        self.attention_gate = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # GRU with skip connections\n",
    "        self.grusc = GRUWithSkipConnection(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Pattern networks\n",
    "        self.pattern_init = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_patterns * hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Attention networks\n",
    "        self.query_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.key_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initial feature processing\n",
    "        x = self.conv1x1(x)  # [B, hidden_dim, H, W]\n",
    "        x = x + self.positional_embedding\n",
    "        h, w = x.shape[-2:]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, H*W, hidden_dim]\n",
    "        \n",
    "        # Initialize patterns\n",
    "        patterns = self.pattern_init(x.mean(1))  # [B, num_patterns * hidden_dim]\n",
    "        patterns = patterns.view(batch_size, self.num_patterns, self.hidden_dim)\n",
    "        \n",
    "        # Iterative pattern refinement\n",
    "        h_state = torch.zeros(batch_size * self.num_patterns, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            # Self-attention\n",
    "            q = self.query_net(patterns.reshape(-1, self.hidden_dim))\n",
    "            k = self.key_net(x.reshape(-1, self.hidden_dim))\n",
    "            \n",
    "            # Compute attention scores\n",
    "            attn = torch.matmul(q.view(batch_size, self.num_patterns, -1),\n",
    "                              k.view(batch_size, -1, self.hidden_dim).transpose(1, 2))\n",
    "            attn = F.softmax(attn / (self.hidden_dim ** 0.5), dim=-1)\n",
    "            \n",
    "            # Apply attention gating\n",
    "            attn = attn * torch.sigmoid(self.attention_gate)\n",
    "            \n",
    "            # Update patterns\n",
    "            context = torch.bmm(attn, x)  # [B, num_patterns, hidden_dim]\n",
    "            \n",
    "            # GRU update\n",
    "            context_flat = context.reshape(-1, self.hidden_dim)\n",
    "            h_state = self.grusc(context_flat, h_state)\n",
    "            patterns = h_state.view(batch_size, self.num_patterns, self.hidden_dim)\n",
    "        \n",
    "        return patterns, attn.view(batch_size, self.num_patterns, h, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PairwiseMatchingModule is a neural network module designed for pairwise matching of query and support features.\n",
    "\n",
    "Args:\n",
    "    hidden_dim (int): The dimension of the hidden layers in the matching network.\n",
    "\n",
    "Attributes:\n",
    "    matching_net (nn.Sequential): A sequential neural network consisting of linear layers and ReLU activations, \n",
    "                                  which processes the concatenated query and support features to produce a matching score.\n",
    "\n",
    "Methods:\n",
    "    forward(query_features, support_features):\n",
    "        Computes the matching scores between query features and support features.\n",
    "\n",
    "        Args:\n",
    "            query_features (torch.Tensor): A tensor of shape [B, H] or [B, *, H] representing the query features.\n",
    "            support_features (torch.Tensor): A tensor of shape [S, H] or [S, *, H] representing the support features.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape [B, S] containing the matching scores for each query-support pair.\n",
    "\"\"\"\n",
    "\n",
    "class PairwiseMatchingModule(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(PairwiseMatchingModule, self).__init__()\n",
    "        self.matching_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query_features, support_features):\n",
    "        # Ensure both inputs are 2D\n",
    "        if query_features.dim() > 2:\n",
    "            query_features = query_features.mean(1)  # Average across any extra dimensions\n",
    "        if support_features.dim() > 2:\n",
    "            support_features = support_features.mean(1)  # Average across any extra dimensions\n",
    "            \n",
    "        batch_size = query_features.size(0)\n",
    "        \n",
    "        # Reshape for pairwise comparison\n",
    "        query_expanded = query_features.unsqueeze(1)  # [B, 1, H]\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([\n",
    "            query_expanded.expand(-1, support_features.size(0), -1),  # [B, S, H]\n",
    "            support_features.unsqueeze(0).expand(batch_size, -1, -1)  # [B, S, H]\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Get scores\n",
    "        scores = self.matching_net(combined.view(-1, combined.size(-1)))\n",
    "        return scores.view(batch_size, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MTUNetPlusPlus is a neural network model designed for few-shot learning tasks. It consists of a backbone network for feature extraction, a pattern extractor for identifying relevant patterns, and a pairwise matching module for comparing query and support images.\n",
    "\n",
    "Attributes:\n",
    "    backbone (nn.Module): The feature extraction backbone network, here a modified ResNet18.\n",
    "    pattern_extractor (PatternExtractor): Module to extract patterns from the features.\n",
    "    matching_module (PairwiseMatchingModule): Module to perform pairwise matching between query and support features.\n",
    "\n",
    "Methods:\n",
    "    __init__(hidden_dim=256):\n",
    "        Initializes the MTUNetPlusPlus model with the specified hidden dimension for the pattern extractor.\n",
    "    \n",
    "    forward(query_img, support_imgs=None, return_features=False):\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            query_img (Tensor): The query image tensor.\n",
    "            support_imgs (Tensor, optional): The support images tensor. Defaults to None.\n",
    "            return_features (bool, optional): If True, returns the extracted features instead of matching scores. Defaults to False.\n",
    "        \n",
    "        Returns:\n",
    "            If return_features is True, returns a tuple of query patterns and attention maps.\n",
    "            If support_imgs is provided, returns the matching scores between query and support images.\n",
    "\"\"\"\n",
    "\n",
    "class MTUNetPlusPlus(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(MTUNetPlusPlus, self).__init__()\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        self.pattern_extractor = PatternExtractor(in_channels=512, hidden_dim=hidden_dim)\n",
    "        self.matching_module = PairwiseMatchingModule(hidden_dim)\n",
    "        \n",
    "    def forward(self, query_img, support_imgs=None, return_features=False):\n",
    "        query_features = self.backbone(query_img)\n",
    "        query_patterns, query_attn = self.pattern_extractor(query_features)\n",
    "        \n",
    "        if support_imgs is None or return_features:\n",
    "            return query_patterns, query_attn\n",
    "        \n",
    "        support_features = self.backbone(support_imgs)\n",
    "        support_patterns, _ = self.pattern_extractor(support_features)\n",
    "        \n",
    "        # Ensure patterns are properly averaged before matching\n",
    "        query_features = query_patterns.mean(1)   # Average across patterns\n",
    "        support_features = support_patterns.mean(1)  # Average across patterns\n",
    "        \n",
    "        scores = self.matching_module(query_features, support_features)\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ModelTrainer class for training and validating a model with support and query images.\n",
    "\n",
    "Attributes:\n",
    "    model (torch.nn.Module): The model to be trained.\n",
    "    train_loader (DataLoader): DataLoader for the training data.\n",
    "    val_loader (DataLoader): DataLoader for the validation data.\n",
    "    optimizer (torch.optim.Optimizer): Optimizer for training the model.\n",
    "    device (torch.device): Device to run the model on (e.g., 'cpu' or 'cuda').\n",
    "    checkpoint_dir (str): Directory to save model checkpoints.\n",
    "    best_accuracy (float): Best validation accuracy achieved.\n",
    "    best_epoch (int): Epoch at which the best validation accuracy was achieved.\n",
    "\n",
    "Methods:\n",
    "    train_episode(support_images, support_labels, query_images, query_labels):\n",
    "        Trains the model for one episode using support and query images and labels.\n",
    "        \n",
    "    validate_episode(support_images, support_labels, query_images, query_labels):\n",
    "        Validates the model for one episode using support and query images and labels.\n",
    "        \n",
    "    train_epoch(epoch):\n",
    "        Trains the model for one epoch and returns the average loss and accuracy.\n",
    "        \n",
    "    validate():\n",
    "        Validates the model on the validation set and returns the average loss and accuracy.\n",
    "        \n",
    "    save_checkpoint(epoch, accuracy):\n",
    "        Saves a checkpoint of the model at the given epoch with the given accuracy.\n",
    "        \n",
    "    _adjust_learning_rates():\n",
    "        Adjusts the learning rates for different parts of the model.\n",
    "        \n",
    "    train_phase(num_epochs, phase_name=\"Training\"):\n",
    "        Trains the model for a specified number of epochs in a given phase.\n",
    "        \n",
    "    train_full():\n",
    "        Completes a two-phase training process: initial training and fine-tuning.\n",
    "\"\"\"\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, device,\n",
    "                 checkpoint_dir='checkpoints'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        self.best_accuracy = 0.0\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    # Modify train_episode method\n",
    "    def train_episode(self, support_images, support_labels, query_images, query_labels):\n",
    "        self.model.train()\n",
    "        \n",
    "        # Reshape tensors to correct dimensions\n",
    "        support_images = support_images.squeeze(0)  # Remove extra batch dimension\n",
    "        query_images = query_images.squeeze(0)\n",
    "        support_labels = support_labels.squeeze(0)\n",
    "        query_labels = query_labels.squeeze(0)\n",
    "        \n",
    "        # Move to device\n",
    "        support_images = support_images.to(self.device)\n",
    "        support_labels = support_labels.to(self.device)\n",
    "        query_images = query_images.to(self.device)\n",
    "        query_labels = query_labels.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        scores = self.model(query_images, support_images)\n",
    "        loss = F.cross_entropy(scores, query_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        predictions = scores.max(1)[1]\n",
    "        accuracy = (predictions == query_labels).float().mean()\n",
    "        \n",
    "        return loss.item(), accuracy.item()\n",
    "        \n",
    "    \n",
    "    def validate_episode(self, support_images, support_labels, query_images, query_labels):\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            support_images = support_images.to(self.device)\n",
    "            support_labels = support_labels.to(self.device)\n",
    "            query_images = query_images.to(self.device)\n",
    "            query_labels = query_labels.to(self.device)\n",
    "            \n",
    "            scores = self.model(query_images, support_images)\n",
    "            loss = F.cross_entropy(scores, query_labels)\n",
    "            \n",
    "            predictions = scores.max(1)[1]\n",
    "            accuracy = (predictions == query_labels).float().mean()\n",
    "        \n",
    "        return loss.item(), accuracy.item()\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n",
    "        for batch_idx, (support_imgs, support_labels, query_imgs, query_labels) in pbar:\n",
    "            loss, accuracy = self.train_episode(support_imgs, support_labels, query_imgs, query_labels)\n",
    "            \n",
    "            total_loss += loss\n",
    "            total_accuracy += accuracy\n",
    "            \n",
    "            pbar.set_description(f'Epoch {epoch} | Loss: {loss:.4f} | Acc: {accuracy:.4f}')\n",
    "        \n",
    "        return total_loss / len(self.train_loader), total_accuracy / len(self.train_loader)\n",
    "    \n",
    "    def validate(self):\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        for support_imgs, support_labels, query_imgs, query_labels in tqdm(self.val_loader):\n",
    "            loss, accuracy = self.validate_episode(support_imgs, support_labels, query_imgs, query_labels)\n",
    "            total_loss += loss\n",
    "            total_accuracy += accuracy\n",
    "        \n",
    "        return total_loss / len(self.val_loader), total_accuracy / len(self.val_loader)\n",
    "    \n",
    "    def save_checkpoint(self, epoch, accuracy):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "        path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        \n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self.best_epoch = epoch\n",
    "            best_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
    "            torch.save(checkpoint, best_path)\n",
    "    \n",
    "    def _adjust_learning_rates(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if any(p in self.model.backbone.parameters() for p in param_group['params']):\n",
    "                param_group['lr'] = 1e-5\n",
    "            else:\n",
    "                param_group['lr'] = 1e-4\n",
    "    \n",
    "    def train_phase(self, num_epochs, phase_name=\"Training\"):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n{phase_name} - Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "            \n",
    "            val_loss, val_acc = self.validate()\n",
    "            print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            self.save_checkpoint(epoch, val_acc)\n",
    "            \n",
    "            if phase_name == \"Initial Training\" and epoch == 40:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.1\n",
    "       \n",
    "    def train_full(self):\n",
    "        \"\"\"Complete two-phase training process\"\"\"\n",
    "        # Phase 1: Initial training\n",
    "        print(\"Starting Phase 1: Initial Training\")\n",
    "        self.train_phase(num_epochs=150, phase_name=\"Initial Training\")\n",
    "        \n",
    "        # Phase 2: Fine-tuning\n",
    "        print(\"\\nStarting Phase 2: Fine-tuning\")\n",
    "        self._adjust_learning_rates()\n",
    "        self.train_phase(num_epochs=20, phase_name=\"Fine-tuning\")\n",
    "        \n",
    "        print(f\"\\nTraining completed! Best accuracy: {self.best_accuracy:.4f} at epoch {self.best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def split_classes(root_dir, val_split=0.2, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split classes into training and validation sets.\n",
    "\n",
    "    This function takes a root directory containing class folders, shuffles the class names, \n",
    "    and splits them into training and validation sets based on the specified validation split proportion.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Path to the data directory containing class folders.\n",
    "        val_split (float, optional): Proportion of classes to use for validation. Default is 0.2.\n",
    "        random_seed (int, optional): Random seed for reproducibility. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - train_classes (list): List of class names for training.\n",
    "            - val_classes (list): List of class names for validation.\n",
    "    \"\"\"\n",
    "    train_classes = classes[:split_idx]\n",
    "    val_classes = classes[split_idx:]\n",
    "    \n",
    "    return train_classes, val_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class EpisodeDataset(Dataset):\n",
    "    def __init__(self, root_dir, allowed_classes, transform=None, n_way=2, n_support=5, n_query=15):\n",
    "        \"\"\"\n",
    "        Dataset class for few-shot learning episodes\n",
    "\n",
    "\n",
    "            root_dir (str): Root directory containing class folders.olders.\n",
    "            allowed_classes (list): List of class names this dataset can use.can use.\n",
    "            transform (callable, optional): Image transformations to be applied.Image transformations to be applied.\n",
    "            n_way (int, optional): Number of classes per episode. Default is 2.sses per episode. Default is 2.\n",
    "            n_support (int, optional): Number of support examples per class. Default is 5.amples per class. Default is 5.\n",
    "            n_query (int, optional): Number of query examples per class. Default is 15.amples per class. Default is 15.\n",
    "\n",
    "ributes:\n",
    "            root_dir (str): Root directory containing class folders.\n",
    "            transform (callable, optional): Image transformations to be applied.\n",
    "            n_way (int): Number of classes per episode.\n",
    "            n_support (int): Number of support examples per class.\n",
    "            n_query (int): Number of query examples per class.\n",
    "            classes (list): List of class names this dataset can use.\n",
    "            class_to_idx (dict): Dictionary mapping class names to indices.\n",
    "            images_by_class (dict): Dictionary mapping class names to lists of image paths.\n",
    "\n",
    "        Attributes:\n",
    "            __len__(): Returns the number of episodes per epoch.\n",
    "            __getitem__(idx): Generates one episode of data.\n",
    "                Args:\n",
    "                    idx (int): Index of the episode.\n",
    "                Returns:\n",
    "                    tuple: Tuple containing support images, support labels, query images, and query labels.\n",
    "            root_dir (str): Root directory containing class folders.\n",
    "        \"\"\"\n",
    "                support_labels.append(label)\n",
    "            \n",
    "            for img_path in selected_images[self.n_support:]:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                query_images.append(image)\n",
    "                query_labels.append(label)\n",
    "        \n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "        \n",
    "        return support_images, support_labels, query_images, query_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main function to set up and train a model using episodic training for few-shot learning.\n",
    "\n",
    "Steps:\n",
    "1. Set random seeds for reproducibility.\n",
    "2. Set the device to GPU if available, otherwise CPU.\n",
    "3. Define image transformations for data augmentation and normalization.\n",
    "4. Split the dataset into training and validation classes.\n",
    "5. Create datasets for training and validation using the specified class splits and transformations.\n",
    "6. Create data loaders for the training and validation datasets.\n",
    "7. Initialize the model and optimizer.\n",
    "8. Create a ModelTrainer instance and start the training process.\n",
    "\n",
    "Functions:\n",
    "- split_classes(data_path): Splits the dataset into training and validation classes.\n",
    "- EpisodeDataset: Custom dataset class for episodic training.\n",
    "- MTUNetPlusPlus: Model architecture.\n",
    "- AdaBelief: Optimizer.\n",
    "- ModelTrainer: Class to handle the training process.\n",
    "\n",
    "Usage:\n",
    "Run the script to start the training process.\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((80, 80)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Split classes into train and validation sets\n",
    "    data_path = '/kaggle/input/ham10000-and-gan/synthetic_images'  # Update this to your actual data path\n",
    "    train_classes, val_classes = split_classes(data_path)\n",
    "    \n",
    "    print(f\"Number of training classes: {len(train_classes)}\")\n",
    "    print(f\"Number of validation classes: {len(val_classes)}\")\n",
    "    \n",
    "    # Create datasets with respective class splits\n",
    "    train_dataset = EpisodeDataset(\n",
    "        root_dir=data_path,\n",
    "        allowed_classes=train_classes,\n",
    "        transform=transform,\n",
    "        n_way=2,\n",
    "        n_support=5,\n",
    "        n_query=15\n",
    "    )\n",
    "    \n",
    "    val_dataset = EpisodeDataset(\n",
    "        root_dir=data_path,\n",
    "        allowed_classes=val_classes,\n",
    "        transform=transform,\n",
    "        n_way=2,\n",
    "        n_support=5,\n",
    "        n_query=15\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Rest of the main function remains the same\n",
    "    model = MTUNetPlusPlus(hidden_dim=256).to(device)\n",
    "    optimizer = AdaBelief(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    trainer.train_full()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
