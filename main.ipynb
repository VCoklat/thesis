{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:15.856177Z",
     "iopub.status.busy": "2024-11-13T03:37:15.855257Z",
     "iopub.status.idle": "2024-11-13T03:37:21.607703Z",
     "shell.execute_reply": "2024-11-13T03:37:21.606712Z",
     "shell.execute_reply.started": "2024-11-13T03:37:15.856134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet imports several essential libraries and modules that are commonly used in data science, machine learning, and deep learning projects.\n",
    "\n",
    "1. **os**: This module provides a way of using operating system-dependent functionality like reading or writing to the file system. It is useful for tasks such as navigating the file system, handling file paths, and manipulating directories.\n",
    "\n",
    "2. **pandas as pd**: Pandas is a powerful data manipulation and analysis library for Python. It provides data structures like DataFrames, which are particularly useful for handling and analyzing structured data. The alias \n",
    "\n",
    "pd\n",
    "\n",
    " is a common convention to make the code more concise.\n",
    "\n",
    "3. **train_test_split from sklearn.model_selection**: This function is part of the scikit-learn library, which is widely used for machine learning tasks. The \n",
    "\n",
    "train_test_split\n",
    "\n",
    " function is used to split a dataset into training and testing sets, which is a crucial step in building and evaluating machine learning models.\n",
    "\n",
    "4. **transforms and datasets from torchvision**: Torchvision is a library that provides tools for computer vision tasks. The \n",
    "\n",
    "transforms\n",
    "\n",
    " module includes common image transformations that are often used in preprocessing steps, such as resizing, cropping, and normalizing images. The \n",
    "\n",
    "datasets\n",
    "\n",
    " module provides access to popular datasets and utilities to load them.\n",
    "\n",
    "5. **Dataset and DataLoader from torch.utils.data**: These classes are part of PyTorch, a deep learning framework. The \n",
    "\n",
    "Dataset\n",
    "\n",
    " class is an abstract class representing a dataset, and the \n",
    "\n",
    "DataLoader\n",
    "\n",
    " class provides an iterable over a dataset, with support for batching, shuffling, and parallel data loading. These are essential for efficiently handling large datasets during training and evaluation of deep learning models.\n",
    "\n",
    "6. **Image from PIL**: The Python Imaging Library (PIL) is a library that adds image processing capabilities to Python. The \n",
    "\n",
    "Image\n",
    "\n",
    " module is used for opening, manipulating, and saving many different image file formats. It is often used in conjunction with torchvision for image preprocessing tasks.\n",
    "\n",
    "Together, these imports set up a robust environment for handling data, preprocessing images, and building machine learning and deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.609762Z",
     "iopub.status.busy": "2024-11-13T03:37:21.60933Z",
     "iopub.status.idle": "2024-11-13T03:37:21.619788Z",
     "shell.execute_reply": "2024-11-13T03:37:21.618742Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.60973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dirs, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs  # List of directories\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Look up the image name\n",
    "        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "        \n",
    "        # Search for the image in the directories\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image {img_name} not found in specified directories.\")\n",
    "        \n",
    "        # Get the label\n",
    "        label = self.data.iloc[idx]['dx']  # Diagnosis column\n",
    "        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n",
    "        label = label_map[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines a custom dataset class named \n",
    "\n",
    "HAM10000Dataset\n",
    "\n",
    " that inherits from PyTorch's \n",
    "\n",
    "Dataset\n",
    "\n",
    " class. This custom dataset is designed to handle the HAM10000 dataset, which is a collection of dermatoscopic images used for skin lesion analysis.\n",
    "\n",
    "1. **Initialization (\n",
    "\n",
    "__init__\n",
    "\n",
    " method)**: The constructor takes three parameters: \n",
    "\n",
    "csv_file\n",
    "\n",
    ", \n",
    "\n",
    "img_dirs\n",
    "\n",
    ", and an optional \n",
    "\n",
    "transform\n",
    "\n",
    ". The \n",
    "\n",
    "csv_file\n",
    "\n",
    " is expected to be a CSV file containing metadata about the images, such as their filenames and labels. The \n",
    "\n",
    "img_dirs\n",
    "\n",
    " is a list of directories where the images are stored. The \n",
    "\n",
    "transform\n",
    "\n",
    " parameter allows for optional image transformations (e.g., resizing, normalization) to be applied to the images. The constructor reads the CSV file into a pandas DataFrame and stores the image directories and transform.\n",
    "\n",
    "2. **Length (\n",
    "\n",
    "__len__\n",
    "\n",
    " method)**: This method returns the number of samples in the dataset by returning the length of the DataFrame. This is a required method for PyTorch datasets, enabling functions like \n",
    "\n",
    "len(dataset)\n",
    "\n",
    " to work correctly.\n",
    "\n",
    "3. **Get Item (\n",
    "\n",
    "__getitem__\n",
    "\n",
    " method)**: This method retrieves a single sample from the dataset. It takes an index \n",
    "\n",
    "idx\n",
    "\n",
    " as input and performs the following steps:\n",
    "   - Looks up the image name in the DataFrame using the provided index and appends the `.jpg` extension.\n",
    "   - Searches for the image file in the specified directories. If the image is found, it is opened and converted to RGB format. If the image is not found in any directory, a \n",
    "\n",
    "FileNotFoundError\n",
    "\n",
    " is raised.\n",
    "   - Retrieves the label for the image from the DataFrame. The label is mapped to an integer using a dictionary that maps unique labels to indices.\n",
    "   - If a transform is provided, it is applied to the image.\n",
    "   - Returns a tuple containing the image and its corresponding label.\n",
    "\n",
    "This custom dataset class is essential for loading and preprocessing the HAM10000 dataset, making it ready for training and evaluating machine learning models. It handles the complexities of locating images across multiple directories, reading image files, and applying necessary transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.621193Z",
     "iopub.status.busy": "2024-11-13T03:37:21.620913Z",
     "iopub.status.idle": "2024-11-13T03:37:21.675607Z",
     "shell.execute_reply": "2024-11-13T03:37:21.674708Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.621164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Check the number of unique images in metadata\n",
    "print(f\"Total images in metadata: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for loading and inspecting metadata related to the HAM10000 dataset, which is used for skin lesion analysis.\n",
    "\n",
    "1. **Setting the Metadata Path**: The variable \n",
    "\n",
    "metadata_path\n",
    "\n",
    " is assigned the file path to the CSV file containing the metadata for the HAM10000 dataset. This path points to a file named `HAM10000_metadata.csv` located in the directory `../input/skin-cancer-mnist-ham10000/`.\n",
    "\n",
    "2. **Loading the Metadata**: The \n",
    "\n",
    "pd.read_csv(metadata_path)\n",
    "\n",
    " function call reads the CSV file into a pandas DataFrame named \n",
    "\n",
    "metadata\n",
    "\n",
    ". This DataFrame will contain various details about the images, such as their filenames, labels, and possibly other relevant information.\n",
    "\n",
    "3. **Checking the Number of Unique Images**: The \n",
    "\n",
    "print\n",
    "\n",
    " statement outputs the total number of images listed in the metadata. The \n",
    "\n",
    "len(metadata)\n",
    "\n",
    " function call returns the number of rows in the DataFrame, which corresponds to the number of unique images described in the metadata file.\n",
    "\n",
    "This code is crucial for verifying that the metadata has been loaded correctly and for understanding the scope of the dataset by checking the total number of images available for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.679097Z",
     "iopub.status.busy": "2024-11-13T03:37:21.67821Z",
     "iopub.status.idle": "2024-11-13T03:37:21.747117Z",
     "shell.execute_reply": "2024-11-13T03:37:21.746374Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.679046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split metadata into train and test sets\n",
    "train_metadata, test_metadata = train_test_split(metadata, test_size=0.99, random_state=42)\n",
    "\n",
    "# Save split metadata for easier loading\n",
    "train_metadata.to_csv(\"train_metadata.csv\", index=False)\n",
    "test_metadata.to_csv(\"test_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for splitting the metadata of the HAM10000 dataset into training and testing sets and then saving these splits to CSV files for easier future access.\n",
    "\n",
    "1. **Splitting the Metadata**: The \n",
    "\n",
    "train_test_split\n",
    "\n",
    " function from scikit-learn is used to split the \n",
    "\n",
    "metadata\n",
    "\n",
    " DataFrame into two separate DataFrames: \n",
    "\n",
    "train_metadata\n",
    "\n",
    " and \n",
    "\n",
    "test_metadata\n",
    "\n",
    ". The \n",
    "\n",
    "test_size=0.99\n",
    "\n",
    " parameter specifies that 99% of the data should be allocated to the test set, leaving only 1% for the training set. The \n",
    "\n",
    "random_state=42\n",
    "\n",
    " parameter ensures that the split is reproducible by setting a seed for the random number generator.\n",
    "\n",
    "2. **Saving the Split Metadata**: The \n",
    "\n",
    "to_csv\n",
    "\n",
    " method is called on both \n",
    "\n",
    "train_metadata\n",
    "\n",
    " and \n",
    "\n",
    "test_metadata\n",
    "\n",
    " DataFrames to save them as CSV files named \n",
    "\n",
    "train_metadata.csv\n",
    "\n",
    " and \n",
    "\n",
    "test_metadata.csv\n",
    "\n",
    ", respectively. The \n",
    "\n",
    "index=False\n",
    "\n",
    " parameter ensures that the row indices are not included in the saved CSV files.\n",
    "\n",
    "This code is essential for preparing the dataset for machine learning tasks. By splitting the metadata into training and testing sets, it allows for proper evaluation of the model's performance. Saving these splits to CSV files makes it convenient to load the pre-split data in future sessions, avoiding the need to perform the split operation repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.748396Z",
     "iopub.status.busy": "2024-11-13T03:37:21.748099Z",
     "iopub.status.idle": "2024-11-13T03:37:21.752645Z",
     "shell.execute_reply": "2024-11-13T03:37:21.751629Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.748365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Directories containing images\n",
    "image_dirs = [\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a list of directories that contain the images for the HAM10000 dataset, which is used for skin lesion analysis.\n",
    "\n",
    "1. **Defining Image Directories**: The variable \n",
    "\n",
    "main.ipynb\n",
    "\n",
    " ) is assigned a list of two directory paths. These directories are specified as strings and point to the locations where the image files are stored. The paths are:\n",
    "   - `../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1`\n",
    "   - `../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2`\n",
    "\n",
    "2. **Purpose of Image Directories**: These directories are likely part of the dataset's structure, where the images have been split into multiple parts for organizational purposes. By listing these directories, the code can later iterate through them to locate and load the images as needed.\n",
    "\n",
    "This setup is crucial for managing and accessing the image files efficiently. By specifying the directories in a list, the code can easily handle the images regardless of their distribution across multiple folders. This approach simplifies the process of loading and preprocessing the images for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.754216Z",
     "iopub.status.busy": "2024-11-13T03:37:21.753872Z",
     "iopub.status.idle": "2024-11-13T03:37:21.763699Z",
     "shell.execute_reply": "2024-11-13T03:37:21.762723Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.754185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images to 256x256\n",
    "    transforms.ToTensor(),         # Convert to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a sequence of image transformations using the \n",
    "\n",
    "transforms.Compose\n",
    "\n",
    " function from the `torchvision` library. These transformations are applied to the images in the HAM10000 dataset to prepare them for input into a machine learning model.\n",
    "\n",
    "1. **Resizing Images**: The \n",
    "\n",
    "transforms.Resize((64, 64))\n",
    "\n",
    " transformation resizes the images to a fixed size of 64x64 pixels. This step ensures that all images have the same dimensions, which is necessary for batch processing in neural networks. The comment incorrectly mentions resizing to 256x256, but the actual code resizes to 64x64.\n",
    "\n",
    "2. **Converting to Tensor**: The \n",
    "\n",
    "transforms.ToTensor()\n",
    "\n",
    " transformation converts the images from PIL format (or numpy arrays) to PyTorch tensors. This conversion is essential because PyTorch models require input data in tensor format. Additionally, this transformation scales the pixel values from the range [0, 255] to [0, 1].\n",
    "\n",
    "3. **Normalizing**: The \n",
    "\n",
    "transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    " transformation normalizes the pixel values of the images. Normalization adjusts the pixel values to have a mean of 0.5 and a standard deviation of 0.5 for each of the three color channels (red, green, and blue). This step helps in stabilizing and speeding up the training process by ensuring that the input data has a consistent distribution.\n",
    "\n",
    "By composing these transformations, the code ensures that the images are uniformly resized, converted to a suitable format for PyTorch, and normalized. These preprocessing steps are crucial for preparing the dataset for training and evaluating machine learning models effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:22.156079Z",
     "iopub.status.busy": "2024-11-13T03:37:22.155683Z",
     "iopub.status.idle": "2024-11-13T03:37:22.189694Z",
     "shell.execute_reply": "2024-11-13T03:37:22.188693Z",
     "shell.execute_reply.started": "2024-11-13T03:37:22.156041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = HAM10000Dataset(csv_file=\"train_metadata.csv\", img_dirs=image_dirs, transform=transform)\n",
    "test_dataset = HAM10000Dataset(csv_file=\"test_metadata.csv\", img_dirs=image_dirs, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet sets up the datasets and data loaders for training and testing a machine learning model using the HAM10000 dataset. It also prints the sizes of the training and testing datasets.\n",
    "\n",
    "1. **Creating Datasets**: \n",
    "   - \n",
    "\n",
    "train_dataset\n",
    "\n",
    " and \n",
    "\n",
    "test_dataset\n",
    "\n",
    " are instances of the custom \n",
    "\n",
    "HAM10000Dataset\n",
    "\n",
    " class. \n",
    "   - The \n",
    "\n",
    "train_dataset\n",
    "\n",
    " is initialized with the metadata file `train_metadata.csv`, the list of image directories \n",
    "\n",
    "image_dirs\n",
    "\n",
    ", and the transformation pipeline \n",
    "\n",
    "transform\n",
    "\n",
    ".\n",
    "   - Similarly, the \n",
    "\n",
    "test_dataset\n",
    "\n",
    " is initialized with the metadata file `test_metadata.csv`, the same image directories, and the same transformation pipeline.\n",
    "   - These datasets will handle loading and preprocessing the images and their corresponding labels.\n",
    "\n",
    "2. **Creating DataLoaders**:\n",
    "   - \n",
    "\n",
    "train_loader\n",
    "\n",
    " and \n",
    "\n",
    "test_loader\n",
    "\n",
    " are instances of PyTorch's \n",
    "\n",
    "DataLoader\n",
    "\n",
    " class.\n",
    "   - \n",
    "\n",
    "train_loader\n",
    "\n",
    " is created with the \n",
    "\n",
    "train_dataset\n",
    "\n",
    ", a batch size of 4, shuffling enabled (\n",
    "\n",
    "shuffle=True\n",
    "\n",
    "), and 2 worker threads (\n",
    "\n",
    "num_workers=2\n",
    "\n",
    ") for parallel data loading. Shuffling ensures that the training data is presented in a different order each epoch, which helps in training the model more effectively.\n",
    "   - \n",
    "\n",
    "test_loader\n",
    "\n",
    " is created with the \n",
    "\n",
    "test_dataset\n",
    "\n",
    ", the same batch size of 4, shuffling disabled (\n",
    "\n",
    "shuffle=False\n",
    "\n",
    "), and 2 worker threads. Shuffling is typically disabled for the test set to ensure consistent evaluation.\n",
    "\n",
    "3. **Printing Dataset Sizes**:\n",
    "   - The \n",
    "\n",
    "print\n",
    "\n",
    " statements output the number of samples in the training and testing datasets by calling \n",
    "\n",
    "len()\n",
    "\n",
    " on \n",
    "\n",
    "train_dataset\n",
    "\n",
    " and \n",
    "\n",
    "test_dataset\n",
    "\n",
    ".\n",
    "   - This provides a quick check to ensure that the datasets have been loaded correctly and to understand the amount of data available for training and testing.\n",
    "\n",
    "Overall, this code is essential for preparing the data pipeline, ensuring that the images and labels are correctly loaded, preprocessed, and batched for training and evaluation of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:23.031142Z",
     "iopub.status.busy": "2024-11-13T03:37:23.030695Z",
     "iopub.status.idle": "2024-11-13T03:37:23.883755Z",
     "shell.execute_reply": "2024-11-13T03:37:23.882658Z",
     "shell.execute_reply.started": "2024-11-13T03:37:23.031103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize one batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")\n",
    "\n",
    "# Display first 4 images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Denormalize\n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for visualizing a batch of images from the training dataset. This helps in verifying that the images are being loaded and preprocessed correctly.\n",
    "\n",
    "1. **Loading a Batch of Images**:\n",
    "   - The line \n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    " retrieves the next batch of images and their corresponding labels from the \n",
    "\n",
    "train_loader\n",
    "\n",
    ". The \n",
    "\n",
    "iter(train_loader)\n",
    "\n",
    " creates an iterator over the \n",
    "\n",
    "train_loader\n",
    "\n",
    ", and \n",
    "\n",
    "next()\n",
    "\n",
    " fetches the next batch.\n",
    "   - The \n",
    "\n",
    "print\n",
    "\n",
    " statements output the shapes of the image and label batches. This provides information about the dimensions of the data, ensuring that the batch size and image dimensions are as expected.\n",
    "\n",
    "2. **Importing Matplotlib**:\n",
    "   - The \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " statement imports the \n",
    "\n",
    "matplotlib.pyplot\n",
    "\n",
    " module, which is used for creating visualizations.\n",
    "\n",
    "3. **Creating a Figure for Display**:\n",
    "   - The line \n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "\n",
    " creates a figure with 4 subplots arranged in a single row. The \n",
    "\n",
    "figsize\n",
    "\n",
    " parameter sets the size of the figure to 12 inches by 4 inches.\n",
    "\n",
    "4. **Displaying the First 4 Images**:\n",
    "   - A `for` loop iterates over the first 4 images in the batch.\n",
    "   - Inside the loop, \n",
    "\n",
    "axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "\n",
    " displays each image. The \n",
    "\n",
    "permute(1, 2, 0)\n",
    "\n",
    " method rearranges the dimensions of the image tensor from (C, H, W) to (H, W, C), which is required for displaying the image using \n",
    "\n",
    "imshow\n",
    "\n",
    ". The \n",
    "\n",
    "numpy()\n",
    "\n",
    " method converts the tensor to a NumPy array, and the multiplication and addition (`* 0.5 + 0.5`) denormalize the pixel values back to the range [0, 1].\n",
    "   - \n",
    "\n",
    "axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "\n",
    " sets the title of each subplot to the corresponding label.\n",
    "   - \n",
    "\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    " removes the axis ticks and labels for a cleaner display.\n",
    "\n",
    "5. **Showing the Figure**:\n",
    "   - The \n",
    "\n",
    "plt.show()\n",
    "\n",
    " statement renders the figure and displays the images.\n",
    "\n",
    "This visualization step is crucial for ensuring that the data loading and preprocessing pipeline is functioning correctly. By displaying a few images from the training set, you can visually inspect the images and their labels to confirm that they are being processed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:40:01.297608Z",
     "iopub.status.busy": "2024-11-13T03:40:01.296762Z",
     "iopub.status.idle": "2024-11-13T03:40:02.565467Z",
     "shell.execute_reply": "2024-11-13T03:40:02.564406Z",
     "shell.execute_reply.started": "2024-11-13T03:40:01.297566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch, -1, width * height)\n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        proj_value = self.value_conv(x).view(batch, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, img_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_channels = img_channels\n",
    "        self.img_size = img_size\n",
    "        self.init_size = img_size // 8  # Downsample by 8 (adjusted for 64x64 output)\n",
    "        self.fc = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128),\n",
    "            SelfAttention(128),  # Self-Attention after first upscale\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttention(32),  # Self-Attention in the middle layers\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(32, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Normalize output to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n",
    "        out = self.upsample(out)\n",
    "        img = self.final_layer(out)\n",
    "        return img\n",
    "\n",
    "# Instantiate the generator\n",
    "latent_dim = 100  # Size of latent vector\n",
    "img_channels = 3  # RGB images\n",
    "img_size = 64  # Output image size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(latent_dim, img_channels, img_size).to(device)\n",
    "\n",
    "# Test the generator\n",
    "z = torch.randn(4, latent_dim).to(device)  # Random latent vector (batch size = 4)\n",
    "generated_images = generator(z)\n",
    "\n",
    "print(f\"Generated image shape: {generated_images.shape}\")  # Should be [4, 3, 64, 64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines and tests a deep learning model for generating images using PyTorch. It includes the implementation of a self-attention mechanism, a residual block, and a generator network.\n",
    "\n",
    "1. **Imports**:\n",
    "   - The code imports the necessary modules from PyTorch, including \n",
    "\n",
    "torch\n",
    "\n",
    " and \n",
    "\n",
    "torch.nn\n",
    "\n",
    ".\n",
    "\n",
    "2. **Self-Attention Class**:\n",
    "   - The \n",
    "\n",
    "SelfAttention\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and implements a self-attention mechanism.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes convolutional layers for query, key, and value projections, and a learnable parameter \n",
    "\n",
    "gamma\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method computes the attention map and applies it to the input feature map, enhancing the model's ability to focus on relevant parts of the image.\n",
    "\n",
    "3. **Residual Block Class**:\n",
    "   - The \n",
    "\n",
    "ResidualBlock\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and implements a residual block.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method defines a sequence of convolutional, batch normalization, and ReLU activation layers.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method adds the input to the output of the block, facilitating gradient flow and improving training stability.\n",
    "\n",
    "4. **Generator Class**:\n",
    "   - The \n",
    "\n",
    "Generator\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and defines the architecture of the generator network.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes the network with a fully connected layer, several upsampling layers, residual blocks, and self-attention layers.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method processes the input latent vector through the network to generate an image.\n",
    "\n",
    "5. **Instantiating and Testing the Generator**:\n",
    "   - The generator is instantiated with a latent dimension of 100, 3 image channels (for RGB images), and an output image size of 64x64 pixels.\n",
    "   - The generator is moved to the appropriate device (GPU if available, otherwise CPU).\n",
    "   - A random latent vector \n",
    "\n",
    "z\n",
    "\n",
    " is generated and passed through the generator to produce a batch of images.\n",
    "   - The shape of the generated images is printed to verify the output dimensions.\n",
    "\n",
    "This code demonstrates the implementation of a generative model with advanced components like self-attention and residual blocks, which are designed to improve the quality and stability of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:41:32.233766Z",
     "iopub.status.busy": "2024-11-13T03:41:32.233316Z",
     "iopub.status.idle": "2024-11-13T03:41:32.46463Z",
     "shell.execute_reply": "2024-11-13T03:41:32.463541Z",
     "shell.execute_reply.started": "2024-11-13T03:41:32.233725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch, -1, width * height)\n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        proj_value = self.value_conv(x).view(batch, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if downsample else nn.Identity()\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AvgPool2d(2) if downsample else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, img_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ResidualBlock(img_channels, 64, downsample=True),            # 64x64 -> 32x32\n",
    "            SelfAttention(64),\n",
    "            ResidualBlock(64, 128, downsample=True),           # 32x32 -> 16x16\n",
    "            SelfAttention(128),\n",
    "            ResidualBlock(128, 256, downsample=True),          # 16x16 -> 8x8\n",
    "            ResidualBlock(256, 512, downsample=True)           # 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1),  # Final score output\n",
    "            nn.Sigmoid()  # Outputs probability of being real or fake\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = self.final_layer(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the discriminator\n",
    "img_channels = 3  # RGB images\n",
    "img_size = 64  # Input image size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "discriminator = Discriminator(img_channels, img_size).to(device)\n",
    "\n",
    "# Test the discriminator\n",
    "batch_size = 4\n",
    "test_images = torch.randn(batch_size, img_channels, img_size, img_size).to(device)  # Fake images batch\n",
    "output = discriminator(test_images)\n",
    "\n",
    "print(f\"Discriminator output shape: {output.shape}\")  # Should be [4, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines and tests a discriminator model for a Generative Adversarial Network (GAN) using PyTorch. The discriminator is designed to distinguish between real and fake images.\n",
    "\n",
    "1. **Imports**:\n",
    "   - The code imports the necessary modules from PyTorch, including \n",
    "\n",
    "torch\n",
    "\n",
    " and \n",
    "\n",
    "torch.nn\n",
    "\n",
    ".\n",
    "\n",
    "2. **Self-Attention Class**:\n",
    "   - The \n",
    "\n",
    "SelfAttention\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and implements a self-attention mechanism.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes convolutional layers for query, key, and value projections, and a learnable parameter \n",
    "\n",
    "gamma\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method computes the attention map and applies it to the input feature map, enhancing the model's ability to focus on relevant parts of the image.\n",
    "\n",
    "3. **Residual Block Class**:\n",
    "   - The \n",
    "\n",
    "ResidualBlock\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and implements a residual block.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method defines a sequence of convolutional, batch normalization, and ReLU activation layers, along with a shortcut connection and optional downsampling.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method processes the input through the convolutional layers, adds the shortcut connection, and applies downsampling if specified.\n",
    "\n",
    "4. **Discriminator Class**:\n",
    "   - The \n",
    "\n",
    "Discriminator\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and defines the architecture of the discriminator network.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes the network with a sequence of residual blocks and self-attention layers, followed by a final layer that flattens the output and applies a linear transformation and sigmoid activation to produce a probability score.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method processes the input image through the network to produce the final output.\n",
    "\n",
    "5. **Instantiating and Testing the Discriminator**:\n",
    "   - The discriminator is instantiated with 3 image channels (for RGB images) and an input image size of 64x64 pixels.\n",
    "   - The discriminator is moved to the appropriate device (GPU if available, otherwise CPU).\n",
    "   - A batch of random fake images is generated and passed through the discriminator to produce an output.\n",
    "   - The shape of the discriminator's output is printed to verify the dimensions, which should be `[4, 1]` for a batch size of 4.\n",
    "\n",
    "This code demonstrates the implementation of a discriminator model with advanced components like self-attention and residual blocks, which are designed to improve the model's ability to distinguish between real and fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:46:53.871919Z",
     "iopub.status.busy": "2024-11-13T03:46:53.871058Z",
     "iopub.status.idle": "2024-11-13T03:46:53.876521Z",
     "shell.execute_reply": "2024-11-13T03:46:53.87544Z",
     "shell.execute_reply.started": "2024-11-13T03:46:53.871864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for managing warning messages in Python.\n",
    "\n",
    "1. **Importing the Warnings Module**:\n",
    "   - The line \n",
    "\n",
    "import warnings\n",
    "\n",
    " imports the \n",
    "\n",
    "warnings\n",
    "\n",
    " module, which is a built-in Python module used to handle warning messages. Warnings are typically issued to alert the user about potential issues in the code that do not necessarily stop the execution but might lead to unexpected behavior.\n",
    "\n",
    "2. **Filtering Warnings**:\n",
    "   - The line \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    " sets a filter to ignore all warning messages. This means that any warnings that would normally be printed to the console will be suppressed and not displayed.\n",
    "   - This can be useful in scenarios where the user is aware of certain non-critical warnings and wants to avoid cluttering the output with these messages. However, it is important to use this with caution, as ignoring warnings might cause the user to miss important information about potential issues in the code.\n",
    "\n",
    "By using this code, the user ensures that the output remains clean and free of warning messages, which can be particularly useful in a production environment or when running long scripts where warnings are expected and understood. However, it is generally a good practice to address the root causes of warnings rather than ignoring them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:47:38.582788Z",
     "iopub.status.busy": "2024-11-13T03:47:38.58237Z",
     "iopub.status.idle": "2024-11-13T03:47:38.671075Z",
     "shell.execute_reply": "2024-11-13T03:47:38.67003Z",
     "shell.execute_reply.started": "2024-11-13T03:47:38.582751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "#def train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1000, lr=0.0002, beta1=0.5, beta2=0.999):\n",
    "def train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1, lr=0.0002, beta1=0.5, beta2=0.999):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        epoch_loss_G = 0.0\n",
    "        epoch_loss_D = 0.0\n",
    "\n",
    "        for real_images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n",
    "            fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():  # Mixed precision training\n",
    "                generated_images = generator(z)\n",
    "                g_loss = criterion(discriminator(generated_images), valid)\n",
    "\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(optimizer_G)\n",
    "            scaler.update()\n",
    "            epoch_loss_G += g_loss.item()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                real_loss = criterion(discriminator(real_images), valid)\n",
    "                fake_loss = criterion(discriminator(generated_images.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(optimizer_D)\n",
    "            scaler.update()\n",
    "            epoch_loss_D += d_loss.item()\n",
    "\n",
    "            # Clear cache to reduce memory fragmentation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Generator Loss: {epoch_loss_G:.4f} | Discriminator Loss: {epoch_loss_D:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "\n",
    "# Call the train_gan function with the train_loader, generator, and discriminator\n",
    "train_gan(generator, discriminator, train_loader, latent_dim, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a function \n",
    "\n",
    "train_gan\n",
    "\n",
    " that trains a Generative Adversarial Network (GAN) using PyTorch. The function trains both the generator and discriminator models over a specified number of epochs.\n",
    "\n",
    "1. **Imports**:\n",
    "   - The code imports the \n",
    "\n",
    "optim\n",
    "\n",
    " module from PyTorch for optimization algorithms and \n",
    "\n",
    "tqdm\n",
    "\n",
    " for displaying progress bars during training.\n",
    "\n",
    "2. **Function Definition**:\n",
    "   - The \n",
    "\n",
    "train_gan\n",
    "\n",
    " function takes several parameters: \n",
    "\n",
    "generator\n",
    "\n",
    ", \n",
    "\n",
    "discriminator\n",
    "\n",
    ", \n",
    "\n",
    "train_loader\n",
    "\n",
    ", \n",
    "\n",
    "latent_dim\n",
    "\n",
    ", \n",
    "\n",
    "device\n",
    "\n",
    ", \n",
    "\n",
    "epochs\n",
    "\n",
    ", \n",
    "\n",
    "lr\n",
    "\n",
    ", \n",
    "\n",
    "beta1\n",
    "\n",
    ", and \n",
    "\n",
    "beta2\n",
    "\n",
    ".\n",
    "   - The default number of epochs is set to 1, but it can be adjusted as needed.\n",
    "\n",
    "3. **Model Preparation**:\n",
    "   - The generator and discriminator models are moved to the specified device (GPU or CPU) using \n",
    "\n",
    "generator.to(device)\n",
    "\n",
    " and \n",
    "\n",
    "discriminator.to(device)\n",
    "\n",
    ".\n",
    "   - The loss function used is \n",
    "\n",
    "nn.BCEWithLogitsLoss()\n",
    "\n",
    ", which combines a sigmoid layer and binary cross-entropy loss.\n",
    "   - Two Adam optimizers are created for the generator (\n",
    "\n",
    "optimizer_G\n",
    "\n",
    ") and discriminator (\n",
    "\n",
    "optimizer_D\n",
    "\n",
    ") with the specified learning rate (\n",
    "\n",
    "lr\n",
    "\n",
    ") and beta values (\n",
    "\n",
    "beta1\n",
    "\n",
    ", \n",
    "\n",
    "beta2\n",
    "\n",
    ").\n",
    "\n",
    "4. **Mixed Precision Training**:\n",
    "   - A gradient scaler (\n",
    "\n",
    "scaler\n",
    "\n",
    ") is initialized for mixed precision training, which can improve performance and reduce memory usage on compatible hardware.\n",
    "\n",
    "5. **Training Loop**:\n",
    "   - The outer loop iterates over the number of epochs.\n",
    "   - Within each epoch, the generator and discriminator models are set to training mode using \n",
    "\n",
    "generator.train()\n",
    "\n",
    " and \n",
    "\n",
    "discriminator.train()\n",
    "\n",
    ".\n",
    "   - Two variables, \n",
    "\n",
    "epoch_loss_G\n",
    "\n",
    " and \n",
    "\n",
    "epoch_loss_D\n",
    "\n",
    ", are initialized to accumulate the generator and discriminator losses for the epoch.\n",
    "\n",
    "6. **Batch Processing**:\n",
    "   - The inner loop iterates over batches of real images from the \n",
    "\n",
    "train_loader\n",
    "\n",
    ", displaying progress with \n",
    "\n",
    "tqdm\n",
    "\n",
    ".\n",
    "   - The batch size is determined from the real images, and the images are moved to the specified device.\n",
    "   - Two tensors, \n",
    "\n",
    "valid\n",
    "\n",
    " and \n",
    "\n",
    "fake\n",
    "\n",
    ", are created to represent the labels for real and fake images, respectively.\n",
    "\n",
    "7. **Training the Generator**:\n",
    "   - The generator's gradients are zeroed using \n",
    "\n",
    "optimizer_G.zero_grad()\n",
    "\n",
    ".\n",
    "   - A batch of random latent vectors (\n",
    "\n",
    "z\n",
    "\n",
    ") is generated and moved to the device.\n",
    "   - Mixed precision training is used to generate images and compute the generator loss (\n",
    "\n",
    "g_loss\n",
    "\n",
    ").\n",
    "   - The loss is scaled, backpropagated, and the optimizer is stepped using the gradient scaler.\n",
    "\n",
    "8. **Training the Discriminator**:\n",
    "   - The discriminator's gradients are zeroed using \n",
    "\n",
    "optimizer_D.zero_grad()\n",
    "\n",
    ".\n",
    "   - Mixed precision training is used to compute the discriminator loss (\n",
    "\n",
    "d_loss\n",
    "\n",
    ") from both real and fake images.\n",
    "   - The loss is scaled, backpropagated, and the optimizer is stepped using the gradient scaler.\n",
    "\n",
    "9. **Memory Management**:\n",
    "   - The CUDA cache is cleared using \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    " to reduce memory fragmentation.\n",
    "\n",
    "10. **Logging**:\n",
    "    - The generator and discriminator losses for the epoch are printed.\n",
    "\n",
    "11. **Function Call**:\n",
    "    - The \n",
    "\n",
    "train_gan\n",
    "\n",
    " function is called with the \n",
    "\n",
    "train_loader\n",
    "\n",
    ", \n",
    "\n",
    "generator\n",
    "\n",
    ", and \n",
    "\n",
    "discriminator\n",
    "\n",
    " to start the training process.\n",
    "\n",
    "This function provides a comprehensive framework for training a GAN, including mixed precision training, progress tracking, and memory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:55:49.315186Z",
     "iopub.status.busy": "2024-11-13T03:55:49.314203Z",
     "iopub.status.idle": "2024-11-13T03:55:49.3272Z",
     "shell.execute_reply": "2024-11-13T03:55:49.326207Z",
     "shell.execute_reply.started": "2024-11-13T03:55:49.315144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define EfficientNetV2 model for HAM10000 with 7 classes\n",
    "class EfficientNetV2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n",
    "        super(EfficientNetV2Classifier, self).__init__()\n",
    "        self.efficientnet_v2 = models.efficientnet_v2_s(pretrained=True)\n",
    "        \n",
    "        in_features = self.efficientnet_v2.classifier[1].in_features\n",
    "        self.efficientnet_v2.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet_v2(x)\n",
    "\n",
    "# Initialize the model\n",
    "model_EfficientNetV2 = EfficientNetV2Classifier(num_classes=7)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_EfficientNetV2.parameters(), lr=0.001)\n",
    "epochs = 1#20\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=1):#20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            print(\"done\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = total_correct / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validation after each epoch\n",
    "        validate_model(model, test_loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / len(test_loader.dataset)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "model_EfficientNetV2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model_EfficientNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines and trains a deep learning model using the EfficientNetV2 architecture for the HAM10000 dataset, which consists of 7 classes of skin lesions.\n",
    "\n",
    "1. **Imports and Warnings**:\n",
    "   - The code imports necessary modules from PyTorch (\n",
    "\n",
    "torch\n",
    "\n",
    ", \n",
    "\n",
    "torch.nn\n",
    "\n",
    ", \n",
    "\n",
    "torch.optim\n",
    "\n",
    ") and \n",
    "\n",
    "models\n",
    "\n",
    " from \n",
    "\n",
    "torchvision\n",
    "\n",
    ".\n",
    "   - Warnings are suppressed using \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    " to keep the output clean.\n",
    "\n",
    "2. **EfficientNetV2 Classifier**:\n",
    "   - The \n",
    "\n",
    "EfficientNetV2Classifier\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and defines a custom classifier based on the EfficientNetV2 architecture.\n",
    "   - In the \n",
    "\n",
    "__init__\n",
    "\n",
    " method, the EfficientNetV2 model is loaded with pretrained weights using \n",
    "\n",
    "models.efficientnet_v2_s(pretrained=True)\n",
    "\n",
    ".\n",
    "   - The classifier layer of the model is replaced with a new sequential layer consisting of a dropout layer and a linear layer to output predictions for 7 classes.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method defines the forward pass of the model, which simply calls the forward method of the EfficientNetV2 model.\n",
    "\n",
    "3. **Model Initialization**:\n",
    "   - An instance of the \n",
    "\n",
    "EfficientNetV2Classifier\n",
    "\n",
    " is created with 7 output classes and assigned to \n",
    "\n",
    "model_EfficientNetV2\n",
    "\n",
    ".\n",
    "\n",
    "4. **Loss and Optimizer**:\n",
    "   - The loss function used is \n",
    "\n",
    "nn.CrossEntropyLoss()\n",
    "\n",
    ", which is suitable for multi-class classification problems.\n",
    "   - The optimizer used is Adam (\n",
    "\n",
    "optim.Adam\n",
    "\n",
    "), with a learning rate of 0.001, to update the model parameters.\n",
    "\n",
    "5. **Training Loop**:\n",
    "   - The \n",
    "\n",
    "train_model\n",
    "\n",
    " function is defined to train the model. It takes the model, training and testing data loaders, loss function, optimizer, and number of epochs as input.\n",
    "   - The model is set to training mode using \n",
    "\n",
    "model.train()\n",
    "\n",
    ".\n",
    "   - For each epoch, the running loss and total correct predictions are initialized.\n",
    "   - The inner loop iterates over batches of data from the training loader. For each batch:\n",
    "     - Data and labels are moved to the specified device.\n",
    "     - The optimizer gradients are zeroed.\n",
    "     - The model outputs are computed, and the loss is calculated.\n",
    "     - The loss is backpropagated, and the optimizer steps are performed.\n",
    "     - The running loss and total correct predictions are updated.\n",
    "   - After each epoch, the average loss and accuracy are printed.\n",
    "   - The \n",
    "\n",
    "validate_model\n",
    "\n",
    " function is called to evaluate the model on the test set.\n",
    "\n",
    "6. **Validation Loop**:\n",
    "   - The \n",
    "\n",
    "validate_model\n",
    "\n",
    " function is defined to evaluate the model on the test set.\n",
    "   - The model is set to evaluation mode using \n",
    "\n",
    "model.eval()\n",
    "\n",
    ".\n",
    "   - The total correct predictions and total loss are initialized.\n",
    "   - The loop iterates over batches of data from the test loader. For each batch:\n",
    "     - Data and labels are moved to the specified device.\n",
    "     - The model outputs are computed, and the loss is calculated.\n",
    "     - The total loss and total correct predictions are updated.\n",
    "   - The average loss and accuracy are printed.\n",
    "\n",
    "7. **Model Training**:\n",
    "   - The model is moved to the specified device using \n",
    "\n",
    "model_EfficientNetV2.to(device)\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "train_model\n",
    "\n",
    " function is called with the model, training and testing data loaders, loss function, optimizer, and number of epochs to start the training process.\n",
    "\n",
    "This code sets up and trains an EfficientNetV2-based classifier for the HAM10000 dataset, providing a comprehensive framework for training and evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:00:06.453375Z",
     "iopub.status.busy": "2024-11-13T04:00:06.452961Z",
     "iopub.status.idle": "2024-11-13T04:00:06.464457Z",
     "shell.execute_reply": "2024-11-13T04:00:06.463412Z",
     "shell.execute_reply.started": "2024-11-13T04:00:06.453324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define ShuffleNetV2 model for HAM10000 with 7 classes\n",
    "class ShuffleNetV2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n",
    "        super(ShuffleNetV2Classifier, self).__init__()\n",
    "        self.shufflenet_v2 = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "        \n",
    "        # Modify the last fully connected layer to match the number of classes\n",
    "        in_features = self.shufflenet_v2.fc.in_features\n",
    "        self.shufflenet_v2.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shufflenet_v2(x)\n",
    "\n",
    "# Initialize the model\n",
    "model_ShuffleNetV2 = ShuffleNetV2Classifier(num_classes=7)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ShuffleNetV2.parameters(), lr=0.001)\n",
    "epochs = 1#20\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=1):#20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = total_correct / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validation after each epoch\n",
    "        validate_model(model, test_loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / len(test_loader.dataset)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "model_ShuffleNetV2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model_ShuffleNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines and trains a deep learning model using the ShuffleNetV2 architecture for the HAM10000 dataset, which consists of 7 classes of skin lesions.\n",
    "\n",
    "1. **ShuffleNetV2 Classifier**:\n",
    "   - The \n",
    "\n",
    "ShuffleNetV2Classifier\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    " and defines a custom classifier based on the ShuffleNetV2 architecture.\n",
    "   - In the \n",
    "\n",
    "__init__\n",
    "\n",
    " method, the ShuffleNetV2 model is loaded with pretrained weights using \n",
    "\n",
    "models.shufflenet_v2_x1_0(pretrained=True)\n",
    "\n",
    ".\n",
    "   - The classifier layer of the model is modified to match the number of classes (7) by replacing the last fully connected layer with a new sequential layer consisting of a dropout layer and a linear layer.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method defines the forward pass of the model, which simply calls the forward method of the ShuffleNetV2 model.\n",
    "\n",
    "2. **Model Initialization**:\n",
    "   - An instance of the \n",
    "\n",
    "ShuffleNetV2Classifier\n",
    "\n",
    " is created with 7 output classes and assigned to \n",
    "\n",
    "model_ShuffleNetV2\n",
    "\n",
    ".\n",
    "\n",
    "3. **Loss and Optimizer**:\n",
    "   - The loss function used is \n",
    "\n",
    "nn.CrossEntropyLoss()\n",
    "\n",
    ", which is suitable for multi-class classification problems.\n",
    "   - The optimizer used is Adam (\n",
    "\n",
    "optim.Adam\n",
    "\n",
    "), with a learning rate of 0.001, to update the model parameters.\n",
    "\n",
    "4. **Training Loop**:\n",
    "   - The \n",
    "\n",
    "train_model\n",
    "\n",
    " function is defined to train the model. It takes the model, training and testing data loaders, loss function, optimizer, and number of epochs as input.\n",
    "   - The model is set to training mode using \n",
    "\n",
    "model.train()\n",
    "\n",
    ".\n",
    "   - For each epoch, the running loss and total correct predictions are initialized.\n",
    "   - The inner loop iterates over batches of data from the training loader. For each batch:\n",
    "     - Data and labels are moved to the specified device.\n",
    "     - The optimizer gradients are zeroed.\n",
    "     - The model outputs are computed, and the loss is calculated.\n",
    "     - The loss is backpropagated, and the optimizer steps are performed.\n",
    "     - The running loss and total correct predictions are updated.\n",
    "   - After each epoch, the average loss and accuracy are printed.\n",
    "   - The \n",
    "\n",
    "validate_model\n",
    "\n",
    " function is called to evaluate the model on the test set.\n",
    "\n",
    "5. **Validation Loop**:\n",
    "   - The \n",
    "\n",
    "validate_model\n",
    "\n",
    " function is defined to evaluate the model on the test set.\n",
    "   - The model is set to evaluation mode using \n",
    "\n",
    "model.eval()\n",
    "\n",
    ".\n",
    "   - The total correct predictions and total loss are initialized.\n",
    "   - The loop iterates over batches of data from the test loader. For each batch:\n",
    "     - Data and labels are moved to the specified device.\n",
    "     - The model outputs are computed, and the loss is calculated.\n",
    "     - The total loss and total correct predictions are updated.\n",
    "   - The average loss and accuracy are printed.\n",
    "\n",
    "6. **Model Training**:\n",
    "   - The model is moved to the specified device using \n",
    "\n",
    "model_ShuffleNetV2.to(device)\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "train_model\n",
    "\n",
    " function is called with the model, training and testing data loaders, loss function, optimizer, and number of epochs to start the training process.\n",
    "\n",
    "This code sets up and trains a ShuffleNetV2-based classifier for the HAM10000 dataset, providing a comprehensive framework for training and evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:00:30.917234Z",
     "iopub.status.busy": "2024-11-13T04:00:30.916705Z",
     "iopub.status.idle": "2024-11-13T04:00:30.924325Z",
     "shell.execute_reply": "2024-11-13T04:00:30.923203Z",
     "shell.execute_reply.started": "2024-11-13T04:00:30.917194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, labels, noise_dim=128):\n",
    "    noise = torch.randn(batch_size, noise_dim)  # Random noise for generator\n",
    "    created_imgs = generator(noise, labels) \n",
    "    EfficientNetV2Classifier_labels = model_EfficientNetV2(created_imgs)\n",
    "    ShuffleNetV2Classifier_labels = model_ShuffleNetV2(created_imgs)\n",
    "    if EfficientNetV2Classifier_labels == labels and ShuffleNetV2Classifier_labels == labels:\n",
    "        return created_imgs\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a function \n",
    "\n",
    "create_support_set\n",
    "\n",
    " that generates a set of images using a generator model and then validates these images using two classifier models, EfficientNetV2 and ShuffleNetV2.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function \n",
    "\n",
    "create_support_set\n",
    "\n",
    " takes five parameters: \n",
    "\n",
    "generator\n",
    "\n",
    ", \n",
    "\n",
    "model_EfficientNetV2\n",
    "\n",
    ", \n",
    "\n",
    "model_ShuffleNetV2\n",
    "\n",
    ", \n",
    "\n",
    "labels\n",
    "\n",
    ", and an optional \n",
    "\n",
    "noise_dim\n",
    "\n",
    " with a default value of 128.\n",
    "   - The purpose of this function is to create a support set of images that are validated by both classifier models.\n",
    "\n",
    "2. **Generating Noise**:\n",
    "   - The line \n",
    "\n",
    "noise = torch.randn(batch_size, noise_dim)\n",
    "\n",
    " generates a batch of random noise vectors. The \n",
    "\n",
    "noise_dim\n",
    "\n",
    " parameter specifies the dimensionality of each noise vector, and \n",
    "\n",
    "batch_size\n",
    "\n",
    " is assumed to be defined elsewhere in the code.\n",
    "   - This random noise serves as input to the generator model to produce synthetic images.\n",
    "\n",
    "3. **Generating Images**:\n",
    "   - The line \n",
    "\n",
    "created_imgs = generator(noise, labels)\n",
    "\n",
    " uses the generator model to create images from the random noise and the provided labels. The generator is expected to take both noise and labels as input to produce labeled images.\n",
    "\n",
    "4. **Classifying Generated Images**:\n",
    "   - The generated images are then passed through two classifier models: \n",
    "\n",
    "model_EfficientNetV2\n",
    "\n",
    " and \n",
    "\n",
    "model_ShuffleNetV2\n",
    "\n",
    ".\n",
    "   - The lines \n",
    "\n",
    "EfficientNetV2Classifier_labels = model_EfficientNetV2(created_imgs)\n",
    "\n",
    " and \n",
    "\n",
    "ShuffleNetV2Classifier_labels = model_ShuffleNetV2(created_imgs)\n",
    "\n",
    " obtain the predicted labels for the generated images from both classifiers.\n",
    "\n",
    "5. **Validation**:\n",
    "   - The function checks if the predicted labels from both classifiers match the provided labels using the condition \n",
    "\n",
    "if EfficientNetV2Classifier_labels == labels and ShuffleNetV2Classifier_labels == labels\n",
    "\n",
    ".\n",
    "   - If both classifiers correctly identify the generated images, the function returns the created images (\n",
    "\n",
    "return created_imgs\n",
    "\n",
    ").\n",
    "   - If either classifier fails to correctly identify the images, the function returns `None`.\n",
    "\n",
    "This function is useful for generating and validating synthetic images, ensuring that the generated images are realistic and correctly labeled according to both classifier models. This can be particularly valuable in scenarios where high-quality labeled data is needed for training or evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:51.370187Z",
     "iopub.status.busy": "2024-11-13T04:10:51.369492Z",
     "iopub.status.idle": "2024-11-13T04:10:51.385033Z",
     "shell.execute_reply": "2024-11-13T04:10:51.383932Z",
     "shell.execute_reply.started": "2024-11-13T04:10:51.370127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 64x64 -> 32x32\n",
    "        )\n",
    "        \n",
    "        # Encoder block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(base_features, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 2, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 32x32 -> 16x16\n",
    "        )\n",
    "        \n",
    "        # Encoder block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 4, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 16x16 -> 8x8\n",
    "        )\n",
    "        \n",
    "        # Encoder block 4\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 8, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each encoder block to the input\n",
    "        x = self.block1(x)  # 64x64 -> 32x32\n",
    "        x = self.block2(x)  # 32x32 -> 16x16\n",
    "        x = self.block3(x)  # 16x16 -> 8x8\n",
    "        x = self.block4(x)  # 8x8 -> 4x4\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a convolutional neural network (CNN) encoder using PyTorch. This encoder is designed to process input images through a series of convolutional layers, batch normalization, activation functions, and pooling layers, progressively reducing the spatial dimensions while increasing the feature depth.\n",
    "\n",
    "1. **Imports and Class Definition**:\n",
    "   - The code imports the necessary modules from PyTorch, including \n",
    "\n",
    "torch\n",
    "\n",
    " and \n",
    "\n",
    "torch.nn\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "CNNEncoder\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    ", which is the base class for all neural network modules in PyTorch.\n",
    "\n",
    "2. **Initialization (\n",
    "\n",
    "__init__\n",
    "\n",
    " method)**:\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes the encoder with two parameters: \n",
    "\n",
    "in_channels\n",
    "\n",
    " (default is 3 for RGB images) and \n",
    "\n",
    "base_features\n",
    "\n",
    " (default is 64).\n",
    "   - Four encoder blocks are defined within the \n",
    "\n",
    "__init__\n",
    "\n",
    " method, each consisting of convolutional layers, batch normalization, ReLU activation, and max pooling.\n",
    "\n",
    "3. **Encoder Block 1**:\n",
    "   - \n",
    "\n",
    "self.block1\n",
    "\n",
    " is a sequential container that includes:\n",
    "     - A convolutional layer with \n",
    "\n",
    "in_channels\n",
    "\n",
    " input channels and \n",
    "\n",
    "base_features\n",
    "\n",
    " output channels, a kernel size of 3, and padding of 1.\n",
    "     - Batch normalization for \n",
    "\n",
    "base_features\n",
    "\n",
    " channels.\n",
    "     - ReLU activation.\n",
    "     - Another convolutional layer with \n",
    "\n",
    "base_features\n",
    "\n",
    " input and output channels, a kernel size of 3, and padding of 1.\n",
    "     - Batch normalization and ReLU activation.\n",
    "     - Max pooling with a kernel size and stride of 2, reducing the spatial dimensions from 64x64 to 32x32.\n",
    "\n",
    "4. **Encoder Block 2**:\n",
    "   - \n",
    "\n",
    "self.block2\n",
    "\n",
    " is similar to block 1 but with:\n",
    "     - Convolutional layers that double the number of features to \n",
    "\n",
    "base_features * 2\n",
    "\n",
    ".\n",
    "     - Max pooling reduces the spatial dimensions from 32x32 to 16x16.\n",
    "\n",
    "5. **Encoder Block 3**:\n",
    "   - \n",
    "\n",
    "self.block3\n",
    "\n",
    " follows the same structure, further doubling the features to \n",
    "\n",
    "base_features * 4\n",
    "\n",
    ".\n",
    "     - Max pooling reduces the spatial dimensions from 16x16 to 8x8.\n",
    "\n",
    "6. **Encoder Block 4**:\n",
    "   - \n",
    "\n",
    "self.block4\n",
    "\n",
    " continues the pattern, doubling the features to \n",
    "\n",
    "base_features * 8\n",
    "\n",
    ".\n",
    "     - Max pooling reduces the spatial dimensions from 8x8 to 4x4.\n",
    "\n",
    "7. **Forward Method**:\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method defines the forward pass of the encoder.\n",
    "   - The input \n",
    "\n",
    "x\n",
    "\n",
    " is sequentially passed through each encoder block (\n",
    "\n",
    "block1\n",
    "\n",
    ", \n",
    "\n",
    "block2\n",
    "\n",
    ", \n",
    "\n",
    "block3\n",
    "\n",
    ", \n",
    "\n",
    "block4\n",
    "\n",
    "), progressively reducing its spatial dimensions and increasing its feature depth.\n",
    "   - The final output is returned after passing through all four blocks.\n",
    "\n",
    "This CNN encoder is designed to extract hierarchical features from input images, making it suitable for tasks such as image classification, segmentation, or as a feature extractor in more complex models. The progressive reduction in spatial dimensions and increase in feature depth allows the network to capture both local and global patterns in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:51.652953Z",
     "iopub.status.busy": "2024-11-13T04:10:51.652535Z",
     "iopub.status.idle": "2024-11-13T04:10:51.672149Z",
     "shell.execute_reply": "2024-11-13T04:10:51.671149Z",
     "shell.execute_reply.started": "2024-11-13T04:10:51.652911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        \n",
    "        # Linear transformations for multi-head attention\n",
    "        self.query_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Conv2d(self.head_dim, self.head_dim, kernel_size=1),\n",
    "                nn.Softmax(dim=-1)  # Softmax across the spatial dimension\n",
    "            ) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        # Channel attention to recalibrate feature maps\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(feature_dim, feature_dim // 16, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(feature_dim // 16, feature_dim, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Spatial attention to emphasize important regions in the spatial dimension\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Final 1x1 conv to combine outputs\n",
    "        self.output_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Compute query, key, and value maps for multi-head attention\n",
    "        queries = self.query_conv(features)  # [B, C, H, W]\n",
    "        keys = self.key_conv(features)       # [B, C, H, W]\n",
    "        values = self.value_conv(features)   # [B, C, H, W]\n",
    "        \n",
    "        B, C, H, W = queries.size()\n",
    "        queries = queries.view(B, self.num_heads, self.head_dim, H * W)  # [B, heads, head_dim, H*W]\n",
    "        keys = keys.view(B, self.num_heads, self.head_dim, H * W)        # [B, heads, head_dim, H*W]\n",
    "        values = values.view(B, self.num_heads, self.head_dim, H * W)    # [B, heads, head_dim, H*W]\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            attn_weights = torch.bmm(queries[:, i], keys[:, i].transpose(1, 2))  # [B, head_dim, head_dim]\n",
    "            attn_weights = self.attn_heads[i](attn_weights.view(B, self.head_dim, H, W))  # Apply learned attention map\n",
    "            attn_output = torch.bmm(attn_weights.view(B, self.head_dim, H * W), values[:, i])  # [B, head_dim, H*W]\n",
    "            attention_outputs.append(attn_output.view(B, self.head_dim, H, W))\n",
    "        \n",
    "        # Concatenate all attention head outputs\n",
    "        multi_head_output = torch.cat(attention_outputs, dim=1)  # [B, C, H, W]\n",
    "        \n",
    "        # Channel Attention\n",
    "        channel_attn_weights = self.channel_attention(multi_head_output)\n",
    "        channel_attn_output = multi_head_output * channel_attn_weights  # Element-wise multiplication (recalibration)\n",
    "        \n",
    "        # Spatial Attention\n",
    "        avg_pool = torch.mean(channel_attn_output, dim=1, keepdim=True)  # Average pooling across channels\n",
    "        max_pool = torch.max(channel_attn_output, dim=1, keepdim=True)[0]  # Max pooling across channels\n",
    "        spatial_attn_weights = self.spatial_attention(torch.cat([avg_pool, max_pool], dim=1))\n",
    "        spatial_attn_output = channel_attn_output * spatial_attn_weights  # Element-wise multiplication (spatial recalibration)\n",
    "        \n",
    "        # Final 1x1 conv to produce the final attention output\n",
    "        output = self.output_conv(spatial_attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines an \n",
    "\n",
    "AttentionModule\n",
    "\n",
    " class in PyTorch, which implements a sophisticated attention mechanism combining multi-head attention, channel attention, and spatial attention. This module is designed to enhance feature representations by focusing on important parts of the input data.\n",
    "\n",
    "1. **Class Definition and Initialization**:\n",
    "   - The \n",
    "\n",
    "AttentionModule\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    ", the base class for all neural network modules in PyTorch.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes the module with two parameters: \n",
    "\n",
    "feature_dim\n",
    "\n",
    ", which specifies the dimensionality of the input features, and \n",
    "\n",
    "num_heads\n",
    "\n",
    ", which defaults to 4 and specifies the number of attention heads.\n",
    "   - The \n",
    "\n",
    "head_dim\n",
    "\n",
    " is calculated by dividing \n",
    "\n",
    "feature_dim\n",
    "\n",
    " by \n",
    "\n",
    "num_heads\n",
    "\n",
    ", determining the dimensionality of each attention head.\n",
    "\n",
    "2. **Linear Transformations for Multi-Head Attention**:\n",
    "   - Three convolutional layers (\n",
    "\n",
    "query_conv\n",
    "\n",
    ", \n",
    "\n",
    "key_conv\n",
    "\n",
    ", and \n",
    "\n",
    "value_conv\n",
    "\n",
    ") are defined with a kernel size of 1. These layers transform the input features into query, key, and value maps, respectively, for the multi-head attention mechanism.\n",
    "\n",
    "3. **Multi-Head Attention Mechanism**:\n",
    "   - A \n",
    "\n",
    "ModuleList\n",
    "\n",
    " named \n",
    "\n",
    "attn_heads\n",
    "\n",
    " is created, containing \n",
    "\n",
    "num_heads\n",
    "\n",
    " sequential modules. Each module consists of a convolutional layer followed by a softmax activation function, which normalizes the attention weights across the spatial dimension.\n",
    "\n",
    "4. **Channel Attention**:\n",
    "   - The \n",
    "\n",
    "channel_attention\n",
    "\n",
    " sequential module recalibrates the feature maps by focusing on important channels. It includes:\n",
    "     - An adaptive average pooling layer that reduces the spatial dimensions to 1x1.\n",
    "     - Two convolutional layers with a ReLU activation in between.\n",
    "     - A sigmoid activation to produce the channel attention weights.\n",
    "\n",
    "5. **Spatial Attention**:\n",
    "   - The \n",
    "\n",
    "spatial_attention\n",
    "\n",
    " sequential module emphasizes important regions in the spatial dimension. It includes:\n",
    "     - A convolutional layer with a kernel size of 7 and padding of 3.\n",
    "     - A sigmoid activation to produce the spatial attention weights.\n",
    "\n",
    "6. **Final Convolutional Layer**:\n",
    "   - A final 1x1 convolutional layer (\n",
    "\n",
    "output_conv\n",
    "\n",
    ") is defined to combine the outputs of the attention mechanisms and produce the final attention-enhanced feature map.\n",
    "\n",
    "7. **Forward Method**:\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method defines the forward pass of the module.\n",
    "   - Query, key, and value maps are computed using the respective convolutional layers.\n",
    "   - The input features are reshaped to facilitate multi-head attention, splitting the feature dimension into multiple heads.\n",
    "   - For each attention head, the attention weights are computed using batch matrix multiplication (\n",
    "\n",
    "torch.bmm\n",
    "\n",
    "), and the attention output is obtained by applying the learned attention map to the value map.\n",
    "   - The outputs of all attention heads are concatenated along the feature dimension.\n",
    "   - Channel attention is applied to recalibrate the feature maps, followed by spatial attention to emphasize important spatial regions.\n",
    "   - The final attention-enhanced feature map is produced using the 1x1 convolutional layer and returned as the output.\n",
    "\n",
    "This \n",
    "\n",
    "AttentionModule\n",
    "\n",
    " class provides a comprehensive attention mechanism that can be integrated into larger neural network architectures to improve their ability to focus on relevant features and regions in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:52.047044Z",
     "iopub.status.busy": "2024-11-13T04:10:52.046315Z",
     "iopub.status.idle": "2024-11-13T04:10:52.058331Z",
     "shell.execute_reply": "2024-11-13T04:10:52.057386Z",
     "shell.execute_reply.started": "2024-11-13T04:10:52.047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MTUNet2(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4):\n",
    "        super(MTUNet2, self).__init__()\n",
    "        \n",
    "        # Complex CNN Encoder shared by both query and support\n",
    "        self.encoder = CNNEncoder(in_channels, base_features)\n",
    "        \n",
    "        # Complex Attention mechanism\n",
    "        self.attn_module = AttentionModule(feature_dim, num_heads=num_heads)\n",
    "        \n",
    "        # Classification Decoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_features*16*8*8, 1024),  # Updated linear layer input size for complex encoder\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query, support):\n",
    "        # Step 1: Extract features from the query image using the updated CNNEncoder\n",
    "        query_features = self.encoder(query)  # Query features are [B, 1024, 8, 8] based on complex CNNEncoder\n",
    "        \n",
    "        # Step 2: Extract and aggregate features from the support set\n",
    "        N = support.size(0)  # Number of support images\n",
    "        support_features = []\n",
    "        for i in range(N):\n",
    "            support_feature = self.encoder(support[i].unsqueeze(0))  # Each support image's features\n",
    "            support_features.append(support_feature)\n",
    "        \n",
    "        # Aggregate support features (using average pooling for simplicity)\n",
    "        support_features = torch.mean(torch.stack(support_features), dim=0)  # [B, 1024, 8, 8]\n",
    "        \n",
    "        # Step 3: Apply complex attention to both query and support features\n",
    "        query_attn = self.attn_module(query_features)  # Attention on query\n",
    "        support_attn = self.attn_module(support_features)  # Attention on support\n",
    "        \n",
    "        # Step 4: Combine query and support features via one-to-one concatenation\n",
    "        combined_features = torch.cat((query_attn, support_attn), dim=1)  # Concatenate along the channel dimension\n",
    "        # Combined features will be [B, 1024 + 1024 = 2048, 8, 8]\n",
    "        \n",
    "        # Step 5: Classification Decoder (use the combined query-support features)\n",
    "        classification_output = self.classifier(combined_features)\n",
    "        \n",
    "        return classification_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a neural network model named \n",
    "\n",
    "MTUNet2\n",
    "\n",
    " using PyTorch. This model is designed for tasks that involve both query and support images, leveraging a complex CNN encoder, an attention mechanism, and a classification decoder.\n",
    "\n",
    "1. **Class Definition and Initialization**:\n",
    "   - The \n",
    "\n",
    "MTUNet2\n",
    "\n",
    " class inherits from \n",
    "\n",
    "nn.Module\n",
    "\n",
    ", the base class for all neural network modules in PyTorch.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes the model with several parameters: \n",
    "\n",
    "in_channels\n",
    "\n",
    " (default is 3 for RGB images), \n",
    "\n",
    "base_features\n",
    "\n",
    " (default is 64), \n",
    "\n",
    "num_classes\n",
    "\n",
    " (default is 5), \n",
    "\n",
    "feature_dim\n",
    "\n",
    " (default is 512), and \n",
    "\n",
    "num_heads\n",
    "\n",
    " (default is 4).\n",
    "   - The model consists of three main components:\n",
    "     - A complex CNN encoder (\n",
    "\n",
    "self.encoder\n",
    "\n",
    ") shared by both query and support images, instantiated from the \n",
    "\n",
    "CNNEncoder\n",
    "\n",
    " class.\n",
    "     - An attention module (\n",
    "\n",
    "self.attn_module\n",
    "\n",
    ") instantiated from the \n",
    "\n",
    "AttentionModule\n",
    "\n",
    " class, which applies a complex attention mechanism to the features.\n",
    "     - A classification decoder (\n",
    "\n",
    "self.classifier\n",
    "\n",
    "), defined as a sequential module that flattens the input, applies a linear transformation followed by a ReLU activation, and then another linear transformation to produce the final class predictions.\n",
    "\n",
    "2. **Forward Method**:\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method defines the forward pass of the model, taking two inputs: \n",
    "\n",
    "query\n",
    "\n",
    " and \n",
    "\n",
    "support\n",
    "\n",
    ".\n",
    "   - **Step 1**: Extract features from the query image using the CNN encoder. The output features have dimensions `[B, 1024, 8, 8]`, where `B` is the batch size.\n",
    "   - **Step 2**: Extract and aggregate features from the support set. The support set contains \n",
    "\n",
    "N\n",
    "\n",
    " images. Each support image is passed through the CNN encoder, and the features are aggregated using average pooling to produce a single feature map with dimensions `[B, 1024, 8, 8]`.\n",
    "   - **Step 3**: Apply the attention module to both query and support features. The attention module enhances the features by focusing on important parts of the input data.\n",
    "   - **Step 4**: Combine the query and support features by concatenating them along the channel dimension, resulting in combined features with dimensions `[B, 2048, 8, 8]`.\n",
    "   - **Step 5**: Pass the combined features through the classification decoder to produce the final class predictions.\n",
    "\n",
    "This model architecture is designed to handle tasks that require the integration of information from both query and support images, making it suitable for applications such as few-shot learning or meta-learning. The use of a complex CNN encoder, attention mechanism, and classification decoder allows the model to effectively extract, enhance, and classify features from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:16:30.480666Z",
     "iopub.status.busy": "2024-11-13T04:16:30.480196Z",
     "iopub.status.idle": "2024-11-13T04:16:30.551491Z",
     "shell.execute_reply": "2024-11-13T04:16:30.550424Z",
     "shell.execute_reply.started": "2024-11-13T04:16:30.480627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MTUNet2(in_channels=3, base_features=64, num_classes=5)\n",
    "criterion_cls = nn.CrossEntropyLoss()  # For classification output\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion_cls, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data, target in enumerate(train_loader):\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Creating support set\n",
    "        support = create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, target, noise_dim=128)\n",
    "\n",
    "        # Forward pass\n",
    "        classification_output = model(data, support)  # Assuming same data for support set in FSL\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_cls = criterion_cls(classification_output, target)  # Assuming target is for classification\n",
    "        \n",
    "        # Backward pass\n",
    "        loss_cls.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss_cls.item()\n",
    "\n",
    "        # Compute accuracy for classification output\n",
    "        _, predicted = torch.max(classification_output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct_cls += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_cls / total\n",
    "    \n",
    "    return running_loss / len(train_loader), accuracy\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion_cls):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_cls = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            classification_output = model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_cls = criterion_cls(classification_output, target)\n",
    "            \n",
    "            test_loss += loss_cls.item()\n",
    "\n",
    "            # Compute accuracy for classification output\n",
    "            _, predicted = torch.max(classification_output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct_cls += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_cls / total\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "num_epochs = 1#500\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, criterion_cls, optimizer, epoch)\n",
    "    print(f'Epoch [{epoch}], Training Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion_cls)\n",
    "    print(f'Epoch [{epoch}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines the setup and training loop for a neural network model named \n",
    "\n",
    "MTUNet2\n",
    "\n",
    " using PyTorch. This includes initializing the model, defining the loss function and optimizer, and implementing the training and evaluation functions.\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The model \n",
    "\n",
    "MTUNet2\n",
    "\n",
    " is instantiated with 3 input channels, 64 base features, and 5 output classes.\n",
    "   - The loss function used is \n",
    "\n",
    "nn.CrossEntropyLoss()\n",
    "\n",
    ", which is suitable for multi-class classification tasks.\n",
    "   - The optimizer used is Adam (\n",
    "\n",
    "optim.Adam\n",
    "\n",
    "), with a learning rate of 0.001, to update the model parameters.\n",
    "\n",
    "2. **Training Function**:\n",
    "   - The \n",
    "\n",
    "train\n",
    "\n",
    " function is defined to train the model for one epoch. It takes the model, training data loader, loss function, optimizer, and the current epoch number as input.\n",
    "   - The model is set to training mode using \n",
    "\n",
    "model.train()\n",
    "\n",
    ".\n",
    "   - A running loss variable is initialized to accumulate the loss over the epoch.\n",
    "   - The function iterates over batches of data from the training loader. For each batch:\n",
    "     - Gradients are cleared using \n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    ".\n",
    "     - A support set is created using the \n",
    "\n",
    "create_support_set\n",
    "\n",
    " function, which generates images and validates them using two classifier models.\n",
    "     - A forward pass is performed by passing the data and support set through the model.\n",
    "     - The classification loss is computed using the loss function.\n",
    "     - The loss is backpropagated, and the optimizer steps are performed to update the model parameters.\n",
    "     - The running loss is accumulated, and the classification accuracy is computed.\n",
    "   - The function returns the average loss and accuracy for the epoch.\n",
    "\n",
    "3. **Evaluation Function**:\n",
    "   - The \n",
    "\n",
    "evaluate\n",
    "\n",
    " function is defined to evaluate the model on the test set. It takes the model, test data loader, and loss function as input.\n",
    "   - The model is set to evaluation mode using \n",
    "\n",
    "model.eval()\n",
    "\n",
    ".\n",
    "   - Variables for test loss and correct predictions are initialized.\n",
    "   - The function iterates over batches of data from the test loader. For each batch:\n",
    "     - A forward pass is performed by passing the data through the model.\n",
    "     - The classification loss is computed and accumulated.\n",
    "     - The classification accuracy is computed.\n",
    "   - The function returns the average loss and accuracy for the test set.\n",
    "\n",
    "4. **Main Training Loop**:\n",
    "   - The main training loop runs for a specified number of epochs (\n",
    "\n",
    "num_epochs\n",
    "\n",
    ").\n",
    "   - For each epoch, the \n",
    "\n",
    "train\n",
    "\n",
    " function is called to train the model, and the training loss and accuracy are printed.\n",
    "   - The \n",
    "\n",
    "evaluate\n",
    "\n",
    " function is called to evaluate the model on the test set, and the test loss and accuracy are printed.\n",
    "\n",
    "This code provides a comprehensive framework for training and evaluating the \n",
    "\n",
    "MTUNet2\n",
    "\n",
    " model, including data loading, model training, loss computation, and accuracy evaluation. The use of a support set and attention mechanism in the model allows for advanced feature extraction and classification capabilities."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
