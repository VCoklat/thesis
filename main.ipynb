{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:15.856177Z",
     "iopub.status.busy": "2024-11-13T03:37:15.855257Z",
     "iopub.status.idle": "2024-11-13T03:37:21.607703Z",
     "shell.execute_reply": "2024-11-13T03:37:21.606712Z",
     "shell.execute_reply.started": "2024-11-13T03:37:15.856134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet imports several essential libraries and modules that are commonly used in data science and machine learning projects.\n",
    "\n",
    "1. **os**: This module provides a way of using operating system-dependent functionality like reading or writing to the file system. It is useful for handling file paths and directories.\n",
    "\n",
    "2. **pandas as pd**: Pandas is a powerful data manipulation and analysis library for Python. It provides data structures like DataFrames, which are essential for handling and analyzing structured data. The alias \n",
    "\n",
    "pd\n",
    "\n",
    " is a common convention to simplify the usage of the library.\n",
    "\n",
    "3. **train_test_split from sklearn.model_selection**: This function from the Scikit-learn library is used to split datasets into training and testing sets. It is crucial for evaluating the performance of machine learning models by training them on one subset of the data and testing them on another.\n",
    "\n",
    "4. **transforms and datasets from torchvision**: These modules are part of the Torchvision library, which is used in conjunction with PyTorch for computer vision tasks. \n",
    "\n",
    "transforms\n",
    "\n",
    " provides common image transformations for data augmentation and preprocessing, while \n",
    "\n",
    "datasets\n",
    "\n",
    " offers access to popular datasets like CIFAR-10 and ImageNet.\n",
    "\n",
    "5. **Dataset and DataLoader from torch.utils.data**: These classes are part of PyTorch's data loading utilities. \n",
    "\n",
    "Dataset\n",
    "\n",
    " is an abstract class representing a dataset, and \n",
    "\n",
    "DataLoader\n",
    "\n",
    " is used to load data in batches, shuffle it, and handle multiprocessing for efficient data loading.\n",
    "\n",
    "6. **Image from PIL**: The Python Imaging Library (PIL) is used for opening, manipulating, and saving many different image file formats. The \n",
    "\n",
    "Image\n",
    "\n",
    " class is specifically used to work with image data, which is often necessary in computer vision tasks.\n",
    "\n",
    "Together, these imports set up the environment for a machine learning workflow that involves data manipulation, dataset splitting, image processing, and efficient data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.609762Z",
     "iopub.status.busy": "2024-11-13T03:37:21.60933Z",
     "iopub.status.idle": "2024-11-13T03:37:21.619788Z",
     "shell.execute_reply": "2024-11-13T03:37:21.618742Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.60973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dirs, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs  # List of directories\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Look up the image name\n",
    "        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "        \n",
    "        # Search for the image in the directories\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image {img_name} not found in specified directories.\")\n",
    "        \n",
    "        # Get the label\n",
    "        label = self.data.iloc[idx]['dx']  # Diagnosis column\n",
    "        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n",
    "        label = label_map[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines a custom dataset class named \n",
    "\n",
    "HAM10000Dataset\n",
    "\n",
    " that inherits from PyTorch's \n",
    "\n",
    "Dataset\n",
    "\n",
    " class. This class is designed to handle the HAM10000 dataset, which contains images of skin lesions along with their corresponding diagnoses.\n",
    "\n",
    "1. **Initialization (\n",
    "\n",
    "__init__\n",
    "\n",
    " method)**: The constructor takes three parameters: \n",
    "\n",
    "csv_file\n",
    "\n",
    ", \n",
    "\n",
    "img_dirs\n",
    "\n",
    ", and \n",
    "\n",
    "transform\n",
    "\n",
    ". The \n",
    "\n",
    "csv_file\n",
    "\n",
    " parameter is the path to a CSV file containing metadata about the images, such as their filenames and diagnoses. The \n",
    "\n",
    "img_dirs\n",
    "\n",
    " parameter is a list of directories where the images are stored. The \n",
    "\n",
    "transform\n",
    "\n",
    " parameter is optional and can be used to apply transformations to the images (e.g., data augmentation). The constructor reads the CSV file into a pandas DataFrame and stores the image directories and transform function as instance variables.\n",
    "\n",
    "2. **Length (\n",
    "\n",
    "__len__\n",
    "\n",
    " method)**: This method returns the number of samples in the dataset by returning the length of the DataFrame. This is a required method for PyTorch datasets, as it allows PyTorch to know how many samples are available.\n",
    "\n",
    "3. **Get Item (\n",
    "\n",
    "__getitem__\n",
    "\n",
    " method)**: This method retrieves a single sample from the dataset. It takes an index \n",
    "\n",
    "idx\n",
    "\n",
    " as input and performs the following steps:\n",
    "   - It constructs the image filename by appending '.jpg' to the `image_id` from the DataFrame.\n",
    "   - It searches for the image file in the specified directories. If the image is found, it is opened and converted to RGB format. If the image is not found in any directory, a \n",
    "\n",
    "FileNotFoundError\n",
    "\n",
    " is raised.\n",
    "   - It retrieves the label (diagnosis) for the image from the DataFrame. The labels are mapped to numerical values using a dictionary that assigns a unique index to each unique diagnosis.\n",
    "   - If a transform function is provided, it is applied to the image.\n",
    "   - The method returns a tuple containing the image and its corresponding label.\n",
    "\n",
    "This custom dataset class allows for efficient loading and preprocessing of the HAM10000 dataset, making it suitable for training machine learning models in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.621193Z",
     "iopub.status.busy": "2024-11-13T03:37:21.620913Z",
     "iopub.status.idle": "2024-11-13T03:37:21.675607Z",
     "shell.execute_reply": "2024-11-13T03:37:21.674708Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.621164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Check the number of unique images in metadata\n",
    "print(f\"Total images in metadata: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for loading and inspecting metadata for the HAM10000 dataset, which is a collection of skin lesion images and their associated information.\n",
    "\n",
    "1. **Setting the Metadata Path**: The variable \n",
    "\n",
    "metadata_path\n",
    "\n",
    " is assigned the file path to the CSV file containing the metadata for the HAM10000 dataset. This path points to a file named `HAM10000_metadata.csv` located in the directory `../input/skin-cancer-mnist-ham10000/`.\n",
    "\n",
    "2. **Loading the Metadata**: The \n",
    "\n",
    "pd.read_csv(metadata_path)\n",
    "\n",
    " function is used to read the CSV file into a pandas DataFrame named \n",
    "\n",
    "metadata\n",
    "\n",
    ". This DataFrame will contain various columns with information about each image, such as the image ID, diagnosis, and other relevant details.\n",
    "\n",
    "3. **Checking the Number of Unique Images**: The \n",
    "\n",
    "print\n",
    "\n",
    " statement outputs the total number of images listed in the metadata. The \n",
    "\n",
    "len(metadata)\n",
    "\n",
    " function returns the number of rows in the DataFrame, which corresponds to the number of unique images described in the metadata file.\n",
    "\n",
    "This code is essential for verifying that the metadata has been loaded correctly and for understanding the size of the dataset, which is a crucial step before proceeding with further data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.679097Z",
     "iopub.status.busy": "2024-11-13T03:37:21.67821Z",
     "iopub.status.idle": "2024-11-13T03:37:21.747117Z",
     "shell.execute_reply": "2024-11-13T03:37:21.746374Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.679046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split metadata into train and test sets\n",
    "train_metadata, test_metadata = train_test_split(metadata, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save split metadata for easier loading\n",
    "train_metadata.to_csv(\"train_metadata.csv\", index=False)\n",
    "test_metadata.to_csv(\"test_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for splitting the metadata of the HAM10000 dataset into training and testing sets and then saving these subsets to CSV files for easier future access.\n",
    "\n",
    "1. **Splitting the Metadata**: The \n",
    "\n",
    "train_test_split\n",
    "\n",
    " function from Scikit-learn is used to divide the \n",
    "\n",
    "metadata\n",
    "\n",
    " DataFrame into two subsets: \n",
    "\n",
    "train_metadata\n",
    "\n",
    " and \n",
    "\n",
    "test_metadata\n",
    "\n",
    ". The \n",
    "\n",
    "test_size=0.2\n",
    "\n",
    " parameter specifies that 20% of the data should be allocated to the test set, while the remaining 80% will be used for training. The \n",
    "\n",
    "random_state=42\n",
    "\n",
    " parameter ensures that the split is reproducible, meaning that the same split will be obtained each time the code is run with this seed value.\n",
    "\n",
    "2. **Saving the Split Metadata**: The \n",
    "\n",
    "to_csv\n",
    "\n",
    " method of the pandas DataFrame is used to save the training and testing metadata to CSV files named \n",
    "\n",
    "train_metadata.csv\n",
    "\n",
    " and \n",
    "\n",
    "test_metadata.csv\n",
    "\n",
    ", respectively. The \n",
    "\n",
    "index=False\n",
    "\n",
    " parameter ensures that the row indices are not included in the saved CSV files, keeping the files clean and focused on the actual data.\n",
    "\n",
    "By splitting the metadata into training and testing sets, this code prepares the dataset for model training and evaluation. Saving these subsets to CSV files allows for quick and easy loading in future steps of the workflow, ensuring that the same data split is used consistently throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.748396Z",
     "iopub.status.busy": "2024-11-13T03:37:21.748099Z",
     "iopub.status.idle": "2024-11-13T03:37:21.752645Z",
     "shell.execute_reply": "2024-11-13T03:37:21.751629Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.748365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Directories containing images\n",
    "image_dirs = [\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.754216Z",
     "iopub.status.busy": "2024-11-13T03:37:21.753872Z",
     "iopub.status.idle": "2024-11-13T03:37:21.763699Z",
     "shell.execute_reply": "2024-11-13T03:37:21.762723Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.754185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images to 256x256\n",
    "    transforms.ToTensor(),         # Convert to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:22.156079Z",
     "iopub.status.busy": "2024-11-13T03:37:22.155683Z",
     "iopub.status.idle": "2024-11-13T03:37:22.189694Z",
     "shell.execute_reply": "2024-11-13T03:37:22.188693Z",
     "shell.execute_reply.started": "2024-11-13T03:37:22.156041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = HAM10000Dataset(csv_file=\"train_metadata.csv\", img_dirs=image_dirs, transform=transform)\n",
    "test_dataset = HAM10000Dataset(csv_file=\"test_metadata.csv\", img_dirs=image_dirs, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:23.031142Z",
     "iopub.status.busy": "2024-11-13T03:37:23.030695Z",
     "iopub.status.idle": "2024-11-13T03:37:23.883755Z",
     "shell.execute_reply": "2024-11-13T03:37:23.882658Z",
     "shell.execute_reply.started": "2024-11-13T03:37:23.031103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize one batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")\n",
    "\n",
    "# Display first 4 images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Denormalize\n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:40:01.297608Z",
     "iopub.status.busy": "2024-11-13T03:40:01.296762Z",
     "iopub.status.idle": "2024-11-13T03:40:02.565467Z",
     "shell.execute_reply": "2024-11-13T03:40:02.564406Z",
     "shell.execute_reply.started": "2024-11-13T03:40:01.297566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch, -1, width * height)\n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        proj_value = self.value_conv(x).view(batch, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, img_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_channels = img_channels\n",
    "        self.img_size = img_size\n",
    "        self.init_size = img_size // 8  # Downsample by 8 (adjusted for 64x64 output)\n",
    "        self.fc = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128),\n",
    "            SelfAttention(128),  # Self-Attention after first upscale\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttention(32),  # Self-Attention in the middle layers\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(32, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Normalize output to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n",
    "        out = self.upsample(out)\n",
    "        img = self.final_layer(out)\n",
    "        return img\n",
    "\n",
    "# Instantiate the generator\n",
    "latent_dim = 100  # Size of latent vector\n",
    "img_channels = 3  # RGB images\n",
    "img_size = 64  # Output image size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(latent_dim, img_channels, img_size).to(device)\n",
    "\n",
    "# Test the generator\n",
    "z = torch.randn(4, latent_dim).to(device)  # Random latent vector (batch size = 4)\n",
    "generated_images = generator(z)\n",
    "\n",
    "print(f\"Generated image shape: {generated_images.shape}\")  # Should be [4, 3, 64, 64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:41:32.233766Z",
     "iopub.status.busy": "2024-11-13T03:41:32.233316Z",
     "iopub.status.idle": "2024-11-13T03:41:32.46463Z",
     "shell.execute_reply": "2024-11-13T03:41:32.463541Z",
     "shell.execute_reply.started": "2024-11-13T03:41:32.233725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch, -1, width * height)\n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        proj_value = self.value_conv(x).view(batch, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if downsample else nn.Identity()\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AvgPool2d(2) if downsample else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, img_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ResidualBlock(img_channels, 64, downsample=True),            # 64x64 -> 32x32\n",
    "            SelfAttention(64),\n",
    "            ResidualBlock(64, 128, downsample=True),           # 32x32 -> 16x16\n",
    "            SelfAttention(128),\n",
    "            ResidualBlock(128, 256, downsample=True),          # 16x16 -> 8x8\n",
    "            ResidualBlock(256, 512, downsample=True)           # 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1),  # Final score output\n",
    "            nn.Sigmoid()  # Outputs probability of being real or fake\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = self.final_layer(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the discriminator\n",
    "img_channels = 3  # RGB images\n",
    "img_size = 64  # Input image size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "discriminator = Discriminator(img_channels, img_size).to(device)\n",
    "\n",
    "# Test the discriminator\n",
    "batch_size = 4\n",
    "test_images = torch.randn(batch_size, img_channels, img_size, img_size).to(device)  # Fake images batch\n",
    "output = discriminator(test_images)\n",
    "\n",
    "print(f\"Discriminator output shape: {output.shape}\")  # Should be [4, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:46:53.871919Z",
     "iopub.status.busy": "2024-11-13T03:46:53.871058Z",
     "iopub.status.idle": "2024-11-13T03:46:53.876521Z",
     "shell.execute_reply": "2024-11-13T03:46:53.87544Z",
     "shell.execute_reply.started": "2024-11-13T03:46:53.871864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:47:38.582788Z",
     "iopub.status.busy": "2024-11-13T03:47:38.58237Z",
     "iopub.status.idle": "2024-11-13T03:47:38.671075Z",
     "shell.execute_reply": "2024-11-13T03:47:38.67003Z",
     "shell.execute_reply.started": "2024-11-13T03:47:38.582751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1000, lr=0.0002, beta1=0.5, beta2=0.999):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        epoch_loss_G = 0.0\n",
    "        epoch_loss_D = 0.0\n",
    "\n",
    "        for real_images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n",
    "            fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():  # Mixed precision training\n",
    "                generated_images = generator(z)\n",
    "                g_loss = criterion(discriminator(generated_images), valid)\n",
    "\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(optimizer_G)\n",
    "            scaler.update()\n",
    "            epoch_loss_G += g_loss.item()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                real_loss = criterion(discriminator(real_images), valid)\n",
    "                fake_loss = criterion(discriminator(generated_images.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(optimizer_D)\n",
    "            scaler.update()\n",
    "            epoch_loss_D += d_loss.item()\n",
    "\n",
    "            # Clear cache to reduce memory fragmentation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Generator Loss: {epoch_loss_G:.4f} | Discriminator Loss: {epoch_loss_D:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "\n",
    "# Call the train_gan function with the train_loader, generator, and discriminator\n",
    "train_gan(generator, discriminator, train_loader, latent_dim, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:55:49.315186Z",
     "iopub.status.busy": "2024-11-13T03:55:49.314203Z",
     "iopub.status.idle": "2024-11-13T03:55:49.3272Z",
     "shell.execute_reply": "2024-11-13T03:55:49.326207Z",
     "shell.execute_reply.started": "2024-11-13T03:55:49.315144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define EfficientNetV2 model for HAM10000 with 7 classes\n",
    "class EfficientNetV2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n",
    "        super(EfficientNetV2Classifier, self).__init__()\n",
    "        self.efficientnet_v2 = models.efficientnet_v2_s(pretrained=True)\n",
    "        \n",
    "        in_features = self.efficientnet_v2.classifier[1].in_features\n",
    "        self.efficientnet_v2.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet_v2(x)\n",
    "\n",
    "# Initialize the model\n",
    "model_EfficientNetV2 = EfficientNetV2Classifier(num_classes=7)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_EfficientNetV2.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            print(\"done\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = total_correct / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validation after each epoch\n",
    "        validate_model(model, test_loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / len(test_loader.dataset)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "model_EfficientNetV2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model_EfficientNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:00:06.453375Z",
     "iopub.status.busy": "2024-11-13T04:00:06.452961Z",
     "iopub.status.idle": "2024-11-13T04:00:06.464457Z",
     "shell.execute_reply": "2024-11-13T04:00:06.463412Z",
     "shell.execute_reply.started": "2024-11-13T04:00:06.453324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define ShuffleNetV2 model for HAM10000 with 7 classes\n",
    "class ShuffleNetV2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n",
    "        super(ShuffleNetV2Classifier, self).__init__()\n",
    "        self.shufflenet_v2 = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "        \n",
    "        # Modify the last fully connected layer to match the number of classes\n",
    "        in_features = self.shufflenet_v2.fc.in_features\n",
    "        self.shufflenet_v2.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shufflenet_v2(x)\n",
    "\n",
    "# Initialize the model\n",
    "model_ShuffleNetV2 = ShuffleNetV2Classifier(num_classes=7)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ShuffleNetV2.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = total_correct / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validation after each epoch\n",
    "        validate_model(model, test_loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / len(test_loader.dataset)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "model_ShuffleNetV2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model_ShuffleNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:00:30.917234Z",
     "iopub.status.busy": "2024-11-13T04:00:30.916705Z",
     "iopub.status.idle": "2024-11-13T04:00:30.924325Z",
     "shell.execute_reply": "2024-11-13T04:00:30.923203Z",
     "shell.execute_reply.started": "2024-11-13T04:00:30.917194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, labels, noise_dim=128):\n",
    "    noise = torch.randn(batch_size, noise_dim)  # Random noise for generator\n",
    "    created_imgs = generator(noise, labels) \n",
    "    EfficientNetV2Classifier_labels = model_EfficientNetV2(created_imgs)\n",
    "    ShuffleNetV2Classifier_labels = model_ShuffleNetV2(created_imgs)\n",
    "    if EfficientNetV2Classifier_labels == labels and ShuffleNetV2Classifier_labels == labels:\n",
    "        return created_imgs\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:51.370187Z",
     "iopub.status.busy": "2024-11-13T04:10:51.369492Z",
     "iopub.status.idle": "2024-11-13T04:10:51.385033Z",
     "shell.execute_reply": "2024-11-13T04:10:51.383932Z",
     "shell.execute_reply.started": "2024-11-13T04:10:51.370127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 64x64 -> 32x32\n",
    "        )\n",
    "        \n",
    "        # Encoder block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(base_features, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 2, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 32x32 -> 16x16\n",
    "        )\n",
    "        \n",
    "        # Encoder block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 4, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 16x16 -> 8x8\n",
    "        )\n",
    "        \n",
    "        # Encoder block 4\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 8, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each encoder block to the input\n",
    "        x = self.block1(x)  # 64x64 -> 32x32\n",
    "        x = self.block2(x)  # 32x32 -> 16x16\n",
    "        x = self.block3(x)  # 16x16 -> 8x8\n",
    "        x = self.block4(x)  # 8x8 -> 4x4\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:51.652953Z",
     "iopub.status.busy": "2024-11-13T04:10:51.652535Z",
     "iopub.status.idle": "2024-11-13T04:10:51.672149Z",
     "shell.execute_reply": "2024-11-13T04:10:51.671149Z",
     "shell.execute_reply.started": "2024-11-13T04:10:51.652911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        \n",
    "        # Linear transformations for multi-head attention\n",
    "        self.query_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Conv2d(self.head_dim, self.head_dim, kernel_size=1),\n",
    "                nn.Softmax(dim=-1)  # Softmax across the spatial dimension\n",
    "            ) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        # Channel attention to recalibrate feature maps\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(feature_dim, feature_dim // 16, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(feature_dim // 16, feature_dim, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Spatial attention to emphasize important regions in the spatial dimension\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Final 1x1 conv to combine outputs\n",
    "        self.output_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Compute query, key, and value maps for multi-head attention\n",
    "        queries = self.query_conv(features)  # [B, C, H, W]\n",
    "        keys = self.key_conv(features)       # [B, C, H, W]\n",
    "        values = self.value_conv(features)   # [B, C, H, W]\n",
    "        \n",
    "        B, C, H, W = queries.size()\n",
    "        queries = queries.view(B, self.num_heads, self.head_dim, H * W)  # [B, heads, head_dim, H*W]\n",
    "        keys = keys.view(B, self.num_heads, self.head_dim, H * W)        # [B, heads, head_dim, H*W]\n",
    "        values = values.view(B, self.num_heads, self.head_dim, H * W)    # [B, heads, head_dim, H*W]\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            attn_weights = torch.bmm(queries[:, i], keys[:, i].transpose(1, 2))  # [B, head_dim, head_dim]\n",
    "            attn_weights = self.attn_heads[i](attn_weights.view(B, self.head_dim, H, W))  # Apply learned attention map\n",
    "            attn_output = torch.bmm(attn_weights.view(B, self.head_dim, H * W), values[:, i])  # [B, head_dim, H*W]\n",
    "            attention_outputs.append(attn_output.view(B, self.head_dim, H, W))\n",
    "        \n",
    "        # Concatenate all attention head outputs\n",
    "        multi_head_output = torch.cat(attention_outputs, dim=1)  # [B, C, H, W]\n",
    "        \n",
    "        # Channel Attention\n",
    "        channel_attn_weights = self.channel_attention(multi_head_output)\n",
    "        channel_attn_output = multi_head_output * channel_attn_weights  # Element-wise multiplication (recalibration)\n",
    "        \n",
    "        # Spatial Attention\n",
    "        avg_pool = torch.mean(channel_attn_output, dim=1, keepdim=True)  # Average pooling across channels\n",
    "        max_pool = torch.max(channel_attn_output, dim=1, keepdim=True)[0]  # Max pooling across channels\n",
    "        spatial_attn_weights = self.spatial_attention(torch.cat([avg_pool, max_pool], dim=1))\n",
    "        spatial_attn_output = channel_attn_output * spatial_attn_weights  # Element-wise multiplication (spatial recalibration)\n",
    "        \n",
    "        # Final 1x1 conv to produce the final attention output\n",
    "        output = self.output_conv(spatial_attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:52.047044Z",
     "iopub.status.busy": "2024-11-13T04:10:52.046315Z",
     "iopub.status.idle": "2024-11-13T04:10:52.058331Z",
     "shell.execute_reply": "2024-11-13T04:10:52.057386Z",
     "shell.execute_reply.started": "2024-11-13T04:10:52.047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MTUNet2(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4):\n",
    "        super(MTUNet2, self).__init__()\n",
    "        \n",
    "        # Complex CNN Encoder shared by both query and support\n",
    "        self.encoder = CNNEncoder(in_channels, base_features)\n",
    "        \n",
    "        # Complex Attention mechanism\n",
    "        self.attn_module = AttentionModule(feature_dim, num_heads=num_heads)\n",
    "        \n",
    "        # Classification Decoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_features*16*8*8, 1024),  # Updated linear layer input size for complex encoder\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query, support):\n",
    "        # Step 1: Extract features from the query image using the updated CNNEncoder\n",
    "        query_features = self.encoder(query)  # Query features are [B, 1024, 8, 8] based on complex CNNEncoder\n",
    "        \n",
    "        # Step 2: Extract and aggregate features from the support set\n",
    "        N = support.size(0)  # Number of support images\n",
    "        support_features = []\n",
    "        for i in range(N):\n",
    "            support_feature = self.encoder(support[i].unsqueeze(0))  # Each support image's features\n",
    "            support_features.append(support_feature)\n",
    "        \n",
    "        # Aggregate support features (using average pooling for simplicity)\n",
    "        support_features = torch.mean(torch.stack(support_features), dim=0)  # [B, 1024, 8, 8]\n",
    "        \n",
    "        # Step 3: Apply complex attention to both query and support features\n",
    "        query_attn = self.attn_module(query_features)  # Attention on query\n",
    "        support_attn = self.attn_module(support_features)  # Attention on support\n",
    "        \n",
    "        # Step 4: Combine query and support features via one-to-one concatenation\n",
    "        combined_features = torch.cat((query_attn, support_attn), dim=1)  # Concatenate along the channel dimension\n",
    "        # Combined features will be [B, 1024 + 1024 = 2048, 8, 8]\n",
    "        \n",
    "        # Step 5: Classification Decoder (use the combined query-support features)\n",
    "        classification_output = self.classifier(combined_features)\n",
    "        \n",
    "        return classification_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:16:30.480666Z",
     "iopub.status.busy": "2024-11-13T04:16:30.480196Z",
     "iopub.status.idle": "2024-11-13T04:16:30.551491Z",
     "shell.execute_reply": "2024-11-13T04:16:30.550424Z",
     "shell.execute_reply.started": "2024-11-13T04:16:30.480627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MTUNet2(in_channels=3, base_features=64, num_classes=5)\n",
    "criterion_cls = nn.CrossEntropyLoss()  # For classification output\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion_cls, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data, target in enumerate(train_loader):\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Creating support set\n",
    "        support = create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, target, noise_dim=128)\n",
    "\n",
    "        # Forward pass\n",
    "        classification_output = model(data, support)  # Assuming same data for support set in FSL\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_cls = criterion_cls(classification_output, target)  # Assuming target is for classification\n",
    "        \n",
    "        # Backward pass\n",
    "        loss_cls.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss_cls.item()\n",
    "\n",
    "        # Compute accuracy for classification output\n",
    "        _, predicted = torch.max(classification_output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct_cls += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_cls / total\n",
    "    \n",
    "    return running_loss / len(train_loader), accuracy\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion_cls):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_cls = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            classification_output = model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_cls = criterion_cls(classification_output, target)\n",
    "            \n",
    "            test_loss += loss_cls.item()\n",
    "\n",
    "            # Compute accuracy for classification output\n",
    "            _, predicted = torch.max(classification_output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct_cls += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_cls / total\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, criterion_cls, optimizer, epoch)\n",
    "    print(f'Epoch [{epoch}], Training Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion_cls)\n",
    "    print(f'Epoch [{epoch}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
