{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torchvision import transforms, datasets\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom PIL import Image","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:15.856177Z","iopub.status.busy":"2024-11-13T03:37:15.855257Z","iopub.status.idle":"2024-11-13T03:37:21.607703Z","shell.execute_reply":"2024-11-13T03:37:21.606712Z","shell.execute_reply.started":"2024-11-13T03:37:15.856134Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet imports several essential libraries and modules that are commonly used in data science, machine learning, and deep learning projects.\n\n\n\n1. **os**: This module provides a way of using operating system-dependent functionality like reading or writing to the file system. It is useful for tasks such as navigating the file system, handling file paths, and manipulating directories.\n\n\n2. **pandas as pd**: Pandas is a powerful data manipulation and analysis library for Python. It provides data structures like DataFrames, which are particularly useful for handling and analyzing structured data. The alias pd  is a common convention to make the code more concise.\n   \n4. **train_test_split from sklearn.model_selection**: This function is part of the scikit-learn library, which is widely used for machine learning tasks. The train_test_split function is used to split a dataset into training and testing sets, which is a crucial step in building and evaluating machine learning models.\n\n4. **transforms and datasets from torchvision**: Torchvision is a library that provides tools for computer vision tasks. The transforms module includes common image transformations that are often used in preprocessing steps, such as resizing, cropping, and normalizing images. The datasets module provides access to popular datasets and utilities to load them.\n\n5. **Dataset and DataLoader from torch.utils.data**: These classes are part of PyTorch, a deep learning framework. The Dataset class is an abstract class representing a dataset, and the DataLoader class provides an iterable over a dataset, with support for batching, shuffling, and parallel data loading. These are essential for efficiently handling large datasets during training and evaluation of deep learning models.\n\n6. **Image from PIL**: The Python Imaging Library (PIL) is a library that adds image processing capabilities to Python. The Image module is used for opening, manipulating, and saving many different image file formats. It is often used in conjunction with torchvision for image preprocessing tasks.\n\nTogether, these imports set up a robust environment for handling data, preprocessing images, and building machine learning and deep learning models.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"class HAM10000Dataset(Dataset):\n\n    def __init__(self, csv_file, img_dirs, transform=None):\n\n        self.data = pd.read_csv(csv_file)\n\n        self.img_dirs = img_dirs  # List of directories\n\n        self.transform = transform\n\n\n\n    def __len__(self):\n\n        return len(self.data)\n\n\n\n    def __getitem__(self, idx):\n\n        # Look up the image name\n\n        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n\n        \n\n        # Search for the image in the directories\n\n        for img_dir in self.img_dirs:\n\n            img_path = os.path.join(img_dir, img_name)\n\n            if os.path.exists(img_path):\n\n                image = Image.open(img_path).convert('RGB')\n\n                break\n\n        else:\n\n            raise FileNotFoundError(f\"Image {img_name} not found in specified directories.\")\n\n        \n\n        # Get the label\n\n        label = self.data.iloc[idx]['dx']  # Diagnosis column\n\n        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n\n        label = label_map[label]\n\n\n\n        if self.transform:\n\n            image = self.transform(image)\n\n\n\n        return image, label\n","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:21.609762Z","iopub.status.busy":"2024-11-13T03:37:21.60933Z","iopub.status.idle":"2024-11-13T03:37:21.619788Z","shell.execute_reply":"2024-11-13T03:37:21.618742Z","shell.execute_reply.started":"2024-11-13T03:37:21.60973Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code defines a custom dataset class named HAM10000Dataset that inherits from PyTorch's Dataset\n**Initialization (init method)**: The constructor takes three parameters: csv_file, img_dirs, and an optional transform. The csv_file is expected to be a CSV file containing metadata about the images, such as their filenames and labels. The img_dirs is a list of directories where the images are stored. The transform parameter allows for optional image transformations (e.g., resizing, normalization) to be applied to the images. The constructor reads the CSV file into a pandas DataFrame and stores the image directories and transform.\n**Length (len method)**: This method returns the number of samples in the dataset by returning the length of the DataFrame. This is a required method for PyTorch datasets, enabling functions like len(dataset) to work correctly.\n**Get Item (get item method)**: This method retrieves a single sample from the dataset. It takes an index idx as input and performs the following steps: \n    Looks up the image name in the DataFrame using the provided index and appends the .jpg extension.\n    Searches for the image file in the specified directories. If the image is found, it is opened and converted to RGB format. If the image is not found in any directory, a FileNotFoundError is raised.\n    Retrieves the label for the image from the DataFrame. The label is mapped to an integer using a dictionary that maps unique labels to indices.\n    If a transform is provided, it is applied to the image.\n    Returns a tuple containing the image and its corresponding label.\n\nThis custom dataset class is essential for loading and preprocessing the HAM10000 dataset, making it ready for training and evaluating machine learning models. It handles the complexities of locating images across multiple directories, reading image files, and applying necessary transformations.\n","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"metadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n\nmetadata = pd.read_csv(metadata_path)\n\n\n\n# Check the number of unique images in metadata\n\nprint(f\"Total images in metadata: {len(metadata)}\")","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:21.621193Z","iopub.status.busy":"2024-11-13T03:37:21.620913Z","iopub.status.idle":"2024-11-13T03:37:21.675607Z","shell.execute_reply":"2024-11-13T03:37:21.674708Z","shell.execute_reply.started":"2024-11-13T03:37:21.621164Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet is responsible for loading and inspecting metadata related to the HAM10000 dataset, which is used for skin lesion analysis.\n\n\n1. **Setting the Metadata Path**: The variable metadata_path is assigned the file path to the CSV file containing the metadata for the HAM10000 dataset. This path points to a file named `HAM10000_metadata.csv` located in the directory `../input/skin-cancer-mnist-ham10000/`.\n\n2. **Loading the Metadata**: The pd.read_csv(metadata_path) function call reads the CSV file into a pandas DataFrame named metadata. This DataFrame will contain various details about the images, such as their filenames, labels, and possibly other relevant information.\n\n3. **Checking the Number of Unique Images**: The print statement outputs the total number of images listed in the metadata. The len(metadata) function call returns the number of rows in the DataFrame, which corresponds to the number of unique images described in the metadata file.\n\n\nThis code is crucial for verifying that the metadata has been loaded correctly and for understanding the scope of the dataset by checking the total number of images available for analysis.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Split metadata into train and test sets\n\ntrain_metadata, test_metadata = train_test_split(metadata, test_size=0.99, random_state=42)\n\n\n\n# Save split metadata for easier loading\n\ntrain_metadata.to_csv(\"train_metadata.csv\", index=False)\n\ntest_metadata.to_csv(\"test_metadata.csv\", index=False)","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:21.679097Z","iopub.status.busy":"2024-11-13T03:37:21.67821Z","iopub.status.idle":"2024-11-13T03:37:21.747117Z","shell.execute_reply":"2024-11-13T03:37:21.746374Z","shell.execute_reply.started":"2024-11-13T03:37:21.679046Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet is responsible for splitting the metadata of the HAM10000 dataset into training and testing sets and then saving these splits to CSV files for easier future access.\n\n\n1. **Splitting the Metadata**: The train_test_split function from scikit-learn is used to split the metadata DataFrame into two separate DataFrames: train_metadata and test_metadata. The test_size=0.99 parameter specifies that 99% of the data should be allocated to the test set, leaving only 1% for the training set. The random_state=42 parameter ensures that the split is reproducible by setting a seed for the random number generator.\n\n2. **Saving the Split Metadata**: The to_csv method is called on both train_metadata and test_metadata DataFrames to save them as CSV files named train_metadata.csv and test_metadata.csv, respectively. The index=False parameter ensures that the row indices are not included in the saved CSV files.\n\nThis code is essential for preparing the dataset for machine learning tasks. By splitting the metadata into training and testing sets, it allows for proper evaluation of the model's performance. Saving these splits to CSV files makes it convenient to load the pre-split data in future sessions, avoiding the need to perform the split operation repeatedly.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Directories containing images\n\nimage_dirs = [\n\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n\n]","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:21.748396Z","iopub.status.busy":"2024-11-13T03:37:21.748099Z","iopub.status.idle":"2024-11-13T03:37:21.752645Z","shell.execute_reply":"2024-11-13T03:37:21.751629Z","shell.execute_reply.started":"2024-11-13T03:37:21.748365Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines a list of directories that contain the images for the HAM10000 dataset, which is used for skin lesion analysis.\n\n1. **Defining Image Directories**: The variable main.ipynb ) is assigned a list of two directory paths. These directories are specified as strings and point to the locations where the image files are stored. The paths are:\n\n   - `../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1`\n\n   - `../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2`\n\n\n2. **Purpose of Image Directories**: These directories are likely part of the dataset's structure, where the images have been split into multiple parts for organizational purposes. By listing these directories, the code can later iterate through them to locate and load the images as needed.\n\nThis setup is crucial for managing and accessing the image files efficiently. By specifying the directories in a list, the code can easily handle the images regardless of their distribution across multiple folders. This approach simplifies the process of loading and preprocessing the images for further analysis or model training.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"transform = transforms.Compose([\n\n    transforms.Resize((64, 64)),  # Resize images to 256x256\n\n    transforms.ToTensor(),         # Convert to PyTorch tensor\n\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n\n])","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:21.754216Z","iopub.status.busy":"2024-11-13T03:37:21.753872Z","iopub.status.idle":"2024-11-13T03:37:21.763699Z","shell.execute_reply":"2024-11-13T03:37:21.762723Z","shell.execute_reply.started":"2024-11-13T03:37:21.754185Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines a sequence of image transformations using the transforms.Compose function from the `torchvision` library. These transformations are applied to the images in the HAM10000 dataset to prepare them for input into a machine learning model.\n\n1. **Resizing Images**: The transforms.Resize((64, 64)) transformation resizes the images to a fixed size of 64x64 pixels. This step ensures that all images have the same dimensions, which is necessary for batch processing in neural networks. The comment incorrectly mentions resizing to 256x256, but the actual code resizes to 64x64.\n\n2. **Converting to Tensor**: The transforms.ToTensor() transformation converts the images from PIL format (or numpy arrays) to PyTorch tensors. This conversion is essential because PyTorch models require input data in tensor format. Additionally, this transformation scales the pixel values from the range [0, 255] to [0, 1].\n\n\n\n3. **Normalizing**: The transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) transformation normalizes the pixel values of the images. Normalization adjusts the pixel values to have a mean of 0.5 and a standard deviation of 0.5 for each of the three color channels (red, green, and blue). This step helps in stabilizing and speeding up the training process by ensuring that the input data has a consistent distribution.\n\nBy composing these transformations, the code ensures that the images are uniformly resized, converted to a suitable format for PyTorch, and normalized. These preprocessing steps are crucial for preparing the dataset for training and evaluating machine learning models effectively.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Datasets\n\ntrain_dataset = HAM10000Dataset(csv_file=\"train_metadata.csv\", img_dirs=image_dirs, transform=transform)\n\ntest_dataset = HAM10000Dataset(csv_file=\"test_metadata.csv\", img_dirs=image_dirs, transform=transform)\n\n\n\n# DataLoaders\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n\n\n\n# Print dataset sizes\n\nprint(f\"Number of training samples: {len(train_dataset)}\")\n\nprint(f\"Number of testing samples: {len(test_dataset)}\")","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:22.156079Z","iopub.status.busy":"2024-11-13T03:37:22.155683Z","iopub.status.idle":"2024-11-13T03:37:22.189694Z","shell.execute_reply":"2024-11-13T03:37:22.188693Z","shell.execute_reply.started":"2024-11-13T03:37:22.156041Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet sets up the datasets and data loaders for training and testing a machine learning model using the HAM10000 dataset. It also prints the sizes of the training and testing datasets.\n\n1. **Creating Datasets**: train_dataset and test_dataset are instances of the custom  HAM10000Dataset class. \n   - The train_dataset is initialized with the metadata file `train_metadata.csv`, the list of image directories image_dirs, and the transformation pipeline transform.\n   - Similarly, the test_dataset is initialized with the metadata file `test_metadata.csv`, the same image directories, and the same transformation pipeline.\n   - These datasets will handle loading and preprocessing the images and their corresponding labels.\n\n2. **Creating DataLoaders**:\n   - train_loader and test_loader are instances of PyTorch's DataLoader class.\n   - train_loader is created with the train_dataset, a batch size of 4, shuffling enabled (shuffle=True), and 2 worker threads (num_workers=2) for parallel data loading. Shuffling ensures that the training data is presented in a different order each epoch, which helps in training the model more effectively.\n   - test_loader is created with the test_dataset, the same batch size of 4, shuffling disabled (shuffle=False), and 2 worker threads. Shuffling is typically disabled for the test set to ensure consistent evaluation.\n\n3. **Printing Dataset Sizes**:\n   - The print statements output the number of samples in the training and testing datasets by calling len() on train_dataset and  test_dataset.\n   - This provides a quick check to ensure that the datasets have been loaded correctly and to understand the amount of data available for training and testing.\n\nOverall, this code is essential for preparing the data pipeline, ensuring that the images and labels are correctly loaded, preprocessed, and batched for training and evaluation of the machine learning model.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Visualize one batch of images\n\nimages, labels = next(iter(train_loader))\n\nprint(f\"Image batch shape: {images.shape}\")\n\nprint(f\"Label batch shape: {labels.shape}\")\n\n\n\n# Display first 4 images\n\nimport matplotlib.pyplot as plt\n\n\n\nfig, axes = plt.subplots(1, 4, figsize=(12, 4))\n\nfor i in range(4):\n\n    axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Denormalize\n\n    axes[i].set_title(f\"Label: {labels[i].item()}\")\n\n    axes[i].axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:37:23.031142Z","iopub.status.busy":"2024-11-13T03:37:23.030695Z","iopub.status.idle":"2024-11-13T03:37:23.883755Z","shell.execute_reply":"2024-11-13T03:37:23.882658Z","shell.execute_reply.started":"2024-11-13T03:37:23.031103Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet is responsible for visualizing a batch of images from the training dataset. This helps in verifying that the images are being loaded and preprocessed correctly.\n\n1. **Loading a Batch of Images**:\n   - The line images, labels = next(iter(train_loader)) retrieves the next batch of images and their corresponding labels from the train_loader. The iter(train_loader) creates an iterator over the train_loader, and next() fetches the next batch.\n   - The print statements output the shapes of the image and label batches. This provides information about the dimensions of the data, ensuring that the batch size and image dimensions are as expected.\n\n2. **Importing Matplotlib**:\n   - The import matplotlib.pyplot as plt statement imports the matplotlib.pyplot module, which is used for creating visualizations.\n\n3. **Creating a Figure for Display**:\n   - The line fig, axes = plt.subplots(1, 4, figsize=(12, 4)) creates a figure with 4 subplots arranged in a single row. The figsize parameter sets the size of the figure to 12 inches by 4 inches.\n\n4. **Displaying the First 4 Images**:\n   - A `for` loop iterates over the first 4 images in the batch.\n   - Inside the loop, axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5) displays each image. The permute(1, 2, 0) method rearranges the dimensions of the image tensor from (C, H, W) to (H, W, C), which is required for displaying the image using imshow. The numpy() method converts the tensor to a NumPy array, and the multiplication and addition (`* 0.5 + 0.5`) denormalize the pixel values back to the range [0, 1].\n   - axes[i].set_title(f\"Label: {labels[i].item()}\") sets the title of each subplot to the corresponding label.\n   - axes[i].axis(\"off\") removes the axis ticks and labels for a cleaner display.\n\n5. **Showing the Figure**:\n   - The plt.show() statement renders the figure and displays the images.This visualization step is crucial for ensuring that the data loading and preprocessing pipeline is functioning correctly. By displaying a few images from the training set, you can visually inspect the images and their labels to confirm that they are being processed as expected.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\n\n\nclass SelfAttention(nn.Module):\n\n    def __init__(self, in_dim):\n\n        super(SelfAttention, self).__init__()\n\n        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n\n        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n\n        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n\n\n    def forward(self, x):\n\n        batch, channels, height, width = x.size()\n\n        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n\n        proj_key = self.key_conv(x).view(batch, -1, width * height)\n\n        attention = torch.bmm(proj_query, proj_key)\n\n        attention = torch.softmax(attention, dim=-1)\n\n\n\n        proj_value = self.value_conv(x).view(batch, -1, width * height)\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n\n        out = out.view(batch, channels, height, width)\n\n        out = self.gamma * out + x\n\n        return out\n\n\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, in_channels):\n\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n\n            nn.BatchNorm2d(in_channels),\n\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n\n            nn.BatchNorm2d(in_channels)\n\n        )\n\n\n\n    def forward(self, x):\n\n        return x + self.block(x)\n\n\n\nclass Generator(nn.Module):\n\n    def __init__(self, latent_dim, img_channels, img_size=64):\n\n        super(Generator, self).__init__()\n\n        \n\n        self.latent_dim = latent_dim\n\n        self.img_channels = img_channels\n\n        self.img_size = img_size\n\n        self.init_size = img_size // 8  # Downsample by 8 (adjusted for 64x64 output)\n\n        self.fc = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)\n\n\n\n        self.upsample = nn.Sequential(\n\n            nn.BatchNorm2d(128),\n\n            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n\n            nn.BatchNorm2d(128),\n\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(128),\n\n            SelfAttention(128),  # Self-Attention after first upscale\n\n            \n\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n\n            nn.BatchNorm2d(64),\n\n            nn.ReLU(inplace=True),\n\n            ResidualBlock(64),\n\n\n\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n\n            nn.BatchNorm2d(32),\n\n            nn.ReLU(inplace=True),\n\n            SelfAttention(32),  # Self-Attention in the middle layers\n\n        )\n\n\n\n        self.final_layer = nn.Sequential(\n\n            nn.Conv2d(32, img_channels, kernel_size=3, stride=1, padding=1),\n\n            nn.Tanh()  # Normalize output to [-1, 1]\n\n        )\n\n\n\n    def forward(self, z):\n\n        out = self.fc(z)\n\n        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n\n        out = self.upsample(out)\n\n        img = self.final_layer(out)\n\n        return img\n\n\n\n# Instantiate the generator\n\nlatent_dim = 100  # Size of latent vector\n\nimg_channels = 3  # RGB images\n\nimg_size = 64  # Output image size\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ngenerator = Generator(latent_dim, img_channels, img_size).to(device)\n\n\n\n# Test the generator\n\nz = torch.randn(4, latent_dim).to(device)  # Random latent vector (batch size = 4)\n\ngenerated_images = generator(z)\n\n\n\nprint(f\"Generated image shape: {generated_images.shape}\")  # Should be [4, 3, 64, 64]\n","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:40:01.297608Z","iopub.status.busy":"2024-11-13T03:40:01.296762Z","iopub.status.idle":"2024-11-13T03:40:02.565467Z","shell.execute_reply":"2024-11-13T03:40:02.564406Z","shell.execute_reply.started":"2024-11-13T03:40:01.297566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines and tests a deep learning model for generating images using PyTorch. It includes the implementation of a self-attention mechanism, a residual block, and a generator network.\n\n1. **Imports**:\n\n   - The code imports the necessary modules from PyTorch, including torc and torch.nn.\n\n2. **Self-Attention Class**:\n\n   - The SelfAttention class inherits from nn.Module and implements a self-attention mechanism.\n   - The __init__ method initializes convolutional layers for query, key, and value projections, and a learnable parameter gamma.\n   - The forward method computes the attention map and applies it to the input feature map, enhancing the model's ability to focus on relevant parts of the image.\n\n3. **Residual Block Class**:\n   - The ResidualBlock class inherits from nn.Module and implements a residual block.\n   - The __init__ method defines a sequence of convolutional, batch normalization, and ReLU activation layers.\n   - The forward method adds the input to the output of the block, facilitating gradient flow and improving training stability.\n\n4. **Generator Class**:\n   - The Generator class inherits from nn.Module and defines the architecture of the generator network.\n   - The __init__ method initializes the network with a fully connected layer, several upsampling layers, residual blocks, and self-attention layers.\n   - The forward method processes the input latent vector through the network to generate an image.\n\n5. **Instantiating and Testing the Generator**:\n   - The generator is instantiated with a latent dimension of 100, 3 image channels (for RGB images), and an output image size of 64x64 pixels.\n   - The generator is moved to the appropriate device (GPU if available, otherwise CPU).\n   - A random latent vector z is generated and passed through the generator to produce a batch of images.\n   - The shape of the generated images is printed to verify the output dimensions.\n\nThis code demonstrates the implementation of a generative model with advanced components like self-attention and residual blocks, which are designed to improve the quality and stability of the generated images.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\n\n\nclass SelfAttention(nn.Module):\n\n    def __init__(self, in_dim):\n\n        super(SelfAttention, self).__init__()\n\n        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n\n        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n\n        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n\n\n    def forward(self, x):\n\n        batch, channels, height, width = x.size()\n\n        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n\n        proj_key = self.key_conv(x).view(batch, -1, width * height)\n\n        attention = torch.bmm(proj_query, proj_key)\n\n        attention = torch.softmax(attention, dim=-1)\n\n\n\n        proj_value = self.value_conv(x).view(batch, -1, width * height)\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n\n        out = out.view(batch, channels, height, width)\n\n        out = self.gamma * out + x\n\n        return out\n\n\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, downsample=True):\n\n        super(ResidualBlock, self).__init__()\n\n        self.downsample = downsample\n\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if downsample else nn.Identity()\n\n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.pool = nn.AvgPool2d(2) if downsample else nn.Identity()\n\n\n\n    def forward(self, x):\n\n        shortcut = self.shortcut(x)\n\n        out = self.conv1(x)\n\n        out = self.bn1(out)\n\n        out = self.relu(out)\n\n        out = self.conv2(out)\n\n        out = self.bn2(out)\n\n        out += shortcut\n\n        out = self.relu(out)\n\n        out = self.pool(out)\n\n        return out\n\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, img_channels, img_size=64):\n\n        super(Discriminator, self).__init__()\n\n\n\n        self.model = nn.Sequential(\n\n            ResidualBlock(img_channels, 64, downsample=True),            # 64x64 -> 32x32\n\n            SelfAttention(64),\n\n            ResidualBlock(64, 128, downsample=True),           # 32x32 -> 16x16\n\n            SelfAttention(128),\n\n            ResidualBlock(128, 256, downsample=True),          # 16x16 -> 8x8\n\n            ResidualBlock(256, 512, downsample=True)           # 8x8 -> 4x4\n\n        )\n\n\n\n        self.final_layer = nn.Sequential(\n\n            nn.Flatten(),\n\n            nn.Linear(512 * 4 * 4, 1),  # Final score output\n\n            nn.Sigmoid()  # Outputs probability of being real or fake\n\n        )\n\n\n\n    def forward(self, img):\n\n        out = self.model(img)\n\n        out = self.final_layer(out)\n\n        return out\n\n\n\n# Instantiate the discriminator\n\nimg_channels = 3  # RGB images\n\nimg_size = 64  # Input image size\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndiscriminator = Discriminator(img_channels, img_size).to(device)\n\n\n\n# Test the discriminator\n\nbatch_size = 4\n\ntest_images = torch.randn(batch_size, img_channels, img_size, img_size).to(device)  # Fake images batch\n\noutput = discriminator(test_images)\n\n\n\nprint(f\"Discriminator output shape: {output.shape}\")  # Should be [4, 1]\n","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:41:32.233766Z","iopub.status.busy":"2024-11-13T03:41:32.233316Z","iopub.status.idle":"2024-11-13T03:41:32.46463Z","shell.execute_reply":"2024-11-13T03:41:32.463541Z","shell.execute_reply.started":"2024-11-13T03:41:32.233725Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines and tests a discriminator model for a Generative Adversarial Network (GAN) using PyTorch. The discriminator is designed to distinguish between real and fake images.\n\n1. **Imports**:\n   - The code imports the necessary modules from PyTorch, including  torch and torch.nn.\n\n2. **Self-Attention Class**:\n   - The SelfAttention class inherits from nn.Module and implements a self-attention mechanism.\n   - The __init__ method initializes convolutional layers for query, key, and value projections, and a learnable parameter gamma.\n   - The forward method computes the attention map and applies it to the input feature map, enhancing the model's ability to focus on relevant parts of the image.\n\n3. **Residual Block Class**:\n   - The ResidualBlock class inherits from nn.Module and implements a residual block.\n   - The __init__ method defines a sequence of convolutional, batch normalization, and ReLU activation layers, along with a shortcut connection and optional downsampling.\n   - The forward method processes the input through the convolutional layers, adds the shortcut connection, and applies downsampling if specified.\n\n4. **Discriminator Class**:\n   - The Discriminator class inherits from nn.Module and defines the architecture of the discriminator network.\n   - The __init__ method initializes the network with a sequence of residual blocks and self-attention layers, followed by a final layer that flattens the output and applies a linear transformation and sigmoid activation to produce a probability score.\n   - The forward method processes the input image through the network to produce the final output.\n\n5. **Instantiating and Testing the Discriminator**:\n   - The discriminator is instantiated with 3 image channels (for RGB images) and an input image size of 64x64 pixels.\n   - The discriminator is moved to the appropriate device (GPU if available, otherwise CPU).\n   - A batch of random fake images is generated and passed through the discriminator to produce an output.\n   - The shape of the discriminator's output is printed to verify the dimensions, which should be `[4, 1]` for a batch size of 4.\n\nThis code demonstrates the implementation of a discriminator model with advanced components like self-attention and residual blocks, which are designed to improve the model's ability to distinguish between real and fake images.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:46:53.871919Z","iopub.status.busy":"2024-11-13T03:46:53.871058Z","iopub.status.idle":"2024-11-13T03:46:53.876521Z","shell.execute_reply":"2024-11-13T03:46:53.87544Z","shell.execute_reply.started":"2024-11-13T03:46:53.871864Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet is responsible for managing warning messages in Python.\n\n1. **Importing the Warnings Module**:\n   - The line import warnings\n imports the warnings module, which is a built-in Python module used to handle warning messages. Warnings are typically issued to alert the user about potential issues in the code that do not necessarily stop the execution but might lead to unexpected behavior.\n\n2. **Filtering Warnings**:\n   - The line warnings.filterwarnings(\"ignore\") sets a filter to ignore all warning messages. This means that any warnings that would normally be printed to the console will be suppressed and not displayed.\n   - This can be useful in scenarios where the user is aware of certain non-critical warnings and wants to avoid cluttering the output with these messages. However, it is important to use this with caution, as ignoring warnings might cause the user to miss important information about potential issues in the code.\n\nBy using this code, the user ensures that the output remains clean and free of warning messages, which can be particularly useful in a production environment or when running long scripts where warnings are expected and understood. However, it is generally a good practice to address the root causes of warnings rather than ignoring them.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"from torch import optim\n\nfrom tqdm import tqdm\n\n\n\n#def train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1000, lr=0.0002, beta1=0.5, beta2=0.999):\n\ndef train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1, lr=0.0002, beta1=0.5, beta2=0.999):\n\n    generator.to(device)\n\n    discriminator.to(device)\n\n\n\n    criterion = nn.BCEWithLogitsLoss()\n\n    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n\n    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n\n    \n\n    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n\n\n\n    for epoch in range(epochs):\n\n        generator.train()\n\n        discriminator.train()\n\n        epoch_loss_G = 0.0\n\n        epoch_loss_D = 0.0\n\n\n\n        for real_images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n\n            batch_size = real_images.size(0)\n\n            real_images = real_images.to(device)\n\n\n\n            valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n\n            fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n\n\n\n            # Train Generator\n\n            optimizer_G.zero_grad()\n\n            z = torch.randn(batch_size, latent_dim).to(device)\n\n            \n\n            with torch.cuda.amp.autocast():  # Mixed precision training\n\n                generated_images = generator(z)\n\n                g_loss = criterion(discriminator(generated_images), valid)\n\n\n\n            scaler.scale(g_loss).backward()\n\n            scaler.step(optimizer_G)\n\n            scaler.update()\n\n            epoch_loss_G += g_loss.item()\n\n\n\n            # Train Discriminator\n\n            optimizer_D.zero_grad()\n\n            with torch.cuda.amp.autocast():\n\n                real_loss = criterion(discriminator(real_images), valid)\n\n                fake_loss = criterion(discriminator(generated_images.detach()), fake)\n\n                d_loss = (real_loss + fake_loss) / 2\n\n\n\n            scaler.scale(d_loss).backward()\n\n            scaler.step(optimizer_D)\n\n            scaler.update()\n\n            epoch_loss_D += d_loss.item()\n\n\n\n            # Clear cache to reduce memory fragmentation\n\n            torch.cuda.empty_cache()\n\n\n\n        print(f\"Epoch [{epoch+1}/{epochs}] | Generator Loss: {epoch_loss_G:.4f} | Discriminator Loss: {epoch_loss_D:.4f}\")\n\n\n\n    print(\"Training completed.\")\n\n\n\n\n\n# Call the train_gan function with the train_loader, generator, and discriminator\n\ntrain_gan(generator, discriminator, train_loader, latent_dim, device)\n","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:47:38.582788Z","iopub.status.busy":"2024-11-13T03:47:38.58237Z","iopub.status.idle":"2024-11-13T03:47:38.671075Z","shell.execute_reply":"2024-11-13T03:47:38.67003Z","shell.execute_reply.started":"2024-11-13T03:47:38.582751Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines a function train_gan that trains a Generative Adversarial Network (GAN) using PyTorch. The function trains both the generator and discriminator models over a specified number of epochs.\n\n1. **Imports**:\n   - The code imports the optim module from PyTorch for optimization algorithms and tqdm for displaying progress bars during training.\n\n2. **Function Definition**:\n   - The train_gan function takes several parameters: generator, discriminator, train_loader, latent_dim, device, epochs, lr, beta1, and beta2.\n   - The default number of epochs is set to 1, but it can be adjusted as needed.\n\n3. **Model Preparation**:\n   - The generator and discriminator models are moved to the specified device (GPU or CPU) using generator.to(device) and discriminator.to(device).\n   - The loss function used is nn.BCEWithLogitsLoss(), which combines a sigmoid layer and binary cross-entropy loss.\n   - Two Adam optimizers are created for the generator (optimizer_G) and discriminator (optimizer_D) with the specified learning rate (lr) and beta values (beta1, beta2).\n\n4. **Mixed Precision Training**:\n   - A gradient scaler (scaler) is initialized for mixed precision training, which can improve performance and reduce memory usage on compatible hardware.\n\n5. **Training Loop**:\n   - The outer loop iterates over the number of epochs.\n   - Within each epoch, the generator and discriminator models are set to training mode using generator.train() and discriminator.train().\n   - Two variables, epoch_loss_G and epoch_loss_D, are initialized to accumulate the generator and discriminator losses for the epoch.\n\n6. **Batch Processing**:\n   - The inner loop iterates over batches of real images from the train_loader, displaying progress with tqdm.\n   - The batch size is determined from the real images, and the images are moved to the specified device.\n   - Two tensors, valid and fake, are created to represent the labels for real and fake images, respectively.\n\n7. **Training the Generator**:\n   - The generator's gradients are zeroed using optimizer_G.zero_grad().\n   - A batch of random latent vectors (z) is generated and moved to the device.\n   - Mixed precision training is used to generate images and compute the generator loss (g_loss).\n   - The loss is scaled, backpropagated, and the optimizer is stepped using the gradient scaler.\n\n8. **Training the Discriminator**:\n   - The discriminator's gradients are zeroed using optimizer_D.zero_grad().\n   - Mixed precision training is used to compute the discriminator loss (d_loss) from both real and fake images.\n   - The loss is scaled, backpropagated, and the optimizer is stepped using the gradient scaler.\n\n9. **Memory Management**:\n   - The CUDA cache is cleared using torch.cuda.empty_cache() to reduce memory fragmentation.\n\n10. **Logging**:\n    - The generator and discriminator losses for the epoch are printed.\n      \n11. **Function Call**:\n    - The train_gan function is called with the train_loader, generator, and discriminator to start the training process.\n\nThis function provides a comprehensive framework for training a GAN, including mixed precision training, progress tracking, and memory management.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nfrom torchvision import models\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n# Define EfficientNetV2 model for HAM10000 with 7 classes\n\nclass EfficientNetV2Classifier(nn.Module):\n\n    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n\n        super(EfficientNetV2Classifier, self).__init__()\n\n        self.efficientnet_v2 = models.efficientnet_v2_s(pretrained=True)\n\n        \n\n        in_features = self.efficientnet_v2.classifier[1].in_features\n\n        self.efficientnet_v2.classifier = nn.Sequential(\n\n            nn.Dropout(p=0.3),\n\n            nn.Linear(in_features, num_classes)\n\n        )\n\n\n\n    def forward(self, x):\n\n        return self.efficientnet_v2(x)\n\n\n\n# Initialize the model\n\nmodel_EfficientNetV2 = EfficientNetV2Classifier(num_classes=7)\n\n\n\n# Loss and optimizer\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(model_EfficientNetV2.parameters(), lr=0.001)\n\nepochs = 1#20\n\n\n\n# Training loop\n\ndef train_model(model, train_loader, test_loader, criterion, optimizer, epochs=1):#20):\n\n    model.train()\n\n    for epoch in range(epochs):\n\n        running_loss = 0.0\n\n        total_correct = 0\n\n        \n\n        for data, labels in train_loader:\n\n            data, labels = data.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(data)\n\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n\n            optimizer.step()\n\n            \n\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n\n            total_correct += (predicted == labels).sum().item()\n\n            print(\"done\")\n\n        \n\n        epoch_loss = running_loss / len(train_loader)\n\n        epoch_accuracy = total_correct / len(train_loader.dataset)\n\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n        \n\n        # Validation after each epoch\n\n        validate_model(model, test_loader)\n\n\n\n# Validation loop\n\ndef validate_model(model, test_loader):\n\n    model.eval()\n\n    total_correct = 0\n\n    total_loss = 0.0\n\n    \n\n    with torch.no_grad():\n\n        for data, labels in test_loader:\n\n            data, labels = data.to(device), labels.to(device)\n\n            outputs = model(data)\n\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n\n            \n\n            _, predicted = torch.max(outputs.data, 1)\n\n            total_correct += (predicted == labels).sum().item()\n\n    \n\n    avg_loss = total_loss / len(test_loader)\n\n    accuracy = total_correct / len(test_loader.dataset)\n\n    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n\n\n\nmodel_EfficientNetV2.to(device)\n\n\n\n# Train the model\n\ntrain_model(model_EfficientNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)","metadata":{"execution":{"iopub.execute_input":"2024-11-13T03:55:49.315186Z","iopub.status.busy":"2024-11-13T03:55:49.314203Z","iopub.status.idle":"2024-11-13T03:55:49.3272Z","shell.execute_reply":"2024-11-13T03:55:49.326207Z","shell.execute_reply.started":"2024-11-13T03:55:49.315144Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines and trains a deep learning model using the EfficientNetV2 architecture for the HAM10000 dataset, which consists of 7 classes of skin lesions.\n\n1. **Imports and Warnings**:\n   - The code imports necessary modules from PyTorch (torch, torch.nn, torch.optim) and\nmodels from torchvision.\n   - Warnings are suppressed using warnings.filterwarnings(\"ignore\") to keep the output clean.\n\n2. **EfficientNetV2 Classifier**:\n   - The EfficientNetV2Classifier class inherits from nn.Module and defines a custom classifier based on the EfficientNetV2 architecture.\n   - In the __init__ method, the EfficientNetV2 model is loaded with pretrained weights using models.efficientnet_v2_s(pretrained=True).\n   - The classifier layer of the model is replaced with a new sequential layer consisting of a dropout layer and a linear layer to output predictions for 7 classes.\n   - The forward method defines the forward pass of the model, which simply calls the forward method of the EfficientNetV2 model.\n\n3. **Model Initialization**:\n   - An instance of the EfficientNetV2Classifier is created with 7 output classes and assigned to model_EfficientNetV2.\n\n4. **Loss and Optimizer**:\n   - The loss function used is nn.CrossEntropyLoss(), which is suitable for multi-class classification problems.\n   - The optimizer used is Adam (optim.Adam), with a learning rate of 0.001, to update the model parameters.\n\n5. **Training Loop**:\n   - The train_model function is defined to train the model. It takes the model, training and testing data loaders, loss function, optimizer, and number of epochs as input.\n   - The model is set to training mode using model.train().\n   - For each epoch, the running loss and total correct predictions are initialized.\n   - The inner loop iterates over batches of data from the training loader. For each batch:\n     - Data and labels are moved to the specified device.\n     - The optimizer gradients are zeroed.\n     - The model outputs are computed, and the loss is calculated.\n     - The loss is backpropagated, and the optimizer steps are performed.\n     - The running loss and total correct predictions are updated.\n   - After each epoch, the average loss and accuracy are printed.\n   - The validate_model function is called to evaluate the model on the test set.\n\n6. **Validation Loop**:\n   - The validate_model function is defined to evaluate the model on the test set.\n   - The model is set to evaluation mode using model.eval().\n   - The total correct predictions and total loss are initialized.\n   - The loop iterates over batches of data from the test loader. For each batch:\n     - Data and labels are moved to the specified device.\n     - The model outputs are computed, and the loss is calculated.\n     - The total loss and total correct predictions are updated.\n   - The average loss and accuracy are printed.\n     \n7. **Model Training**:\n   - The model is moved to the specified device using model_EfficientNetV2.to(device).\n   - The train_model function is called with the model, training and testing data loaders, loss function, optimizer, and number of epochs to start the training process.\n\n\nThis code sets up and trains an EfficientNetV2-based classifier for the HAM10000 dataset, providing a comprehensive framework for training and evaluating the model.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Define ShuffleNetV2 model for HAM10000 with 7 classes\n\nclass ShuffleNetV2Classifier(nn.Module):\n\n    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n\n        super(ShuffleNetV2Classifier, self).__init__()\n\n        self.shufflenet_v2 = models.shufflenet_v2_x1_0(pretrained=True)\n\n        \n\n        # Modify the last fully connected layer to match the number of classes\n\n        in_features = self.shufflenet_v2.fc.in_features\n\n        self.shufflenet_v2.fc = nn.Sequential(\n\n            nn.Dropout(p=0.3),\n\n            nn.Linear(in_features, num_classes)\n\n        )\n\n\n\n    def forward(self, x):\n\n        return self.shufflenet_v2(x)\n\n\n\n# Initialize the model\n\nmodel_ShuffleNetV2 = ShuffleNetV2Classifier(num_classes=7)\n\n\n\n# Loss and optimizer\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(model_ShuffleNetV2.parameters(), lr=0.001)\n\nepochs = 1#20\n\n\n\n# Training loop\n\ndef train_model(model, train_loader, test_loader, criterion, optimizer, epochs=1):#20):\n\n    model.train()\n\n    for epoch in range(epochs):\n\n        running_loss = 0.0\n\n        total_correct = 0\n\n        \n\n        for data, labels in train_loader:\n\n            data, labels = data.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(data)\n\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n\n            optimizer.step()\n\n            \n\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n\n            total_correct += (predicted == labels).sum().item()\n\n        \n\n        epoch_loss = running_loss / len(train_loader)\n\n        epoch_accuracy = total_correct / len(train_loader.dataset)\n\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n\n        \n\n        # Validation after each epoch\n\n        validate_model(model, test_loader)\n\n\n\n# Validation loop\n\ndef validate_model(model, test_loader):\n\n    model.eval()\n\n    total_correct = 0\n\n    total_loss = 0.0\n\n    \n\n    with torch.no_grad():\n\n        for data, labels in test_loader:\n\n            data, labels = data.to(device), labels.to(device)\n\n            outputs = model(data)\n\n            loss = criterion(outputs, labels)\n\n            total_loss += loss.item()\n\n            \n\n            _, predicted = torch.max(outputs.data, 1)\n\n            total_correct += (predicted == labels).sum().item()\n\n    \n\n    avg_loss = total_loss / len(test_loader)\n\n    accuracy = total_correct / len(test_loader.dataset)\n\n    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n\n\n\nmodel_ShuffleNetV2.to(device)\n\n\n\n# Train the model\n\ntrain_model(model_ShuffleNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)","metadata":{"execution":{"iopub.execute_input":"2024-11-13T04:00:06.453375Z","iopub.status.busy":"2024-11-13T04:00:06.452961Z","iopub.status.idle":"2024-11-13T04:00:06.464457Z","shell.execute_reply":"2024-11-13T04:00:06.463412Z","shell.execute_reply.started":"2024-11-13T04:00:06.453324Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines and trains a deep learning model using the ShuffleNetV2 architecture for the HAM10000 dataset, which consists of 7 classes of skin lesions.\n\n1. **ShuffleNetV2 Classifier**:\n   - The ShuffleNetV2Classifier class inherits from nn.Module and defines a custom classifier based on the ShuffleNetV2 architecture.\n   - In the __init__ method, the ShuffleNetV2 model is loaded with pretrained weights using models.shufflenet_v2_x1_0(pretrained=True).\n   - The classifier layer of the model is modified to match the number of classes (7) by replacing the last fully connected layer with a new sequential layer consisting of a dropout layer and a linear layer.\n   - The forward method defines the forward pass of the model, which simply calls the forward method of the ShuffleNetV2 model.\n\n2. **Model Initialization**:\n   - An instance of the ShuffleNetV2Classifier is created with 7 output classes and assigned to model_ShuffleNetV2.\n\n3. **Loss and Optimizer**:\n   - The loss function used is nn.CrossEntropyLoss(), which is suitable for multi-class classification problems.\n   - The optimizer used is Adam (optim.Adam), with a learning rate of 0.001, to update the model parameters.\n\n4. **Training Loop** :\n   - The train_model function is defined to train the model. It takes the model, training and testing data loaders, loss function, optimizer, and number of epochs as input.\n   - The model is set to training mode using model.train().\n   - For each epoch, the running loss and total correct predictions are initialized.\n   - The inner loop iterates over batches of data from the training loader. For each batch:\n     - Data and labels are moved to the specified device.\n     - The optimizer gradients are zeroed.\n     - The model outputs are computed, and the loss is calculated.\n     - The loss is backpropagated, and the optimizer steps are performed.\n     - The running loss and total correct predictions are updated.\n   - After each epoch, the average loss and accuracy are printed.\n   - The validate_model function is called to evaluate the model on the test set.\n\n5. **Validation Loop**:\n   - The validate_model function is defined to evaluate the model on the test set.\n   - The model is set to evaluation mode using model.eval().\n   - The total correct predictions and total loss are initialized.\n   - The loop iterates over batches of data from the test loader. For each batch:\n     - Data and labels are moved to the specified device.\n     - The model outputs are computed, and the loss is calculated.\n     - The total loss and total correct predictions are updated.\n   - The average loss and accuracy are printed.\n\n6. **Model Training**:\n   - The model is moved to the specified device using model_ShuffleNetV2.to(device).\n   - The train_model function is called with the model, training and testing data loaders, loss function, optimizer, and number of epochs to start the training process.\n\nThis code sets up and trains a ShuffleNetV2-based classifier for the HAM10000 dataset, providing a comprehensive framework for training and evaluating the model.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"def create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, labels, noise_dim=128):\n\n    noise = torch.randn(batch_size, noise_dim)  # Random noise for generator\n\n    created_imgs = generator(noise, labels) \n\n    EfficientNetV2Classifier_labels = model_EfficientNetV2(created_imgs)\n\n    ShuffleNetV2Classifier_labels = model_ShuffleNetV2(created_imgs)\n\n    if EfficientNetV2Classifier_labels == labels and ShuffleNetV2Classifier_labels == labels:\n\n        return created_imgs\n\n    else:\n\n        return None","metadata":{"execution":{"iopub.execute_input":"2024-11-13T04:00:30.917234Z","iopub.status.busy":"2024-11-13T04:00:30.916705Z","iopub.status.idle":"2024-11-13T04:00:30.924325Z","shell.execute_reply":"2024-11-13T04:00:30.923203Z","shell.execute_reply.started":"2024-11-13T04:00:30.917194Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines a function create_support_set that generates a set of images using a generator model and then validates these images using two classifier models, EfficientNetV2 and ShuffleNetV2.\n\n1. **Function Definition**:\n   - The function create_support_set takes five parameters: generator, model_EfficientNetV2, model_ShuffleNetV2, labels, and an optional noise_dim with a default value of 128.\n   - The purpose of this function is to create a support set of images that are validated by both classifier models.\n\n2. **Generating Noise**:\n   - The line noise = torch.randn(batch_size, noise_dim) generates a batch of random noise vectors. The noise_dim parameter specifies the dimensionality of each noise vector, and batch_size is assumed to be defined elsewhere in the code.\n   - This random noise serves as input to the generator model to produce synthetic images.\n\n3. **Generating Images**:\n   - The line created_imgs = generator(noise, labels) uses the generator model to create images from the random noise and the provided labels. The generator is expected to take both noise and labels as input to produce labeled images.\n\n4. **Classifying Generated Images**:\n   - The generated images are then passed through two classifier models:  model_EfficientNetV2 and model_ShuffleNetV2.\n   - The lines\nEfficientNetV2Classifier_labels = model_EfficientNetV2(created_imgs) and ShuffleNetV2Classifier_labels = model_ShuffleNetV2(created_imgs) obtain the predicted labels for the generated images from both classifiers.\n\n5. **Validation**:\n   - The function checks if the predicted labels from both classifiers match the provided labels using the condition if EfficientNetV2Classifier_labels == labels and ShuffleNetV2Classifier_labels == labels.\n   - If both classifiers correctly identify the generated images, the function returns the created images (return created_imgs).\n   - If either classifier fails to correctly identify the images, the function returns `None`.\n\nThis function is useful for generating and validating synthetic images, ensuring that the generated images are realistic and correctly labeled according to both classifier models. This can be particularly valuable in scenarios where high-quality labeled data is needed for training or evaluation purposes.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\n\n\nclass CNNEncoder(nn.Module):\n\n    def __init__(self, in_channels=3, base_features=64):\n\n        super(CNNEncoder, self).__init__()\n\n        \n\n        # Encoder block 1\n\n        self.block1 = nn.Sequential(\n\n            nn.Conv2d(in_channels, base_features, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features),\n\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features),\n\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d(2, 2)  # Reduces 64x64 -> 32x32\n\n        )\n\n        \n\n        # Encoder block 2\n\n        self.block2 = nn.Sequential(\n\n            nn.Conv2d(base_features, base_features * 2, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features * 2),\n\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(base_features * 2, base_features * 2, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features * 2),\n\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d(2, 2)  # Reduces 32x32 -> 16x16\n\n        )\n\n        \n\n        # Encoder block 3\n\n        self.block3 = nn.Sequential(\n\n            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features * 4),\n\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(base_features * 4, base_features * 4, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features * 4),\n\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d(2, 2)  # Reduces 16x16 -> 8x8\n\n        )\n\n        \n\n        # Encoder block 4\n\n        self.block4 = nn.Sequential(\n\n            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features * 8),\n\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(base_features * 8, base_features * 8, kernel_size=3, padding=1),\n\n            nn.BatchNorm2d(base_features * 8),\n\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d(2, 2)  # Reduces 8x8 -> 4x4\n\n        )\n\n\n\n    def forward(self, x):\n\n        # Apply each encoder block to the input\n\n        x = self.block1(x)  # 64x64 -> 32x32\n\n        x = self.block2(x)  # 32x32 -> 16x16\n\n        x = self.block3(x)  # 16x16 -> 8x8\n\n        x = self.block4(x)  # 8x8 -> 4x4\n\n        return x\n","metadata":{"execution":{"iopub.execute_input":"2024-11-13T04:10:51.370187Z","iopub.status.busy":"2024-11-13T04:10:51.369492Z","iopub.status.idle":"2024-11-13T04:10:51.385033Z","shell.execute_reply":"2024-11-13T04:10:51.383932Z","shell.execute_reply.started":"2024-11-13T04:10:51.370127Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines a convolutional neural network (CNN) encoder using PyTorch. This encoder is designed to process input images through a series of convolutional layers, batch normalization, activation functions, and pooling layers, progressively reducing the spatial dimensions while increasing the feature depth.\n\n1. **Imports and Class Definition**:\n   - The code imports the necessary modules from PyTorch, including torch and  torch.nn.\n   - The CNNEncoder class inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n\n2. **Initialization (__init__ method)**:\n   - The __init__ method initializes the encoder with two parameters: in_channels (default is 3 for RGB images) and base_features (default is 64).\n   - Four encoder blocks are defined within the __init__ method, each consisting of convolutional layers, batch normalization, ReLU activation, and max pooling.\n\n3. **Encoder Block 1**:\n   - self.block1 is a sequential container that includes:\n     - A convolutional layer with in_channels input channels and base_features output channels, a kernel size of 3, and padding of 1.\n     - Batch normalization for base_features channels.\n     - ReLU activation.\n     - Another convolutional layer with base_features\n\n input and output channels, a kernel size of 3, and padding of 1.\n     - Batch normalization and ReLU activation.\n     - Max pooling with a kernel size and stride of 2, reducing the spatial dimensions from 64x64 to 32x32.\n\n4. **Encoder Block 2**:\n   - self.block2 is similar to block 1 but with:\n     - Convolutional layers that double the number of features to base_features * 2.\n     - Max pooling reduces the spatial dimensions from 32x32 to 16x16.\n\n5. **Encoder Block 3**:\n   - self.block3 follows the same structure, further doubling the features to base_features * 4.\n     - Max pooling reduces the spatial dimensions from 16x16 to 8x8.\n\n6. **Encoder Block 4**:\n   - self.block4 continues the pattern, doubling the features to base_features * 8.\n     - Max pooling reduces the spatial dimensions from 8x8 to 4x4.\n\n7. **Forward Method**:\n   - The forward method defines the forward pass of the encoder.\n   - The input x is sequentially passed through each encoder block (block1, block2, block3, block4), progressively reducing its spatial dimensions and increasing its feature depth.\n   - The final output is returned after passing through all four blocks.\n\nThis CNN encoder is designed to extract hierarchical features from input images, making it suitable for tasks such as image classification, segmentation, or as a feature extractor in more complex models. The progressive reduction in spatial dimensions and increase in feature depth allows the network to capture both local and global patterns in the input data.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\n\n\nclass AttentionModule(nn.Module):\n\n    def __init__(self, feature_dim, num_heads=4):\n\n        super(AttentionModule, self).__init__()\n\n        \n\n        self.num_heads = num_heads\n\n        self.head_dim = feature_dim // num_heads\n\n        \n\n        # Linear transformations for multi-head attention\n\n        self.query_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n\n        self.key_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n\n        self.value_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n\n        \n\n        # Multi-head attention mechanism\n\n        self.attn_heads = nn.ModuleList(\n\n            [nn.Sequential(\n\n                nn.Conv2d(self.head_dim, self.head_dim, kernel_size=1),\n\n                nn.Softmax(dim=-1)  # Softmax across the spatial dimension\n\n            ) for _ in range(num_heads)]\n\n        )\n\n        \n\n        # Channel attention to recalibrate feature maps\n\n        self.channel_attention = nn.Sequential(\n\n            nn.AdaptiveAvgPool2d(1),\n\n            nn.Conv2d(feature_dim, feature_dim // 16, kernel_size=1),\n\n            nn.ReLU(),\n\n            nn.Conv2d(feature_dim // 16, feature_dim, kernel_size=1),\n\n            nn.Sigmoid()\n\n        )\n\n        \n\n        # Spatial attention to emphasize important regions in the spatial dimension\n\n        self.spatial_attention = nn.Sequential(\n\n            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n\n            nn.Sigmoid()\n\n        )\n\n        \n\n        # Final 1x1 conv to combine outputs\n\n        self.output_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n\n    \n\n    def forward(self, features):\n\n        # Compute query, key, and value maps for multi-head attention\n\n        queries = self.query_conv(features)  # [B, C, H, W]\n\n        keys = self.key_conv(features)       # [B, C, H, W]\n\n        values = self.value_conv(features)   # [B, C, H, W]\n\n        \n\n        B, C, H, W = queries.size()\n\n        queries = queries.view(B, self.num_heads, self.head_dim, H * W)  # [B, heads, head_dim, H*W]\n\n        keys = keys.view(B, self.num_heads, self.head_dim, H * W)        # [B, heads, head_dim, H*W]\n\n        values = values.view(B, self.num_heads, self.head_dim, H * W)    # [B, heads, head_dim, H*W]\n\n        \n\n        # Multi-head attention\n\n        attention_outputs = []\n\n        for i in range(self.num_heads):\n\n            attn_weights = torch.bmm(queries[:, i], keys[:, i].transpose(1, 2))  # [B, head_dim, head_dim]\n\n            attn_weights = self.attn_heads[i](attn_weights.view(B, self.head_dim, H, W))  # Apply learned attention map\n\n            attn_output = torch.bmm(attn_weights.view(B, self.head_dim, H * W), values[:, i])  # [B, head_dim, H*W]\n\n            attention_outputs.append(attn_output.view(B, self.head_dim, H, W))\n\n        \n\n        # Concatenate all attention head outputs\n\n        multi_head_output = torch.cat(attention_outputs, dim=1)  # [B, C, H, W]\n\n        \n\n        # Channel Attention\n\n        channel_attn_weights = self.channel_attention(multi_head_output)\n\n        channel_attn_output = multi_head_output * channel_attn_weights  # Element-wise multiplication (recalibration)\n\n        \n\n        # Spatial Attention\n\n        avg_pool = torch.mean(channel_attn_output, dim=1, keepdim=True)  # Average pooling across channels\n\n        max_pool = torch.max(channel_attn_output, dim=1, keepdim=True)[0]  # Max pooling across channels\n\n        spatial_attn_weights = self.spatial_attention(torch.cat([avg_pool, max_pool], dim=1))\n\n        spatial_attn_output = channel_attn_output * spatial_attn_weights  # Element-wise multiplication (spatial recalibration)\n\n        \n\n        # Final 1x1 conv to produce the final attention output\n\n        output = self.output_conv(spatial_attn_output)\n\n        return output\n","metadata":{"execution":{"iopub.execute_input":"2024-11-13T04:10:51.652953Z","iopub.status.busy":"2024-11-13T04:10:51.652535Z","iopub.status.idle":"2024-11-13T04:10:51.672149Z","shell.execute_reply":"2024-11-13T04:10:51.671149Z","shell.execute_reply.started":"2024-11-13T04:10:51.652911Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines an AttentionModule class in PyTorch, which implements a sophisticated attention mechanism combining multi-head attention, channel attention, and spatial attention. This module is designed to enhance feature representations by focusing on important parts of the input data.\n\n1. **Class Definition and Initialization**:\n   - The AttentionModule class inherits from nn.Module, the base class for all neural network modules in PyTorch.\n   - The __init__ method initializes the module with two parameters: feature_dim, which specifies the dimensionality of the input features, and num_heads, which defaults to 4 and specifies the number of attention heads.\n   - The head_dim is calculated by dividing feature_dim by num_heads, determining the dimensionality of each attention head.\n\n2. **Linear Transformations for Multi-Head Attention**:\n   - Three convolutional layers (query_conv, key_conv, and value_conv) are defined with a kernel size of 1. These layers transform the input features into query, key, and value maps, respectively, for the multi-head attention mechanism.\n\n\n3. **Multi-Head Attention Mechanism**:\n   - A ModuleList named attn_heads is created, containing num_heads sequential modules. Each module consists of a convolutional layer followed by a softmax activation function, which normalizes the attention weights across the spatial dimension.\n\n4. **Channel Attention**:\n   - The channel_attention sequential module recalibrates the feature maps by focusing on important channels. It includes:\n     - An adaptive average pooling layer that reduces the spatial dimensions to 1x1.\n     - Two convolutional layers with a ReLU activation in between.\n     - A sigmoid activation to produce the channel attention weights.\n\n5. **Spatial Attention**:\n   - The spatial_attention sequential module emphasizes important regions in the spatial dimension. It includes:\n     - A convolutional layer with a kernel size of 7 and padding of 3.\n     - A sigmoid activation to produce the spatial attention weights.\n\n6. **Final Convolutional Layer**:\n   - A final 1x1 convolutional layer (output_conv) is defined to combine the outputs of the attention mechanisms and produce the final attention-enhanced feature map.\n\n7. **Forward Method**:\n   - The forward method defines the forward pass of the module.\n   - Query, key, and value maps are computed using the respective convolutional layers.\n   - The input features are reshaped to facilitate multi-head attention, splitting the feature dimension into multiple heads.\n   - For each attention head, the attention weights are computed using batch matrix multiplication (torch.bmm), and the attention output is obtained by applying the learned attention map to the value map.\n   - The outputs of all attention heads are concatenated along the feature dimension.\n   - Channel attention is applied to recalibrate the feature maps, followed by spatial attention to emphasize important spatial regions.\n   - The final attention-enhanced feature map is produced using the 1x1 convolutional layer and returned as the output.\n\nThis AttentionModule class provides a comprehensive attention mechanism that can be integrated into larger neural network architectures to improve their ability to focus on relevant features and regions in the input data.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\n\n\nclass MTUNet2(nn.Module):\n\n    def __init__(self, in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4):\n\n        super(MTUNet2, self).__init__()\n\n        \n\n        # Complex CNN Encoder shared by both query and support\n\n        self.encoder = CNNEncoder(in_channels, base_features)\n\n        \n\n        # Complex Attention mechanism\n\n        self.attn_module = AttentionModule(feature_dim, num_heads=num_heads)\n\n        \n\n        # Classification Decoder\n\n        self.classifier = nn.Sequential(\n\n            nn.Flatten(),\n\n            nn.Linear(base_features*16*8*8, 1024),  # Updated linear layer input size for complex encoder\n\n            nn.ReLU(),\n\n            nn.Linear(1024, num_classes)\n\n        )\n\n    \n\n    def forward(self, query, support):\n\n        # Step 1: Extract features from the query image using the updated CNNEncoder\n\n        query_features = self.encoder(query)  # Query features are [B, 1024, 8, 8] based on complex CNNEncoder\n\n        \n\n        # Step 2: Extract and aggregate features from the support set\n\n        N = support.size(0)  # Number of support images\n\n        support_features = []\n\n        for i in range(N):\n\n            support_feature = self.encoder(support[i].unsqueeze(0))  # Each support image's features\n\n            support_features.append(support_feature)\n\n        \n\n        # Aggregate support features (using average pooling for simplicity)\n\n        support_features = torch.mean(torch.stack(support_features), dim=0)  # [B, 1024, 8, 8]\n\n        \n\n        # Step 3: Apply complex attention to both query and support features\n\n        query_attn = self.attn_module(query_features)  # Attention on query\n\n        support_attn = self.attn_module(support_features)  # Attention on support\n\n        \n\n        # Step 4: Combine query and support features via one-to-one concatenation\n\n        combined_features = torch.cat((query_attn, support_attn), dim=1)  # Concatenate along the channel dimension\n\n        # Combined features will be [B, 1024 + 1024 = 2048, 8, 8]\n\n        \n\n        # Step 5: Classification Decoder (use the combined query-support features)\n\n        classification_output = self.classifier(combined_features)\n\n        \n\n        return classification_output\n","metadata":{"execution":{"iopub.execute_input":"2024-11-13T04:10:52.047044Z","iopub.status.busy":"2024-11-13T04:10:52.046315Z","iopub.status.idle":"2024-11-13T04:10:52.058331Z","shell.execute_reply":"2024-11-13T04:10:52.057386Z","shell.execute_reply.started":"2024-11-13T04:10:52.047Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines a neural network model named MTUNet2 using PyTorch. This model is designed for tasks that involve both query and support images, leveraging a complex CNN encoder, an attention mechanism, and a classification decoder.\n\n1. **Class Definition and Initialization**:\n   - The MTUNet2 class inherits from nn.Module, the base class for all neural network modules in PyTorch.\n   - The __init__ method initializes the model with several parameters: in_channels (default is 3 for RGB images), base_features (default is 64), num_classes (default is 5), feature_dim (default is 512), and num_heads (default is 4).\n   - The model consists of three main components:\n     - A complex CNN encoder (self.encoder) shared by both query and support images, instantiated from the CNNEncoder class.\n     - An attention module (self.attn_module) instantiated from the \nAttentionModule class, which applies a complex attention mechanism to the features.\n     - A classification decoder (self.classifier), defined as a sequential module that flattens the input, applies a linear transformation followed by a ReLU activation, and then another linear transformation to produce the final class predictions.\n\n2. **Forward Method**:\n   - The forward method defines the forward pass of the model, taking two inputs: query and support.\n   - **Step 1**: Extract features from the query image using the CNN encoder. The output features have dimensions `[B, 1024, 8, 8]`, where `B` is the batch size.\n   - **Step 2**: Extract and aggregate features from the support set. The support set contains  N images. Each support image is passed through the CNN encoder, and the features are aggregated using average pooling to produce a single feature map with dimensions `[B, 1024, 8, 8]`.\n   - **Step 3**: Apply the attention module to both query and support features. The attention module enhances the features by focusing on important parts of the input data.\n   - **Step 4**: Combine the query and support features by concatenating them along the channel dimension, resulting in combined features with dimensions `[B, 2048, 8, 8]`.\n   - **Step 5**: Pass the combined features through the classification decoder to produce the final class predictions.\n\nThis model architecture is designed to handle tasks that require the integration of information from both query and support images, making it suitable for applications such as few-shot learning or meta-learning. The use of a complex CNN encoder, attention mechanism, and classification decoder allows the model to effectively extract, enhance, and classify features from the input data.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"import torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\n\n\n# Initialize the model, loss function, and optimizer\n\nmodel = MTUNet2(in_channels=3, base_features=64, num_classes=5)\n\ncriterion_cls = nn.CrossEntropyLoss()  # For classification output\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n\n# Training function\n\ndef train(model, train_loader, criterion_cls, optimizer, epoch):\n\n    model.train()\n\n    running_loss = 0.0\n\n    \n\n    for data, target in enumerate(train_loader):\n\n        \n\n        # Clear gradients\n\n        optimizer.zero_grad()\n\n\n\n        # Creating support set\n\n        support = create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, target, noise_dim=128)\n\n\n\n        # Forward pass\n\n        classification_output = model(data, support)  # Assuming same data for support set in FSL\n\n        \n\n        # Compute loss\n\n        loss_cls = criterion_cls(classification_output, target)  # Assuming target is for classification\n\n        \n\n        # Backward pass\n\n        loss_cls.backward()\n\n        optimizer.step()\n\n\n\n        # Accumulate the running loss\n\n        running_loss += loss_cls.item()\n\n\n\n        # Compute accuracy for classification output\n\n        _, predicted = torch.max(classification_output.data, 1)\n\n        total += target.size(0)\n\n        correct_cls += (predicted == target).sum().item()\n\n\n\n    accuracy = 100 * correct_cls / total\n\n    \n\n    return running_loss / len(train_loader), accuracy\n\n\n\n\n\n# Evaluation function\n\ndef evaluate(model, test_loader, criterion_cls):\n\n    model.eval()\n\n    test_loss = 0.0\n\n    correct_cls = 0\n\n    total = 0\n\n\n\n    with torch.no_grad():\n\n        for data, target in test_loader:\n\n\n\n            # Forward pass\n\n            classification_output = model(data)\n\n            \n\n            # Compute loss\n\n            loss_cls = criterion_cls(classification_output, target)\n\n            \n\n            test_loss += loss_cls.item()\n\n\n\n            # Compute accuracy for classification output\n\n            _, predicted = torch.max(classification_output.data, 1)\n\n            total += target.size(0)\n\n            correct_cls += (predicted == target).sum().item()\n\n\n\n    accuracy = 100 * correct_cls / total\n\n    avg_loss = test_loss / len(test_loader)\n\n    \n\n    return avg_loss, accuracy\n\n\n\n\n\n# Main training loop\n\nnum_epochs = 1#500\n\nfor epoch in range(1, num_epochs + 1):\n\n    train_loss, train_accuracy = train(model, train_loader, criterion_cls, optimizer, epoch)\n\n    print(f'Epoch [{epoch}], Training Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n\n\n\n    test_loss, test_accuracy = evaluate(model, test_loader, criterion_cls)\n\n    print(f'Epoch [{epoch}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n\n    print()","metadata":{"execution":{"iopub.execute_input":"2024-11-13T04:16:30.480666Z","iopub.status.busy":"2024-11-13T04:16:30.480196Z","iopub.status.idle":"2024-11-13T04:16:30.551491Z","shell.execute_reply":"2024-11-13T04:16:30.550424Z","shell.execute_reply.started":"2024-11-13T04:16:30.480627Z"},"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code snippet defines the setup and training loop for a neural network model named MTUNet2 using PyTorch. This includes initializing the model, defining the loss function and optimizer, and implementing the training and evaluation functions.\n\n1. **Initialization**:\n   - The model MTUNet2 is instantiated with 3 input channels, 64 base features, and 5 output classes.\n   - The loss function used is nn.CrossEntropyLoss(), which is suitable for multi-class classification tasks.\n   - The optimizer used is Adam (optim.Adam), with a learning rate of 0.001, to update the model parameters.\n\n2. **Training Function**:\n   - The train function is defined to train the model for one epoch. It takes the model, training data loader, loss function, optimizer, and the current epoch number as input.\n   - The model is set to training mode using model.train().\n   - A running loss variable is initialized to accumulate the loss over the epoch.\n   - The function iterates over batches of data from the training loader. For each batch:\n     - Gradients are cleared using optimizer.zero_grad().\n     - A support set is created using the create_support_set function, which generates images and validates them using two classifier models.\n     - A forward pass is performed by passing the data and support set through the model.\n     - The classification loss is computed using the loss function.\n     - The loss is backpropagated, and the optimizer steps are performed to update the model parameters.\n     - The running loss is accumulated, and the classification accuracy is computed.\n   - The function returns the average loss and accuracy for the epoch.\n\n3. **Evaluation Function**:\n   - The evaluate function is defined to evaluate the model on the test set. It takes the model, test data loader, and loss function as input.\n   - The model is set to evaluation mode using model.eval().\n   - Variables for test loss and correct predictions are initialized.\n   - The function iterates over batches of data from the test loader. For each batch:\n     - A forward pass is performed by passing the data through the model.\n     - The classification loss is computed and accumulated.\n     - The classification accuracy is computed.\n   - The function returns the average loss and accuracy for the test set.\n\n4. **Main Training Loop**:\n   - The main training loop runs for a specified number of epochs (num_epochs).\n   - For each epoch, the train function is called to train the model, and the training loss and accuracy are printed.\n   - The evaluate function is called to evaluate the model on the test set, and the test loss and accuracy are printed.\n\nThis code provides a comprehensive framework for training and evaluating the MTUNet2 model, including data loading, model training, loss computation, and accuracy evaluation. The use of a support set and attention mechanism in the model allows for advanced feature extraction and classification capabilities.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Part 1: Model Components\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# CNN Encoder class\nclass CNNEncoder(nn.Module):\n    def __init__(self, in_channels=3, base_features=64):\n        super(CNNEncoder, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(in_channels, base_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block2 = nn.Sequential(\n            nn.Conv2d(base_features, base_features * 2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features * 2, base_features * 2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block3 = nn.Sequential(\n            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 4),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features * 4, base_features * 4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.block4 = nn.Sequential(\n            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 8),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features * 8, base_features * 8, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 8),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        return x\n\n# Define the AttentionModule class\nclass AttentionModule(nn.Module):\n    def __init__(self, feature_dim, num_heads=4):\n        super(AttentionModule, self).__init__()\n        self.num_heads = num_heads\n        self.head_dim = feature_dim // num_heads\n        self.query_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n        self.key_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n        self.value_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(feature_dim, feature_dim // 16, kernel_size=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(feature_dim // 16, feature_dim, kernel_size=1),\n            nn.Sigmoid()\n        )\n\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(feature_dim, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n\n        self.output_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n\n    def forward(self, x):\n        batch_size, _, width, height = x.size()\n        query = self.query_conv(x).view(batch_size, self.num_heads, self.head_dim, width * height)\n        key = self.key_conv(x).view(batch_size, self.num_heads, self.head_dim, width * height)\n        value = self.value_conv(x).view(batch_size, self.num_heads, self.head_dim, width * height)\n\n        attn_output = []\n        for i in range(self.num_heads):\n            energy = torch.bmm(query[:, i].permute(0, 2, 1), key[:, i])\n            attention = F.softmax(energy, dim=-1)\n            attn_out = torch.bmm(value[:, i], attention.permute(0, 2, 1))\n            attn_out = attn_out.view(batch_size, self.head_dim, width, height)\n            attn_output.append(attn_out)\n\n        attn_output = torch.cat(attn_output, dim=1)\n        channel_weights = self.channel_attention(attn_output)\n        attn_output = attn_output * channel_weights\n\n        spatial_weights = self.spatial_attention(attn_output)\n        attn_output = attn_output * spatial_weights\n\n        out = self.output_conv(attn_output)\n        return out\n\n# Define the MTUNet2 class\nclass MTUNet2(nn.Module):\n    def __init__(self, in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4):\n        super(MTUNet2, self).__init__()\n        self.encoder = CNNEncoder(in_channels, base_features)\n        self.attn_module = AttentionModule(feature_dim=feature_dim * 8, num_heads=num_heads)\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(feature_dim * 8 * 4 * 4, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, query, support):\n        query_features = self.encoder(query)\n        B, N, C, H, W = support.size()\n        support = support.view(B * N, C, H, W)\n        support_features = self.encoder(support)\n        support_features = support_features.view(B, N, -1, H // 16, W // 16)\n        support_features = support_features.mean(dim=1)\n\n        query_attn = self.attn_module(query_features)\n        support_attn = self.attn_module(support_features)\n\n        combined_features = torch.cat([query_attn, support_attn], dim=1)\n        outputs = self.classifier(combined_features)\n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the train function\ndef train(model, train_loader, criterion, optimizer, epoch, num_epochs):\n    model.train()  # Set the model to training mode\n    running_loss = 0.0  # Initialize running loss\n    total_correct = 0  # Initialize the count of correct predictions\n    total_samples = 0  # Initialize the count of total samples\n\n    for query, support, labels in train_loader:\n        # Move data to the correct device\n        query, support, labels = query.to(device), support.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(query, support)\n\n        # Compute the loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate the loss\n        running_loss += loss.item()\n\n        # Compute the number of correct predictions\n        _, predicted = torch.max(outputs, 1)\n        total_correct += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n    # Calculate average loss and accuracy\n    avg_loss = running_loss / len(train_loader)\n    accuracy = 100 * total_correct / total_samples\n    print(f'Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n\n    return avg_loss, accuracy\n\n# Define the evaluation function\ndef evaluate(model, test_loader, criterion):\n    model.eval()  # Set the model to evaluation mode\n    total_correct = 0  # Initialize the count of correct predictions\n    total_samples = 0  # Initialize the count of total samples\n    total_loss = 0.0  # Initialize the total loss\n\n    with torch.no_grad():  # Disable gradient calculation for evaluation\n        for query, support, labels in test_loader:\n            # Move data to the correct device\n            query, support, labels = query.to(device), support.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(query, support)\n\n            # Compute the loss\n            loss = criterion(outputs, labels)\n\n            # Accumulate the loss\n            total_loss += loss.item()\n\n            # Compute the number of correct predictions\n            _, predicted = torch.max(outputs, 1)\n            total_correct += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n\n    # Calculate average loss and accuracy\n    avg_loss = total_loss / len(test_loader)\n    accuracy = 100 * total_correct / total_samples\n    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n\n    return avg_loss, accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Main training loop\ndef main_train_loop(train_loader, test_loader, model, criterion, optimizer, num_epochs):\n    for epoch in range(1, num_epochs + 1):\n        # Train the model for one epoch\n        train_loss, train_acc = train(model, train_loader, criterion, optimizer, epoch, num_epochs)\n        \n        # Evaluate the model on the test set\n        test_loss, test_acc = evaluate(model, test_loader, criterion)\n\n# Configuration\nnum_epochs = 10\nlearning_rate = 0.001\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize the model\nmodel = MTUNet2(in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4).to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Assuming train_loader and test_loader are defined with compatible shapes and data\n\n# Example data loaders (replace with actual data initialization)\n# train_loader = DataLoader(...)\n# test_loader = DataLoader(...)\n\n# Start the training process\n# Make sure train_loader and test_loader are properly initialized before running\n# main_train_loop(train_loader, test_loader, model, criterion, optimizer, num_epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}