{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.019283Z",
     "iopub.status.busy": "2024-11-28T10:09:46.018968Z",
     "iopub.status.idle": "2024-11-28T10:09:46.024222Z",
     "shell.execute_reply": "2024-11-28T10:09:46.023214Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.019255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet imports several essential libraries and modules that are commonly used in data science, machine learning, and deep learning projects.\n",
    "\n",
    "\n",
    "\n",
    "1. **os**: This module provides a way of using operating system-dependent functionality like reading or writing to the file system. It is useful for tasks such as navigating the file system, handling file paths, and manipulating directories.\n",
    "\n",
    "\n",
    "2. **pandas as pd**: Pandas is a powerful data manipulation and analysis library for Python. It provides data structures like DataFrames, which are particularly useful for handling and analyzing structured data. The alias pd  is a common convention to make the code more concise.\n",
    "   \n",
    "4. **train_test_split from sklearn.model_selection**: This function is part of the scikit-learn library, which is widely used for machine learning tasks. The train_test_split function is used to split a dataset into training and testing sets, which is a crucial step in building and evaluating machine learning models.\n",
    "\n",
    "4. **transforms and datasets from torchvision**: Torchvision is a library that provides tools for computer vision tasks. The transforms module includes common image transformations that are often used in preprocessing steps, such as resizing, cropping, and normalizing images. The datasets module provides access to popular datasets and utilities to load them.\n",
    "\n",
    "5. **Dataset and DataLoader from torch.utils.data**: These classes are part of PyTorch, a deep learning framework. The Dataset class is an abstract class representing a dataset, and the DataLoader class provides an iterable over a dataset, with support for batching, shuffling, and parallel data loading. These are essential for efficiently handling large datasets during training and evaluation of deep learning models.\n",
    "\n",
    "6. **Image from PIL**: The Python Imaging Library (PIL) is a library that adds image processing capabilities to Python. The Image module is used for opening, manipulating, and saving many different image file formats. It is often used in conjunction with torchvision for image preprocessing tasks.\n",
    "\n",
    "Together, these imports set up a robust environment for handling data, preprocessing images, and building machine learning and deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.037974Z",
     "iopub.status.busy": "2024-11-28T10:09:46.037731Z",
     "iopub.status.idle": "2024-11-28T10:09:46.047014Z",
     "shell.execute_reply": "2024-11-28T10:09:46.046159Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.037951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "class HAM10000Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, img_dirs, transform=None, support_set_size=5):\n",
    "\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs\n",
    "        self.transform = transform\n",
    "        self.support_set_size = support_set_size\n",
    "        # Pre-compute label mapping\n",
    "\n",
    "        self.label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n",
    "\n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "    \n",
    "        return len(self.data)\n",
    "    \n",
    "     \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        # Obtain the query image\n",
    "    \n",
    "        question_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "    \n",
    "        label = self.data.iloc[idx]['dx']\n",
    "    \n",
    "     \n",
    "    \n",
    "        query_image = self._load_image(question_name)\n",
    "    \n",
    "     \n",
    "    \n",
    "        # Get the processed label\n",
    "    \n",
    "        label_id = self.label_map[label]\n",
    "    \n",
    "     \n",
    "    \n",
    "        # Construct support set\n",
    "    \n",
    "        support_images = []\n",
    "    \n",
    "        label_class_data = self.data[self.data['dx'] == label]\n",
    "    \n",
    "        support_indices = random.sample(list(label_class_data.index), self.support_set_size)\n",
    "    \n",
    "     \n",
    "    \n",
    "        for support_idx in support_indices:\n",
    "    \n",
    "            support_name = self.data.iloc[support_idx]['image_id'] + '.jpg'\n",
    "    \n",
    "            support_image = self._load_image(support_name)\n",
    "    \n",
    "            support_images.append(support_image)\n",
    "    \n",
    "     \n",
    "    \n",
    "        return query_image, torch.stack(support_images), label_id\n",
    "    \n",
    "     \n",
    "    \n",
    "    def _load_image(self, img_name):\n",
    "    \n",
    "        for img_dir in self.img_dirs:\n",
    "    \n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "    \n",
    "            if os.path.exists(img_path):\n",
    "    \n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "                if self.transform:\n",
    "    \n",
    "                    image = self.transform(image)\n",
    "    \n",
    "                return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines a custom dataset class named HAM10000Dataset that inherits from PyTorch's Dataset\n",
    "**Initialization (init method)**: The constructor takes three parameters: csv_file, img_dirs, and an optional transform. The csv_file is expected to be a CSV file containing metadata about the images, such as their filenames and labels. The img_dirs is a list of directories where the images are stored. The transform parameter allows for optional image transformations (e.g., resizing, normalization) to be applied to the images. The constructor reads the CSV file into a pandas DataFrame and stores the image directories and transform.\n",
    "**Length (len method)**: This method returns the number of samples in the dataset by returning the length of the DataFrame. This is a required method for PyTorch datasets, enabling functions like len(dataset) to work correctly.\n",
    "**Get Item (get item method)**: This method retrieves a single sample from the dataset. It takes an index idx as input and performs the following steps: \n",
    "    Looks up the image name in the DataFrame using the provided index and appends the .jpg extension.\n",
    "    Searches for the image file in the specified directories. If the image is found, it is opened and converted to RGB format. If the image is not found in any directory, a FileNotFoundError is raised.\n",
    "    Retrieves the label for the image from the DataFrame. The label is mapped to an integer using a dictionary that maps unique labels to indices.\n",
    "    If a transform is provided, it is applied to the image.\n",
    "    Returns a tuple containing the image and its corresponding label.\n",
    "\n",
    "This custom dataset class is essential for loading and preprocessing the HAM10000 dataset, making it ready for training and evaluating machine learning models. It handles the complexities of locating images across multiple directories, reading image files, and applying necessary transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.073776Z",
     "iopub.status.busy": "2024-11-28T10:09:46.073253Z",
     "iopub.status.idle": "2024-11-28T10:09:46.104983Z",
     "shell.execute_reply": "2024-11-28T10:09:46.104212Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.073744Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in metadata: 10015\n"
     ]
    }
   ],
   "source": [
    "metadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
    "\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "\n",
    "\n",
    "# Check the number of unique images in metadata\n",
    "\n",
    "print(f\"Total images in metadata: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for loading and inspecting metadata related to the HAM10000 dataset, which is used for skin lesion analysis.\n",
    "\n",
    "\n",
    "1. **Setting the Metadata Path**: The variable metadata_path is assigned the file path to the CSV file containing the metadata for the HAM10000 dataset. This path points to a file named `HAM10000_metadata.csv` located in the directory `../input/skin-cancer-mnist-ham10000/`.\n",
    "\n",
    "2. **Loading the Metadata**: The pd.read_csv(metadata_path) function call reads the CSV file into a pandas DataFrame named metadata. This DataFrame will contain various details about the images, such as their filenames, labels, and possibly other relevant information.\n",
    "\n",
    "3. **Checking the Number of Unique Images**: The print statement outputs the total number of images listed in the metadata. The len(metadata) function call returns the number of rows in the DataFrame, which corresponds to the number of unique images described in the metadata file.\n",
    "\n",
    "\n",
    "This code is crucial for verifying that the metadata has been loaded correctly and for understanding the scope of the dataset by checking the total number of images available for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.107205Z",
     "iopub.status.busy": "2024-11-28T10:09:46.106537Z",
     "iopub.status.idle": "2024-11-28T10:09:46.148053Z",
     "shell.execute_reply": "2024-11-28T10:09:46.147349Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.107160Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split metadata into train and test sets\n",
    "\n",
    "train_metadata, test_metadata = train_test_split(metadata, test_size=0.99, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Save split metadata for easier loading\n",
    "\n",
    "train_metadata.to_csv(\"train_metadata.csv\", index=False)\n",
    "\n",
    "test_metadata.to_csv(\"test_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for splitting the metadata of the HAM10000 dataset into training and testing sets and then saving these splits to CSV files for easier future access.\n",
    "\n",
    "\n",
    "1. **Splitting the Metadata**: The train_test_split function from scikit-learn is used to split the metadata DataFrame into two separate DataFrames: train_metadata and test_metadata. The test_size=0.99 parameter specifies that 99% of the data should be allocated to the test set, leaving only 1% for the training set. The random_state=42 parameter ensures that the split is reproducible by setting a seed for the random number generator.\n",
    "\n",
    "2. **Saving the Split Metadata**: The to_csv method is called on both train_metadata and test_metadata DataFrames to save them as CSV files named train_metadata.csv and test_metadata.csv, respectively. The index=False parameter ensures that the row indices are not included in the saved CSV files.\n",
    "\n",
    "This code is essential for preparing the dataset for machine learning tasks. By splitting the metadata into training and testing sets, it allows for proper evaluation of the model's performance. Saving these splits to CSV files makes it convenient to load the pre-split data in future sessions, avoiding the need to perform the split operation repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.149575Z",
     "iopub.status.busy": "2024-11-28T10:09:46.149259Z",
     "iopub.status.idle": "2024-11-28T10:09:46.153292Z",
     "shell.execute_reply": "2024-11-28T10:09:46.152525Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.149533Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Directories containing images\n",
    "\n",
    "image_dirs = [\n",
    "\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n",
    "\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a list of directories that contain the images for the HAM10000 dataset, which is used for skin lesion analysis.\n",
    "\n",
    "1. **Defining Image Directories**: The variable main.ipynb ) is assigned a list of two directory paths. These directories are specified as strings and point to the locations where the image files are stored. The paths are:\n",
    "\n",
    "   - `../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1`\n",
    "\n",
    "   - `../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2`\n",
    "\n",
    "\n",
    "2. **Purpose of Image Directories**: These directories are likely part of the dataset's structure, where the images have been split into multiple parts for organizational purposes. By listing these directories, the code can later iterate through them to locate and load the images as needed.\n",
    "\n",
    "This setup is crucial for managing and accessing the image files efficiently. By specifying the directories in a list, the code can easily handle the images regardless of their distribution across multiple folders. This approach simplifies the process of loading and preprocessing the images for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.154822Z",
     "iopub.status.busy": "2024-11-28T10:09:46.154498Z",
     "iopub.status.idle": "2024-11-28T10:09:46.162472Z",
     "shell.execute_reply": "2024-11-28T10:09:46.161781Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.154796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((64, 64)),  # Resize images to 256x256\n",
    "\n",
    "    transforms.ToTensor(),         # Convert to PyTorch tensor\n",
    "\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a sequence of image transformations using the transforms.Compose function from the `torchvision` library. These transformations are applied to the images in the HAM10000 dataset to prepare them for input into a machine learning model.\n",
    "\n",
    "1. **Resizing Images**: The transforms.Resize((64, 64)) transformation resizes the images to a fixed size of 64x64 pixels. This step ensures that all images have the same dimensions, which is necessary for batch processing in neural networks. The comment incorrectly mentions resizing to 256x256, but the actual code resizes to 64x64.\n",
    "\n",
    "2. **Converting to Tensor**: The transforms.ToTensor() transformation converts the images from PIL format (or numpy arrays) to PyTorch tensors. This conversion is essential because PyTorch models require input data in tensor format. Additionally, this transformation scales the pixel values from the range [0, 255] to [0, 1].\n",
    "\n",
    "\n",
    "\n",
    "3. **Normalizing**: The transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) transformation normalizes the pixel values of the images. Normalization adjusts the pixel values to have a mean of 0.5 and a standard deviation of 0.5 for each of the three color channels (red, green, and blue). This step helps in stabilizing and speeding up the training process by ensuring that the input data has a consistent distribution.\n",
    "\n",
    "By composing these transformations, the code ensures that the images are uniformly resized, converted to a suitable format for PyTorch, and normalized. These preprocessing steps are crucial for preparing the dataset for training and evaluating machine learning models effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.165546Z",
     "iopub.status.busy": "2024-11-28T10:09:46.164687Z",
     "iopub.status.idle": "2024-11-28T10:09:46.187449Z",
     "shell.execute_reply": "2024-11-28T10:09:46.186799Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.165481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "\n",
    "train_dataset = HAM10000Dataset(csv_file=\"train_metadata.csv\", img_dirs=image_dirs, transform=transform)\n",
    "\n",
    "test_dataset = HAM10000Dataset(csv_file=\"test_metadata.csv\", img_dirs=image_dirs, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet sets up the datasets and data loaders for training and testing a machine learning model using the HAM10000 dataset. It also prints the sizes of the training and testing datasets.\n",
    "\n",
    "1. **Creating Datasets**: train_dataset and test_dataset are instances of the custom  HAM10000Dataset class. \n",
    "   - The train_dataset is initialized with the metadata file `train_metadata.csv`, the list of image directories image_dirs, and the transformation pipeline transform.\n",
    "   - Similarly, the test_dataset is initialized with the metadata file `test_metadata.csv`, the same image directories, and the same transformation pipeline.\n",
    "   - These datasets will handle loading and preprocessing the images and their corresponding labels.\n",
    "\n",
    "2. **Creating DataLoaders**:\n",
    "   - train_loader and test_loader are instances of PyTorch's DataLoader class.\n",
    "   - train_loader is created with the train_dataset, a batch size of 4, shuffling enabled (shuffle=True), and 2 worker threads (num_workers=2) for parallel data loading. Shuffling ensures that the training data is presented in a different order each epoch, which helps in training the model more effectively.\n",
    "   - test_loader is created with the test_dataset, the same batch size of 4, shuffling disabled (shuffle=False), and 2 worker threads. Shuffling is typically disabled for the test set to ensure consistent evaluation.\n",
    "\n",
    "3. **Printing Dataset Sizes**:\n",
    "   - The print statements output the number of samples in the training and testing datasets by calling len() on train_dataset and  test_dataset.\n",
    "   - This provides a quick check to ensure that the datasets have been loaded correctly and to understand the amount of data available for training and testing.\n",
    "\n",
    "Overall, this code is essential for preparing the data pipeline, ensuring that the images and labels are correctly loaded, preprocessed, and batched for training and evaluation of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.226442Z",
     "iopub.status.busy": "2024-11-28T10:09:46.226180Z",
     "iopub.status.idle": "2024-11-28T10:09:46.236162Z",
     "shell.execute_reply": "2024-11-28T10:09:46.235260Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.226416Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Part 1: Model Components\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CNN Encoder class\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(base_features, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 2, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 4, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 8, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.238129Z",
     "iopub.status.busy": "2024-11-28T10:09:46.237847Z",
     "iopub.status.idle": "2024-11-28T10:09:46.249407Z",
     "shell.execute_reply": "2024-11-28T10:09:46.248373Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.238101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        \n",
    "        # Linear transformations for multi-head attention\n",
    "        self.query_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Conv2d(self.head_dim, self.head_dim, kernel_size=1),\n",
    "                nn.Softmax(dim=-1)  # Softmax across the spatial dimension\n",
    "            ) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        # Channel attention to recalibrate feature maps\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(feature_dim, feature_dim // 16, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(feature_dim // 16, feature_dim, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Spatial attention to emphasize important regions in the spatial dimension\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Final 1x1 conv to combine outputs\n",
    "        self.output_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Compute query, key, and value maps for multi-head attention\n",
    "        queries = self.query_conv(features)  # [B, C, H, W]\n",
    "        keys = self.key_conv(features)       # [B, C, H, W]\n",
    "        values = self.value_conv(features)   # [B, C, H, W]\n",
    "        \n",
    "        B, C, H, W = queries.size()\n",
    "        queries = queries.view(B, self.num_heads, self.head_dim, H * W).permute(0, 1, 3, 2)  # [B, heads, H*W, head_dim]\n",
    "        keys = keys.view(B, self.num_heads, self.head_dim, H * W).permute(0, 1, 3, 2)        # [B, heads, H*W, head_dim]\n",
    "        values = values.view(B, self.num_heads, self.head_dim, H * W).permute(0, 1, 3, 2)    # [B, heads, H*W, head_dim]\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            attn_weights = torch.bmm(queries[:, i], keys[:, i].transpose(1, 2))  # [B, H*W, H*W]\n",
    "            attn_weights = self.attn_heads[i](attn_weights.view(B, H, W, H * W)).view(B, H * W, H * W)  # Apply learned attention map\n",
    "            attn_output = torch.bmm(attn_weights, values[:, i])  # [B, H*W, head_dim]\n",
    "            attn_output = attn_output.view(B, H, W, self.head_dim).permute(0, 3, 1, 2)  # [B, head_dim, H, W]\n",
    "            attention_outputs.append(attn_output)\n",
    "        \n",
    "        # Concatenate all attention head outputs\n",
    "        multi_head_output = torch.cat(attention_outputs, dim=1)  # [B, C, H, W]\n",
    "        \n",
    "        # Channel Attention\n",
    "        channel_attn_weights = self.channel_attention(multi_head_output)\n",
    "        channel_attn_output = multi_head_output * channel_attn_weights  # Element-wise multiplication (recalibration)\n",
    "        \n",
    "        # Spatial Attention\n",
    "        avg_pool = torch.mean(channel_attn_output, dim=1, keepdim=True)  # Average pooling across channels\n",
    "        max_pool = torch.max(channel_attn_output, dim=1, keepdim=True)[0]  # Max pooling across channels\n",
    "        spatial_attn_weights = self.spatial_attention(torch.cat([avg_pool, max_pool], dim=1))\n",
    "        spatial_attn_output = channel_attn_output * spatial_attn_weights  # Element-wise multiplication (spatial recalibration)\n",
    "        \n",
    "        # Final 1x1 conv to produce the final attention output\n",
    "        output = self.output_conv(spatial_attn_output)\n",
    "        return output# Define the AttentionModule class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.250806Z",
     "iopub.status.busy": "2024-11-28T10:09:46.250506Z",
     "iopub.status.idle": "2024-11-28T10:09:46.261009Z",
     "shell.execute_reply": "2024-11-28T10:09:46.260155Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.250769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the MTUNet2 class\n",
    "class MTUNet2(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4):\n",
    "        super(MTUNet2, self).__init__()\n",
    "        self.encoder = CNNEncoder(in_channels, base_features)\n",
    "        self.attn_module = AttentionModule(feature_dim=feature_dim * 8, num_heads=num_heads)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim * 8 * 4 * 4, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, query, support):\n",
    "        query_features = self.encoder(query)\n",
    "        B, N, C, H, W = support.size()\n",
    "        support = support.view(B * N, C, H, W)\n",
    "        support_features = self.encoder(support)\n",
    "        support_features = support_features.view(B, N, -1, H // 16, W // 16)\n",
    "        support_features = support_features.mean(dim=1)\n",
    "\n",
    "        query_attn = self.attn_module(query_features)\n",
    "        support_attn = self.attn_module(support_features)\n",
    "\n",
    "        combined_features = torch.cat([query_attn, support_attn], dim=1)\n",
    "        outputs = self.classifier(combined_features)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.262636Z",
     "iopub.status.busy": "2024-11-28T10:09:46.262298Z",
     "iopub.status.idle": "2024-11-28T10:09:46.273787Z",
     "shell.execute_reply": "2024-11-28T10:09:46.272894Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.262591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the train function\n",
    "def train(model, train_loader, criterion, optimizer, epoch, num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    total_correct = 0  # Initialize the count of correct predictions\n",
    "    total_samples = 0  # Initialize the count of total samples\n",
    "\n",
    "    for query, support, labels in train_loader:\n",
    "        # Move data to the correct device\n",
    "        query, support, labels = query.to(device), support.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(query, support)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Compute the number of correct predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    print(f'Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0  # Initialize the count of correct predictions\n",
    "    total_samples = 0  # Initialize the count of total samples\n",
    "    total_loss = 0.0  # Initialize the total loss\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for query, support, labels in test_loader:\n",
    "            # Move data to the correct device\n",
    "            query, support, labels = query.to(device), support.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(query, support)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate the loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the number of correct predictions\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    print(f'Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T10:09:46.278521Z",
     "iopub.status.busy": "2024-11-28T10:09:46.278027Z",
     "iopub.status.idle": "2024-11-28T10:09:48.563232Z",
     "shell.execute_reply": "2024-11-28T10:09:48.561461Z",
     "shell.execute_reply.started": "2024-11-28T10:09:46.278473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 100\n",
      "Number of testing samples: 9915\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [4096, 4096, 1, 1], expected input[4, 512, 4, 4] to have 4096 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of testing samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Start the training process\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure train_loader and test_loader are properly initialized before running\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmain_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 8\u001b[0m, in \u001b[0;36mmain_train_loop\u001b[0;34m(train_loader, test_loader, model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain_train_loop\u001b[39m(train_loader, test_loader, model, criterion, optimizer, num_epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Train the model for one epoch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion)\n",
      "Cell \u001b[0;32mIn[80], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epoch, num_epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[79], line 22\u001b[0m, in \u001b[0;36mMTUNet2.forward\u001b[0;34m(self, query, support)\u001b[0m\n\u001b[1;32m     19\u001b[0m support_features \u001b[38;5;241m=\u001b[39m support_features\u001b[38;5;241m.\u001b[39mview(B, N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m, W \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     20\u001b[0m support_features \u001b[38;5;241m=\u001b[39m support_features\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m query_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m support_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_module(support_features)\n\u001b[1;32m     25\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([query_attn, support_attn], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[78], line 31\u001b[0m, in \u001b[0;36mAttentionModule.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m batch_size, _, width, height \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Multi-head attention: Transform input using query, key, and value projections\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, width \u001b[38;5;241m*\u001b[39m height)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)    \n\u001b[1;32m     32\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_conv(x)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, width \u001b[38;5;241m*\u001b[39m height)\n\u001b[1;32m     33\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_conv(x)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, width \u001b[38;5;241m*\u001b[39m height)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [4096, 4096, 1, 1], expected input[4, 512, 4, 4] to have 4096 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Main training loop\n",
    "def main_train_loop(train_loader, test_loader, model, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train the model for one epoch\n",
    "        train_loss, train_acc = train(model, train_loader, criterion, optimizer, epoch, num_epochs)\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "# Configuration\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = MTUNet2(in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Assuming train_loader and test_loader are defined with compatible shapes and data\n",
    "\n",
    "# Example data loaders (replace with actual data initialization)\n",
    "# train_loader = DataLoader(...)\n",
    "# test_loader = DataLoader(...)\n",
    "# DataLoaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "# Print dataset sizes\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "# Start the training process\n",
    "# Make sure train_loader and test_loader are properly initialized before running\n",
    "main_train_loop(train_loader, test_loader, model, criterion, optimizer, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
