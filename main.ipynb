{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7ca9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T09:09:25.126422Z",
     "iopub.status.busy": "2024-12-13T09:09:25.126096Z",
     "iopub.status.idle": "2024-12-13T10:31:22.900029Z",
     "shell.execute_reply": "2024-12-13T10:31:22.898813Z"
    },
    "papermill": {
     "duration": 4917.779383,
     "end_time": "2024-12-13T10:31:22.901973",
     "exception": false,
     "start_time": "2024-12-13T09:09:25.122590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Unique Classes: ['akiec' 'bcc' 'bkl' 'df' 'mel' 'nv' 'vasc']\n",
      "Number of Classes: 7\n",
      "Epoch [1/150], G Loss: 3.6399, D Loss: 0.2509\n",
      "Epoch [2/150], G Loss: 1.2217, D Loss: 0.3693\n",
      "Epoch [3/150], G Loss: 1.2936, D Loss: 0.4613\n",
      "Epoch [4/150], G Loss: 0.4609, D Loss: 0.7677\n",
      "Epoch [5/150], G Loss: 1.9399, D Loss: 0.6003\n",
      "Epoch [6/150], G Loss: 0.1885, D Loss: 1.0772\n",
      "Epoch [7/150], G Loss: 1.6875, D Loss: 0.4345\n",
      "Epoch [8/150], G Loss: 1.2611, D Loss: 0.4058\n",
      "Epoch [9/150], G Loss: 1.6114, D Loss: 0.5176\n",
      "Epoch [10/150], G Loss: 1.4509, D Loss: 0.4143\n",
      "Epoch [11/150], G Loss: 1.7691, D Loss: 0.4729\n",
      "Epoch [12/150], G Loss: 2.2295, D Loss: 0.3934\n",
      "Epoch [13/150], G Loss: 2.0972, D Loss: 0.4571\n",
      "Epoch [14/150], G Loss: 1.0907, D Loss: 0.4890\n",
      "Epoch [15/150], G Loss: 1.2897, D Loss: 0.3511\n",
      "Epoch [16/150], G Loss: 1.1321, D Loss: 0.3629\n",
      "Epoch [17/150], G Loss: 2.4096, D Loss: 0.3063\n",
      "Epoch [18/150], G Loss: 1.8098, D Loss: 0.2847\n",
      "Epoch [19/150], G Loss: 1.8373, D Loss: 0.3393\n",
      "Epoch [20/150], G Loss: 1.8295, D Loss: 0.5443\n",
      "Epoch [21/150], G Loss: 1.0419, D Loss: 0.5409\n",
      "Epoch [22/150], G Loss: 2.3451, D Loss: 0.5449\n",
      "Epoch [23/150], G Loss: 1.6862, D Loss: 0.2963\n",
      "Epoch [24/150], G Loss: 0.8019, D Loss: 0.6627\n",
      "Epoch [25/150], G Loss: 2.3288, D Loss: 0.5104\n",
      "Epoch [26/150], G Loss: 1.8898, D Loss: 0.4137\n",
      "Epoch [27/150], G Loss: 2.7711, D Loss: 0.4146\n",
      "Epoch [28/150], G Loss: 1.6709, D Loss: 0.4234\n",
      "Epoch [29/150], G Loss: 2.5001, D Loss: 0.5822\n",
      "Epoch [30/150], G Loss: 2.1161, D Loss: 0.3106\n",
      "Epoch [31/150], G Loss: 1.9196, D Loss: 0.3558\n",
      "Epoch [32/150], G Loss: 5.5735, D Loss: 1.0882\n",
      "Epoch [33/150], G Loss: 3.1834, D Loss: 0.5338\n",
      "Epoch [34/150], G Loss: 1.1575, D Loss: 0.4836\n",
      "Epoch [35/150], G Loss: 1.3301, D Loss: 0.4401\n",
      "Epoch [36/150], G Loss: 2.1638, D Loss: 0.4648\n",
      "Epoch [37/150], G Loss: 1.3805, D Loss: 0.4444\n",
      "Epoch [38/150], G Loss: 1.3099, D Loss: 0.4150\n",
      "Epoch [39/150], G Loss: 1.5171, D Loss: 0.5148\n",
      "Epoch [40/150], G Loss: 1.6687, D Loss: 0.4728\n",
      "Epoch [41/150], G Loss: 2.1170, D Loss: 0.5363\n",
      "Epoch [42/150], G Loss: 1.6442, D Loss: 0.3323\n",
      "Epoch [43/150], G Loss: 2.2310, D Loss: 0.5386\n",
      "Epoch [44/150], G Loss: 0.7432, D Loss: 0.6828\n",
      "Epoch [45/150], G Loss: 1.4272, D Loss: 0.4149\n",
      "Epoch [46/150], G Loss: 2.0265, D Loss: 0.4479\n",
      "Epoch [47/150], G Loss: 1.4797, D Loss: 0.3619\n",
      "Epoch [48/150], G Loss: 1.3262, D Loss: 0.4898\n",
      "Epoch [49/150], G Loss: 1.8025, D Loss: 0.3196\n",
      "Epoch [50/150], G Loss: 1.2608, D Loss: 0.6087\n",
      "Epoch [51/150], G Loss: 1.2816, D Loss: 0.5360\n",
      "Epoch [52/150], G Loss: 1.5470, D Loss: 0.3530\n",
      "Epoch [53/150], G Loss: 1.4828, D Loss: 0.4245\n",
      "Epoch [54/150], G Loss: 0.8966, D Loss: 0.4923\n",
      "Epoch [55/150], G Loss: 1.1887, D Loss: 0.4339\n",
      "Epoch [56/150], G Loss: 1.8411, D Loss: 0.4216\n",
      "Epoch [57/150], G Loss: 1.0221, D Loss: 0.5111\n",
      "Epoch [58/150], G Loss: 1.9762, D Loss: 0.5033\n",
      "Epoch [59/150], G Loss: 1.8665, D Loss: 0.4821\n",
      "Epoch [60/150], G Loss: 1.3969, D Loss: 0.3699\n",
      "Epoch [61/150], G Loss: 2.4705, D Loss: 0.5356\n",
      "Epoch [62/150], G Loss: 1.2892, D Loss: 0.4972\n",
      "Epoch [63/150], G Loss: 1.5316, D Loss: 0.4594\n",
      "Epoch [64/150], G Loss: 1.2605, D Loss: 0.4394\n",
      "Epoch [65/150], G Loss: 1.8808, D Loss: 0.5164\n",
      "Epoch [66/150], G Loss: 1.6456, D Loss: 0.5178\n",
      "Epoch [67/150], G Loss: 1.6903, D Loss: 0.4733\n",
      "Epoch [68/150], G Loss: 1.6141, D Loss: 0.4676\n",
      "Epoch [69/150], G Loss: 1.3684, D Loss: 0.4526\n",
      "Epoch [70/150], G Loss: 1.2942, D Loss: 0.3976\n",
      "Epoch [71/150], G Loss: 1.8851, D Loss: 0.3332\n",
      "Epoch [72/150], G Loss: 1.8047, D Loss: 0.3452\n",
      "Epoch [73/150], G Loss: 1.0860, D Loss: 0.5728\n",
      "Epoch [74/150], G Loss: 1.6114, D Loss: 0.4005\n",
      "Epoch [75/150], G Loss: 1.4336, D Loss: 0.4072\n",
      "Epoch [76/150], G Loss: 1.6647, D Loss: 0.4670\n",
      "Epoch [77/150], G Loss: 1.9464, D Loss: 0.4267\n",
      "Epoch [78/150], G Loss: 1.9100, D Loss: 0.4984\n",
      "Epoch [79/150], G Loss: 1.7686, D Loss: 0.3743\n",
      "Epoch [80/150], G Loss: 1.8398, D Loss: 0.3597\n",
      "Epoch [81/150], G Loss: 1.6828, D Loss: 0.3817\n",
      "Epoch [82/150], G Loss: 1.6151, D Loss: 0.4209\n",
      "Epoch [83/150], G Loss: 1.6864, D Loss: 0.4066\n",
      "Epoch [84/150], G Loss: 1.3391, D Loss: 0.5445\n",
      "Epoch [85/150], G Loss: 1.6976, D Loss: 0.3822\n",
      "Epoch [86/150], G Loss: 1.4113, D Loss: 0.5219\n",
      "Epoch [87/150], G Loss: 1.7716, D Loss: 0.4069\n",
      "Epoch [88/150], G Loss: 1.2095, D Loss: 0.6284\n",
      "Epoch [89/150], G Loss: 2.1257, D Loss: 0.3865\n",
      "Epoch [90/150], G Loss: 2.3446, D Loss: 0.5156\n",
      "Epoch [91/150], G Loss: 0.9965, D Loss: 0.6021\n",
      "Epoch [92/150], G Loss: 1.8726, D Loss: 0.3345\n",
      "Epoch [93/150], G Loss: 0.9242, D Loss: 0.5073\n",
      "Epoch [94/150], G Loss: 1.9186, D Loss: 0.3401\n",
      "Epoch [95/150], G Loss: 1.7872, D Loss: 0.3942\n",
      "Epoch [96/150], G Loss: 1.4854, D Loss: 0.6299\n",
      "Epoch [97/150], G Loss: 1.5561, D Loss: 0.5503\n",
      "Epoch [98/150], G Loss: 1.6760, D Loss: 0.3481\n",
      "Epoch [99/150], G Loss: 1.3547, D Loss: 0.3872\n",
      "Epoch [100/150], G Loss: 1.6981, D Loss: 0.4132\n",
      "Epoch [101/150], G Loss: 1.8943, D Loss: 0.3848\n",
      "Epoch [102/150], G Loss: 1.5102, D Loss: 0.4153\n",
      "Epoch [103/150], G Loss: 1.7309, D Loss: 0.4741\n",
      "Epoch [104/150], G Loss: 1.4503, D Loss: 0.3685\n",
      "Epoch [105/150], G Loss: 1.7946, D Loss: 0.5633\n",
      "Epoch [106/150], G Loss: 1.3891, D Loss: 0.3754\n",
      "Epoch [107/150], G Loss: 1.9487, D Loss: 0.3113\n",
      "Epoch [108/150], G Loss: 2.0122, D Loss: 0.3566\n",
      "Epoch [109/150], G Loss: 2.5804, D Loss: 0.4478\n",
      "Epoch [110/150], G Loss: 2.6967, D Loss: 0.4617\n",
      "Epoch [111/150], G Loss: 1.7462, D Loss: 0.4267\n",
      "Epoch [112/150], G Loss: 2.5284, D Loss: 0.3691\n",
      "Epoch [113/150], G Loss: 2.1902, D Loss: 0.5496\n",
      "Epoch [114/150], G Loss: 2.3449, D Loss: 0.4058\n",
      "Epoch [115/150], G Loss: 1.6120, D Loss: 0.3832\n",
      "Epoch [116/150], G Loss: 2.1993, D Loss: 0.3907\n",
      "Epoch [117/150], G Loss: 1.9041, D Loss: 0.4653\n",
      "Epoch [118/150], G Loss: 2.6938, D Loss: 0.5903\n",
      "Epoch [119/150], G Loss: 2.2018, D Loss: 0.5432\n",
      "Epoch [120/150], G Loss: 1.7796, D Loss: 0.4109\n",
      "Epoch [121/150], G Loss: 1.7509, D Loss: 0.4462\n",
      "Epoch [122/150], G Loss: 2.0813, D Loss: 0.2753\n",
      "Epoch [123/150], G Loss: 1.3005, D Loss: 0.3434\n",
      "Epoch [124/150], G Loss: 1.2375, D Loss: 0.4411\n",
      "Epoch [125/150], G Loss: 1.4299, D Loss: 0.3517\n",
      "Epoch [126/150], G Loss: 2.1094, D Loss: 0.2779\n",
      "Epoch [127/150], G Loss: 1.7710, D Loss: 0.3066\n",
      "Epoch [128/150], G Loss: 2.0510, D Loss: 0.4369\n",
      "Epoch [129/150], G Loss: 1.5035, D Loss: 0.4078\n",
      "Epoch [130/150], G Loss: 2.2678, D Loss: 0.3599\n",
      "Epoch [131/150], G Loss: 1.8686, D Loss: 0.3712\n",
      "Epoch [132/150], G Loss: 1.9945, D Loss: 0.4975\n",
      "Epoch [133/150], G Loss: 1.6488, D Loss: 0.3890\n",
      "Epoch [134/150], G Loss: 1.5981, D Loss: 0.4880\n",
      "Epoch [135/150], G Loss: 1.9730, D Loss: 0.4555\n",
      "Epoch [136/150], G Loss: 1.9575, D Loss: 0.4618\n",
      "Epoch [137/150], G Loss: 1.6837, D Loss: 0.4126\n",
      "Epoch [138/150], G Loss: 1.5880, D Loss: 0.4529\n",
      "Epoch [139/150], G Loss: 1.7139, D Loss: 0.4099\n",
      "Epoch [140/150], G Loss: 1.5503, D Loss: 0.3446\n",
      "Epoch [141/150], G Loss: 1.6874, D Loss: 0.3341\n",
      "Epoch [142/150], G Loss: 1.6299, D Loss: 0.4783\n",
      "Epoch [143/150], G Loss: 2.1420, D Loss: 0.3772\n",
      "Epoch [144/150], G Loss: 1.5768, D Loss: 0.4372\n",
      "Epoch [145/150], G Loss: 1.5176, D Loss: 0.3926\n",
      "Epoch [146/150], G Loss: 1.5938, D Loss: 0.4161\n",
      "Epoch [147/150], G Loss: 1.7635, D Loss: 0.3338\n",
      "Epoch [148/150], G Loss: 1.8643, D Loss: 0.3600\n",
      "Epoch [149/150], G Loss: 1.7732, D Loss: 0.3740\n",
      "Epoch [150/150], G Loss: 1.6719, D Loss: 0.4273\n",
      "Synthetic image generation and filtering complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dirs, transform=None, device='cuda'):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.data['encoded_label'] = self.label_encoder.fit_transform(self.data['dx'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                label = self.data.iloc[idx]['encoded_label']\n",
    "                return image, label\n",
    "        raise FileNotFoundError(f\"Image {img_name} not found in directories {self.img_dirs}\")\n",
    "\n",
    "class FastGANGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, img_channels=3, num_classes=7, device='cuda'):\n",
    "        super(FastGANGenerator, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Condition embedding\n",
    "        self.label_embedding = nn.Embedding(num_classes, num_classes).to(device)\n",
    "        \n",
    "        # Generator architecture with conditional input\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, img_channels * 64 * 64),\n",
    "            nn.Tanh()\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # Ensure z and labels are on the correct device\n",
    "        z = z.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        \n",
    "        # Embed labels\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        \n",
    "        # Concatenate noise and label embedding\n",
    "        conditional_input = torch.cat([z, label_embed], dim=1)\n",
    "        \n",
    "        # Generate images\n",
    "        img = self.model(conditional_input)\n",
    "        img = img.view(img.size(0), 3, 64, 64)\n",
    "        return img\n",
    "\n",
    "class FastGANDiscriminator(nn.Module):\n",
    "    def __init__(self, img_channels=3, num_classes=7, device='cuda'):\n",
    "        super(FastGANDiscriminator, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_embedding = nn.Embedding(num_classes, num_classes).to(device)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_channels * 64 * 64 + num_classes, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self, img, labels):\n",
    "        # Ensure img and labels are on the correct device\n",
    "        img = img.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        \n",
    "        # Flatten image\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        \n",
    "        # Embed labels\n",
    "        label_embed = self.label_embedding(labels)\n",
    "        \n",
    "        # Concatenate image and label embedding\n",
    "        conditional_input = torch.cat([img_flat, label_embed], dim=1)\n",
    "        \n",
    "        # Compute validity\n",
    "        validity = self.model(conditional_input)\n",
    "        return validity\n",
    "\n",
    "def train_fastgan(generator, discriminator, dataloader, num_epochs=150):\n",
    "    \"\"\"\n",
    "    Train Conditional FASTGAN\n",
    "    \"\"\"\n",
    "    device = generator.device\n",
    "    \n",
    "    # Optimizers\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for real_images, labels in dataloader:\n",
    "            real_images = real_images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch_size = real_images.size(0)\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            valid = torch.ones(batch_size, 1).to(device)\n",
    "            fake = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Sample noise and labels\n",
    "            z = torch.randn(batch_size, 100).to(device)\n",
    "            gen_labels = torch.randint(0, len(generator.label_embedding.weight), (batch_size,)).to(device)\n",
    "            \n",
    "            # Generate images\n",
    "            generated_images = generator(z, gen_labels)\n",
    "            \n",
    "            # Generator loss\n",
    "            g_loss = adversarial_loss(discriminator(generated_images, gen_labels), valid)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # Train Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            # Real images loss\n",
    "            real_loss = adversarial_loss(discriminator(real_images, labels), valid)\n",
    "            \n",
    "            # Fake images loss\n",
    "            fake_loss = adversarial_loss(discriminator(generated_images.detach(), gen_labels), fake)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], G Loss: {g_loss.item():.4f}, D Loss: {d_loss.item():.4f}\")\n",
    "    \n",
    "    return generator\n",
    "\n",
    "def main():\n",
    "    # Determine device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # 1. Prepare HAM10000 Dataset\n",
    "    csv_file = '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n",
    "    img_dirs = ['/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_1', '/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_images_part_2']\n",
    "    \n",
    "    # Create dataset and data loader\n",
    "    dataset = HAM10000Dataset(csv_file, img_dirs, transform=transform, device=device)\n",
    "    \n",
    "    # Get number of classes\n",
    "    num_classes = len(dataset.label_encoder.classes_)\n",
    "    print(\"Unique Classes:\", dataset.label_encoder.classes_)\n",
    "    print(\"Number of Classes:\", num_classes)\n",
    "    \n",
    "    # Create data loader\n",
    "    batch_size = 64\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # 2. Setup FASTGAN Configuration\n",
    "    generator = FastGANGenerator(num_classes=num_classes, device=device)\n",
    "    discriminator = FastGANDiscriminator(num_classes=num_classes, device=device)\n",
    "    \n",
    "    # 3. Train FASTGAN and Generate 1000 Synthetic Images\n",
    "    trained_generator = train_fastgan(generator, discriminator, data_loader)\n",
    "    \n",
    "    # Generate synthetic images\n",
    "    os.makedirs('synthetic_images', exist_ok=True)\n",
    "    \n",
    "    # Generate images for each class\n",
    "    synthetic_images_by_class = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for class_idx in range(num_classes):\n",
    "            # Generate 100 images for each class\n",
    "            z = torch.randn(100, 100).to(device)\n",
    "            class_labels = torch.full((100,), class_idx, dtype=torch.long).to(device)\n",
    "            \n",
    "            synthetic_images = trained_generator(z, class_labels)\n",
    "            \n",
    "            # Save images\n",
    "            synthetic_images_by_class[class_idx] = synthetic_images.cpu()\n",
    "            \n",
    "            # Save images to disk\n",
    "            class_name = dataset.label_encoder.inverse_transform([class_idx])[0]\n",
    "            class_dir = os.path.join('synthetic_images', class_name)\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            \n",
    "            for i, img in enumerate(synthetic_images):\n",
    "                save_path = os.path.join(class_dir, f'synthetic_image_{i}.png')\n",
    "                save_image((img * 0.5 + 0.5), save_path)\n",
    "    \n",
    "    print(\"Synthetic image generation and filtering complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4921.718859,
   "end_time": "2024-12-13T10:31:24.432654",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-13T09:09:22.713795",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
