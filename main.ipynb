{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-03T05:39:47.288239Z",
     "iopub.status.busy": "2024-12-03T05:39:47.287255Z",
     "iopub.status.idle": "2024-12-03T05:39:57.729130Z",
     "shell.execute_reply": "2024-12-03T05:39:57.728011Z",
     "shell.execute_reply.started": "2024-12-03T05:39:47.288180Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (10.3.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum) (1.26.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install captum torch torchvision pandas pillow matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import Saliency\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Dataset Class\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dirs, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "        \n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                break\n",
    "        \n",
    "        label = self.data.iloc[idx]['dx']\n",
    "        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n",
    "        label = label_map[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Attention Module\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels//8, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels//8, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        q = self.query(x).view(batch, -1, height*width).permute(0,2,1)\n",
    "        k = self.key(x).view(batch, -1, height*width)\n",
    "        v = self.value(x).view(batch, -1, height*width)\n",
    "        \n",
    "        attn = torch.bmm(q, k)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        out = torch.bmm(v, attn.permute(0,2,1))\n",
    "        return out.view(batch, channels, height, width)\n",
    "\n",
    "# UNet blocks\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# UNet with Attention\n",
    "class UNetWithAttention(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(64, 128)\n",
    "        )\n",
    "        self.attention1 = AttentionBlock(128)\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(128, 256)\n",
    "        )\n",
    "        self.attention2 = AttentionBlock(256)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x2 = self.attention1(x2)\n",
    "        x3 = self.down2(x2)\n",
    "        x3 = self.attention2(x3)\n",
    "        return self.classifier(x3)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "# Visualization function\n",
    "def visualize_comparison(model, dataset, n_samples=3):\n",
    "    plt.figure(figsize=(15, 4*n_samples))\n",
    "    \n",
    "    indices = np.random.choice(len(dataset), n_samples, replace=False)\n",
    "    \n",
    "    for idx, sample_idx in enumerate(indices):\n",
    "        image, label = dataset[sample_idx]\n",
    "        image = image.to(device)\n",
    "        \n",
    "        saliency_map = generate_saliency_map(model, image, label)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            d_output = discriminator(image.unsqueeze(0))\n",
    "        \n",
    "        plt.subplot(n_samples, 4, idx*4 + 1)\n",
    "        plt.imshow(image.cpu().permute(1,2,0))\n",
    "        plt.title(f'Original (Class {label})')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n_samples, 4, idx*4 + 2)\n",
    "        plt.imshow(saliency_map.mean(axis=0), cmap='hot')\n",
    "        plt.title('Saliency Map')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n_samples, 4, idx*4 + 3)\n",
    "        attention = d_output.cpu().numpy().reshape(1, -1)\n",
    "        plt.imshow(attention, cmap='viridis')\n",
    "        plt.title('Discriminator Attention')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(n_samples, 4, idx*4 + 4)\n",
    "        overlay = image.cpu().permute(1,2,0).numpy()\n",
    "        overlay = overlay * 0.7 + np.expand_dims(saliency_map.mean(axis=0), -1) * 0.3\n",
    "        plt.imshow(overlay)\n",
    "        plt.title('Combined View')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, discriminator, train_loader, criterion, optimizer, d_optimizer):\n",
    "    model.train()\n",
    "    discriminator.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_d_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        batch_size = data.size(0)\n",
    "        \n",
    "        # Train discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        \n",
    "        d_real = discriminator(data)\n",
    "        d_real_loss = F.binary_cross_entropy(d_real, real_labels)\n",
    "        d_real_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train classifier\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_d_loss += d_real_loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f} D_Loss: {d_real_loss.item():.4f}')\n",
    "    \n",
    "    return epoch_loss / len(train_loader), epoch_d_loss / len(train_loader)\n",
    "\n",
    "# Setup and training\n",
    "if __name__ == \"__main__\":\n",
    "    # Data paths\n",
    "    metadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
    "    image_dirs = [\n",
    "        \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n",
    "        \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
    "    ]\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    train_metadata, test_metadata = train_test_split(metadata, test_size=0.001, random_state=42)\n",
    "    \n",
    "    # Save splits\n",
    "    train_metadata.to_csv(\"train_metadata.csv\", index=False)\n",
    "    test_metadata.to_csv(\"test_metadata.csv\", index=False)\n",
    "    \n",
    "    # Setup transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = HAM10000Dataset(\"train_metadata.csv\", image_dirs, transform=transform)\n",
    "    test_dataset = HAM10000Dataset(\"test_metadata.csv\", image_dirs, transform=transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize models\n",
    "    model = UNetWithAttention(3, 7).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    # Setup optimizers\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters())\n",
    "    \n",
    "    # Training\n",
    "    n_epochs = 10\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    training_time = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "        \n",
    "        train_loss, d_loss = train_epoch(model, discriminator, train_loader, criterion, optimizer, d_optimizer)\n",
    "        train_acc = calculate_accuracy(model, train_loader, device)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        training_time.append(epoch_time)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss={train_loss:.4f}, Accuracy={train_acc:.4f}, Time={epoch_time:.2f}s')\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    print(f\"\\nTraining completed in {total_time/60:.2f} minutes\")\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_comparison(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
