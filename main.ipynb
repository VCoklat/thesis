{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install captum torch torchvision pandas pillow matplotlib scikit-learn","metadata":{"execution":{"iopub.execute_input":"2024-12-03T05:39:47.288239Z","iopub.status.busy":"2024-12-03T05:39:47.287255Z","iopub.status.idle":"2024-12-03T05:39:57.729130Z","shell.execute_reply":"2024-12-03T05:39:57.728011Z","shell.execute_reply.started":"2024-12-03T05:39:47.288180Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid, fd = os.forkpty()\n"]},{"name":"stdout","output_type":"stream","text":["Collecting captum\n","  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (10.3.0)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum) (1.26.4)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum) (4.66.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: captum\n","Successfully installed captum-0.7.0\n"]}],"execution_count":4},{"cell_type":"code","source":"import os\n\nimport pandas as pd\n\nfrom PIL import Image\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom captum.attr import Saliency\n\n\n\nclass HAM10000Dataset(Dataset):\n\n    def __init__(self, csv_file, img_dirs, transform=None):\n\n        self.data = pd.read_csv(csv_file)\n\n        self.img_dirs = img_dirs  # List of directories\n\n        self.transform = transform\n\n\n\n    def __len__(self):\n\n        return len(self.data)\n\n\n\n    def __getitem__(self, idx):\n\n        # Look up the image name\n\n        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n\n        \n\n        # Search for the image in the directories\n\n        for img_dir in self.img_dirs:\n\n            img_path = os.path.join(img_dir, img_name)\n\n            if os.path.exists(img_path):\n\n                image = Image.open(img_path).convert('RGB')\n\n                break\n\n        else:\n\n            raise FileNotFoundError(f\"Image {img_name} not found in specified directories.\")\n\n        \n\n        # Get the label\n\n        label = self.data.iloc[idx]['dx']  # Diagnosis column\n\n        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n\n        label = label_map[label]\n\n\n\n        if self.transform:\n\n            image = self.transform(image)\n\n\n\n        return image, label\n\n\n\nmetadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n\nmetadata = pd.read_csv(metadata_path)\n\n\n\n# Check the number of unique images in metadata\n\nprint(f\"Total images in metadata: {len(metadata)}\")\n\n\n\n# Split metadata into train and test sets\n\ntrain_metadata, test_metadata = train_test_split(metadata, test_size=0.001, random_state=42)\n\n\n\n# Save split metadata for easier loading\n\ntrain_metadata.to_csv(\"train_metadata.csv\", index=False)\n\ntest_metadata.to_csv(\"test_metadata.csv\", index=False)\n\n\n\n# Directories containing images\n\nimage_dirs = [\n\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n\n]\n\n# Attention Module\n\nclass AttentionBlock(nn.Module):\n\n    def __init__(self, in_channels):\n\n        super().__init__()\n\n        self.query = nn.Conv2d(in_channels, in_channels//8, 1)\n\n        self.key = nn.Conv2d(in_channels, in_channels//8, 1)\n\n        self.value = nn.Conv2d(in_channels, in_channels, 1)\n\n        \n\n    def forward(self, x):\n\n        batch, channels, height, width = x.size()\n\n        q = self.query(x).view(batch, -1, height*width).permute(0,2,1)\n\n        k = self.key(x).view(batch, -1, height*width)\n\n        v = self.value(x).view(batch, -1, height*width)\n\n        \n\n        attn = torch.bmm(q, k)\n\n        attn = F.softmax(attn, dim=2)\n\n        out = torch.bmm(v, attn.permute(0,2,1))\n\n        return out.view(batch, channels, height, width)\n\n\n\n# UNet blocks\n\nclass DoubleConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n\n        super().__init__()\n\n        self.conv = nn.Sequential(\n\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n\n            nn.BatchNorm2d(out_channels),\n\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n\n            nn.BatchNorm2d(out_channels),\n\n            nn.ReLU(inplace=True)\n\n        )\n\n        \n\n    def forward(self, x):\n\n        return self.conv(x)\n\n\n\n# UNet with Attention\n\nclass UNetWithAttention(nn.Module):\n\n    def __init__(self, n_channels, n_classes):\n\n        super().__init__()\n\n        self.n_channels = n_channels\n\n        self.n_classes = n_classes\n\n        \n\n        self.inc = DoubleConv(n_channels, 64)\n\n        self.down1 = nn.Sequential(\n\n            nn.MaxPool2d(2),\n\n            DoubleConv(64, 128)\n\n        )\n\n        self.attention1 = AttentionBlock(128)\n\n        self.down2 = nn.Sequential(\n\n            nn.MaxPool2d(2),\n\n            DoubleConv(128, 256)\n\n        )\n\n        self.attention2 = AttentionBlock(256)\n\n        \n\n        self.classifier = nn.Sequential(\n\n            nn.AdaptiveAvgPool2d((1,1)),\n\n            nn.Flatten(),\n\n            nn.Linear(256, n_classes)\n\n        )\n\n        \n\n    def forward(self, x):\n\n        x1 = self.inc(x)\n\n        x2 = self.down1(x1)\n\n        x2 = self.attention1(x2)\n\n        x3 = self.down2(x2)\n\n        x3 = self.attention2(x3)\n\n        return self.classifier(x3)\n\n\n\n# Modified Discriminator to flatten output\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n\n        super().__init__()\n\n        self.main = nn.Sequential(\n\n            nn.Conv2d(3, 64, 4, 2, 1),\n\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(64, 128, 4, 2, 1),\n\n            nn.BatchNorm2d(128),\n\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(128, 256, 4, 2, 1),\n\n            nn.BatchNorm2d(256),\n\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(256, 1, 4, 1, 0),\n\n            nn.Flatten(),  # Add flatten layer\n\n            nn.Sigmoid()\n\n        )\n\n        \n\n    def forward(self, x):\n\n        return self.main(x)\n\n\n\n# Training setup\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\ntransform = transforms.Compose([\n\n    transforms.Resize((224, 224)),\n\n    transforms.ToTensor(),\n\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n\n                       std=[0.229, 0.224, 0.225])\n\n])\n\n\n\n# Create datasets\n\ntrain_dataset = HAM10000Dataset(\"train_metadata.csv\", image_dirs, transform=transform)\n\ntest_dataset = HAM10000Dataset(\"test_metadata.csv\", image_dirs, transform=transform)\n\n\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n\n\n# Initialize models\n\nmodel = UNetWithAttention(3, 7).to(device)\n\ndiscriminator = Discriminator().to(device)\n\n\n\n# Loss and optimizers\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(model.parameters())\n\nd_optimizer = torch.optim.Adam(discriminator.parameters())\n\n\n\n# Modified training loop\n\ndef train_epoch(model, discriminator, train_loader, criterion, optimizer, d_optimizer):\n\n    model.train()\n\n    discriminator.train()\n\n    \n\n    for batch_idx, (data, target) in enumerate(train_loader):\n\n        data, target = data.to(device), target.to(device)\n\n        batch_size = data.size(0)\n\n        \n\n        # Train discriminator\n\n        d_optimizer.zero_grad()\n\n        real_labels = torch.ones(batch_size).to(device)\n\n        fake_labels = torch.zeros(batch_size).to(device)\n\n        \n\n        d_real = discriminator(data)\n\n        d_real_loss = F.binary_cross_entropy(d_real.squeeze(), real_labels)\n\n        d_real_loss.backward()\n\n        d_optimizer.step()\n\n        \n\n        # Train classifier\n\n        optimizer.zero_grad()\n\n        output = model(data)\n\n        loss = criterion(output, target)\n\n        loss.backward()\n\n        optimizer.step()\n\n        \n\n        if batch_idx % 100 == 0:\n\n            print(f'Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f} D_Loss: {d_real_loss.item():.4f}')\n\n\n\n# Saliency map visualization\n\ndef generate_saliency_map(model, input_image):\n\n    saliency = Saliency(model)\n\n    input_image = input_image.unsqueeze(0).requires_grad_()\n\n    attribution = saliency.attribute(input_image)\n\n    \n\n    return attribution.squeeze().cpu().detach().numpy()\n\n\n\n# Training\n\nn_epochs = 10\n\nfor epoch in range(n_epochs):\n\n    print(f'Epoch {epoch+1}/{n_epochs}')\n\n    train_epoch(model, discriminator, train_loader, criterion, optimizer, d_optimizer)\n\n\n\n# Example of saliency map generation\n\nsample_data, _ = next(iter(test_loader))\n\nsample_image = sample_data[0].to(device)\n\nsaliency_map = generate_saliency_map(model, sample_image)\n\n\n\nplt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\n\nplt.imshow(sample_image.cpu().permute(1,2,0))\n\nplt.title('Original Image')\n\nplt.subplot(1,2,2)\n\nplt.imshow(saliency_map.mean(axis=0), cmap='hot')\n\nplt.title('Saliency Map')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-12-03T05:48:42.934795Z","iopub.execute_input":"2024-12-03T05:48:42.935109Z","iopub.status.idle":"2024-12-03T05:48:43.975075Z","shell.execute_reply.started":"2024-12-03T05:48:42.935081Z","shell.execute_reply":"2024-12-03T05:48:43.973717Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total images in metadata: 10015\nEpoch 1/10\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 439\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 439\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Example of saliency map generation\u001b[39;00m\n\u001b[1;32m    445\u001b[0m sample_data, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n","Cell \u001b[0;32mIn[9], line 387\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, discriminator, train_loader, criterion, optimizer, d_optimizer)\u001b[0m\n\u001b[1;32m    381\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    385\u001b[0m d_real \u001b[38;5;241m=\u001b[39m discriminator(data)\n\u001b[0;32m--> 387\u001b[0m d_real_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_real\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m d_real_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    391\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3161\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3166\u001b[0m     )\n\u001b[1;32m   3168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3169\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n","\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 625])) is deprecated. Please ensure they have the same size."],"ename":"ValueError","evalue":"Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 625])) is deprecated. Please ensure they have the same size.","output_type":"error"}],"execution_count":9},{"cell_type":"markdown","source":"# Data Loading and Preprocessing\n\n\n\nLoad HAM10000 dataset, implement data augmentation, and prepare few-shot episodes with support and query sets.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\n\nimport pandas as pd\n\n\n\nimport os\n\n\n\nimport cv2\n\n\n\nimport matplotlib.pyplot as plt\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n\n\n\n\n\n# Load the dataset\n\n\n\nmetadata = pd.read_csv('../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\n\nimage_path = '../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1'\n\n\n\n\n\n\n\n# Function to load images\n\n\n\ndef load_images(df, image_path):\n\n\n\n    images = []\n\n\n\n    for img_id in df['image_id']:\n\n\n\n        img = cv2.imread(os.path.join(image_path, f'{img_id}.jpg'))\n\n\n\n        img = cv2.resize(img, (128, 128))\n\n\n\n        images.append(img)\n\n\n\n    return np.array(images)\n\n\n\n\n\n\n\n# Load images\n\n\n\nimages = load_images(metadata, image_path)\n\n\n\nlabels = metadata['dx'].values\n\n\n\n\n\n\n\n# Split the data into training and validation sets\n\n\n\ntrain_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n\n\n\n\n\n\n\n# Data augmentation\n\n\n\ndatagen = ImageDataGenerator(\n\n\n\n    rotation_range=20,\n\n\n\n    width_shift_range=0.2,\n\n\n\n    height_shift_range=0.2,\n\n\n\n    shear_range=0.2,\n\n\n\n    zoom_range=0.2,\n\n\n\n    horizontal_flip=True,\n\n\n\n    fill_mode='nearest'\n\n\n\n)\n\n\n\n\n\n\n\n# Prepare few-shot episodes\n\n\n\ndef create_few_shot_episodes(images, labels, n_way=5, k_shot=1, k_query=1):\n\n\n\n    unique_labels = np.unique(labels)\n\n\n\n    episodes = []\n\n\n\n    for _ in range(len(images) // (n_way * (k_shot + k_query))):\n\n\n\n        selected_labels = np.random.choice(unique_labels, n_way, replace=False)\n\n\n\n        support_set = []\n\n\n\n        query_set = []\n\n\n\n        for label in selected_labels:\n\n\n\n            label_indices = np.where(labels == label)[0]\n\n\n\n            selected_indices = np.random.choice(label_indices, k_shot + k_query, replace=False)\n\n\n\n            support_set.append(images[selected_indices[:k_shot]])\n\n\n\n            query_set.append(images[selected_indices[k_shot:]])\n\n\n\n        episodes.append((np.array(support_set), np.array(query_set)))\n\n\n\n    return episodes\n\n\n\n\n\n\n\n# Create few-shot episodes\n\n\n\nfew_shot_episodes = create_few_shot_episodes(train_images, train_labels)","metadata":{"execution":{"iopub.execute_input":"2024-12-03T05:32:57.353884Z","iopub.status.busy":"2024-12-03T05:32:57.353521Z","iopub.status.idle":"2024-12-03T05:32:57.563422Z","shell.execute_reply":"2024-12-03T05:32:57.562039Z","shell.execute_reply.started":"2024-12-03T05:32:57.353849Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[ WARN:0@175.033] global loadsave.cpp:241 findDecoder imread_('../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0031633.jpg'): can't open/read file: check file path/integrity\n"]},{"ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mload_images\u001b[0;34m(df, image_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_id \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     32\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 34\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images)\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"]}],"execution_count":2},{"cell_type":"markdown","source":"# Define UNet Architecture with Attention\n\n\n\nImplement UNet with skip connections and self-attention mechanism for better feature extraction.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Input, Activation, BatchNormalization, Add, Multiply\n\n\n\nfrom tensorflow.keras.models import Model\n\n\n\n\n\n\n\n# Attention block\n\n\n\ndef attention_block(x, g, inter_channel):\n\n\n\n    theta_x = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(x)\n\n\n\n    phi_g = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(g)\n\n\n\n    f = Activation('relu')(Add()([theta_x, phi_g]))\n\n\n\n    psi_f = Conv2D(1, (1, 1), strides=(1, 1), padding='same')(f)\n\n\n\n    rate = Activation('sigmoid')(psi_f)\n\n\n\n    att_x = Multiply()([x, rate])\n\n\n\n    return att_x\n\n\n\n\n\n\n\n# UNet with Attention\n\n\n\ndef unet_with_attention(input_shape=(128, 128, 3)):\n\n\n\n    inputs = Input(input_shape)\n\n\n\n    \n\n\n\n    # Encoder\n\n\n\n    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n\n\n\n    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n\n\n\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n\n\n    \n\n\n\n    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n\n\n\n    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n\n\n\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n\n\n    \n\n\n\n    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n\n\n\n    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n\n\n\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n\n\n    \n\n\n\n    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n\n\n\n    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n\n\n\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n\n\n    \n\n\n\n    # Bottleneck\n\n\n\n    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n\n\n\n    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)\n\n\n\n    \n\n\n\n    # Decoder\n\n\n\n    up6 = UpSampling2D(size=(2, 2))(conv5)\n\n\n\n    att6 = attention_block(conv4, up6, 512)\n\n\n\n    merge6 = Concatenate()([att6, up6])\n\n\n\n    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(merge6)\n\n\n\n    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n\n\n\n    \n\n\n\n    up7 = UpSampling2D(size=(2, 2))(conv6)\n\n\n\n    att7 = attention_block(conv3, up7, 256)\n\n\n\n    merge7 = Concatenate()([att7, up7])\n\n\n\n    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(merge7)\n\n\n\n    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n\n\n\n    \n\n\n\n    up8 = UpSampling2D(size=(2, 2))(conv7)\n\n\n\n    att8 = attention_block(conv2, up8, 128)\n\n\n\n    merge8 = Concatenate()([att8, up8])\n\n\n\n    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge8)\n\n\n\n    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n\n\n\n    \n\n\n\n    up9 = UpSampling2D(size=(2, 2))(conv8)\n\n\n\n    att9 = attention_block(conv1, up9, 64)\n\n\n\n    merge9 = Concatenate()([att9, up9])\n\n\n\n    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge9)\n\n\n\n    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n\n\n\n    \n\n\n\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n\n\n    \n\n\n\n    model = Model(inputs, conv10)\n\n\n\n    \n\n\n\n    return model\n\n\n\n\n\n\n\n# Create the model\n\n\n\nunet_model = unet_with_attention()\n\n\n\nunet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n\nunet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-12-03T05:32:57.564222Z","iopub.status.idle":"2024-12-03T05:32:57.564526Z","shell.execute_reply":"2024-12-03T05:32:57.564398Z","shell.execute_reply.started":"2024-12-03T05:32:57.564382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Implement Few-Shot Learning Components\n\n\n\nCreate prototypical network components for few-shot learning, including support set embedding and query set comparison.","metadata":{}},{"cell_type":"code","source":"# Implement Few-Shot Learning Components\n\n\n\n\n\n\n\nimport tensorflow as tf\n\n\n\nfrom tensorflow.keras.layers import Dense, Flatten\n\n\n\nfrom tensorflow.keras.models import Model\n\n\n\n\n\n\n\n# Prototypical Network Components\n\n\n\nclass PrototypicalNetwork(Model):\n\n\n\n    def __init__(self, encoder):\n\n\n\n        super(PrototypicalNetwork, self).__init__()\n\n\n\n        self.encoder = encoder\n\n\n\n\n\n\n\n    def call(self, support_set, query_set):\n\n\n\n        # Embed the support set\n\n\n\n        support_embeddings = self.encoder(support_set)\n\n\n\n        support_embeddings = tf.reduce_mean(support_embeddings, axis=1)  # Average embeddings for each class\n\n\n\n\n\n\n\n        # Embed the query set\n\n\n\n        query_embeddings = self.encoder(query_set)\n\n\n\n\n\n\n\n        return support_embeddings, query_embeddings\n\n\n\n\n\n\n\n# Encoder Model\n\n\n\ndef create_encoder(input_shape=(128, 128, 3)):\n\n\n\n    inputs = Input(input_shape)\n\n\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n\n\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n\n\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n\n\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n\n\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n\n\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n\n\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n\n\n    x = Flatten()(x)\n\n\n\n    x = Dense(1024, activation='relu')(x)\n\n\n\n    model = Model(inputs, x)\n\n\n\n    return model\n\n\n\n\n\n\n\n# Create encoder and prototypical network\n\n\n\nencoder = create_encoder()\n\n\n\nproto_net = PrototypicalNetwork(encoder)\n\n\n\n\n\n\n\n# Example usage with few-shot episodes\n\n\n\nsupport_set, query_set = few_shot_episodes[0]\n\n\n\nsupport_embeddings, query_embeddings = proto_net(support_set, query_set)\n\n\n\n\n\n\n\n# Calculate distances between support and query embeddings\n\n\n\ndef euclidean_distance(a, b):\n\n\n\n    return tf.sqrt(tf.reduce_sum(tf.square(a - b), axis=-1))\n\n\n\n\n\n\n\ndistances = euclidean_distance(tf.expand_dims(query_embeddings, 1), tf.expand_dims(support_embeddings, 0))\n\n\n\n\n\n\n\n# Predict class based on nearest prototype\n\n\n\npredictions = tf.argmin(distances, axis=-1)\n\n\n\n\n\n\n\n# Print predictions\n\n\n\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-12-03T05:32:57.566343Z","iopub.status.idle":"2024-12-03T05:32:57.567058Z","shell.execute_reply":"2024-12-03T05:32:57.566835Z","shell.execute_reply.started":"2024-12-03T05:32:57.566811Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FASTGAN Implementation\n\n\n\nImplement FASTGAN for data augmentation to enhance few-shot learning performance.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, ReLU, Flatten, Dense, Reshape\n\n\n\nfrom tensorflow.keras.models import Model\n\n\n\n\n\n\n\n# Define the generator model for FASTGAN\n\n\n\ndef build_generator(latent_dim):\n\n\n\n    model = tf.keras.Sequential()\n\n\n\n    model.add(Dense(8 * 8 * 256, input_dim=latent_dim))\n\n\n\n    model.add(Reshape((8, 8, 256)))\n\n\n\n    model.add(BatchNormalization())\n\n\n\n    model.add(ReLU())\n\n\n\n\n\n\n\n    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n\n\n\n    model.add(BatchNormalization())\n\n\n\n    model.add(ReLU())\n\n\n\n\n\n\n\n    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n\n\n\n    model.add(BatchNormalization())\n\n\n\n    model.add(ReLU())\n\n\n\n\n\n\n\n    model.add(Conv2DTranspose(32, kernel_size=4, strides=2, padding='same'))\n\n\n\n    model.add(BatchNormalization())\n\n\n\n    model.add(ReLU())\n\n\n\n\n\n\n\n    model.add(Conv2D(3, kernel_size=3, padding='same', activation='tanh'))\n\n\n\n    return model\n\n\n\n\n\n\n\n# Define the discriminator model for FASTGAN\n\n\n\ndef build_discriminator(input_shape):\n\n\n\n    model = tf.keras.Sequential()\n\n\n\n    model.add(Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=input_shape))\n\n\n\n    model.add(LeakyReLU(alpha=0.2))\n\n\n\n\n\n\n\n    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n\n\n\n    model.add(BatchNormalization())\n\n\n\n    model.add(LeakyReLU(alpha=0.2))\n\n\n\n\n\n\n\n    model.add(Conv2D(256, kernel_size=4, strides=2, padding='same'))\n\n\n\n    model.add(BatchNormalization())\n\n\n\n    model.add(LeakyReLU(alpha=0.2))\n\n\n\n\n\n\n\n    model.add(Flatten())\n\n\n\n    model.add(Dense(1, activation='sigmoid'))\n\n\n\n    return model\n\n\n\n\n\n\n\n# Define the GAN model combining generator and discriminator\n\n\n\ndef build_gan(generator, discriminator):\n\n\n\n    discriminator.trainable = False\n\n\n\n    model = tf.keras.Sequential([generator, discriminator])\n\n\n\n    return model\n\n\n\n\n\n\n\n# Set parameters\n\n\n\nlatent_dim = 100\n\n\n\ninput_shape = (128, 128, 3)\n\n\n\n\n\n\n\n# Build and compile the discriminator\n\n\n\ndiscriminator = build_discriminator(input_shape)\n\n\n\ndiscriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n\n\n\n\n\n# Build the generator\n\n\n\ngenerator = build_generator(latent_dim)\n\n\n\n\n\n\n\n# Build and compile the GAN\n\n\n\ngan = build_gan(generator, discriminator)\n\n\n\ngan.compile(optimizer='adam', loss='binary_crossentropy')\n\n\n\n\n\n\n\n# Function to generate and save images\n\n\n\ndef generate_and_save_images(model, epoch, test_input):\n\n\n\n    predictions = model(test_input, training=False)\n\n\n\n    fig = plt.figure(figsize=(4, 4))\n\n\n\n\n\n\n\n    for i in range(predictions.shape[0]):\n\n\n\n        plt.subplot(4, 4, i + 1)\n\n\n\n        plt.imshow((predictions[i] * 127.5 + 127.5).astype(np.uint8))\n\n\n\n        plt.axis('off')\n\n\n\n\n\n\n\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n\n\n\n    plt.show()\n\n\n\n\n\n\n\n# Training the GAN\n\n\n\ndef train_gan(generator, discriminator, gan, dataset, latent_dim, epochs=10000, batch_size=64, save_interval=200):\n\n\n\n    half_batch = batch_size // 2\n\n\n\n\n\n\n\n    for epoch in range(epochs):\n\n\n\n        # Train discriminator\n\n\n\n        idx = np.random.randint(0, dataset.shape[0], half_batch)\n\n\n\n        real_images = dataset[idx]\n\n\n\n        real_labels = np.ones((half_batch, 1))\n\n\n\n\n\n\n\n        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n\n\n        fake_images = generator.predict(noise)\n\n\n\n        fake_labels = np.zeros((half_batch, 1))\n\n\n\n\n\n\n\n        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n\n\n\n        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n\n\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n\n\n\n\n\n\n        # Train generator\n\n\n\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n\n\n        valid_labels = np.ones((batch_size, 1))\n\n\n\n        g_loss = gan.train_on_batch(noise, valid_labels)\n\n\n\n\n\n\n\n        # Print progress\n\n\n\n        if epoch % save_interval == 0:\n\n\n\n            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n\n\n\n            generate_and_save_images(generator, epoch, np.random.normal(0, 1, (16, latent_dim)))\n\n\n\n\n\n\n\n# Prepare the dataset for GAN training\n\n\n\ntrain_images = (train_images.astype(np.float32) - 127.5) / 127.5\n\n\n\n\n\n\n\n# Train the GAN\n\n\n\ntrain_gan(generator, discriminator, gan, train_images, latent_dim)","metadata":{"execution":{"iopub.status.busy":"2024-12-03T05:32:57.568407Z","iopub.status.idle":"2024-12-03T05:32:57.568748Z","shell.execute_reply":"2024-12-03T05:32:57.568591Z","shell.execute_reply.started":"2024-12-03T05:32:57.568575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training\n\n\n\nTrain the model using episodic training paradigm, combining few-shot learning with GAN-augmented data.","metadata":{}},{"cell_type":"code","source":"# Model Training\n\n\n\n\n\n\n\n# Define the episodic training function\n\n\n\ndef train_prototypical_network(proto_net, episodes, epochs=10):\n\n\n\n    optimizer = tf.keras.optimizers.Adam()\n\n\n\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n\n\n\n\n\n\n    for epoch in range(epochs):\n\n\n\n        epoch_loss = 0\n\n\n\n        for support_set, query_set in episodes:\n\n\n\n            with tf.GradientTape() as tape:\n\n\n\n                support_embeddings, query_embeddings = proto_net(support_set, query_set)\n\n\n\n                distances = euclidean_distance(tf.expand_dims(query_embeddings, 1), tf.expand_dims(support_embeddings, 0))\n\n\n\n                predictions = tf.argmin(distances, axis=-1)\n\n\n\n                loss = loss_fn(tf.range(len(predictions)), predictions)\n\n\n\n            gradients = tape.gradient(loss, proto_net.trainable_variables)\n\n\n\n            optimizer.apply_gradients(zip(gradients, proto_net.trainable_variables))\n\n\n\n            epoch_loss += loss\n\n\n\n\n\n\n\n        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(episodes)}')\n\n\n\n\n\n\n\n# Train the Prototypical Network\n\n\n\ntrain_prototypical_network(proto_net, few_shot_episodes)\n\n\n\n\n\n\n\n# Integrate GAN-augmented data into training\n\n\n\ndef augment_with_gan(generator, support_set, k_shot, latent_dim):\n\n\n\n    noise = np.random.normal(0, 1, (k_shot, latent_dim))\n\n\n\n    generated_images = generator.predict(noise)\n\n\n\n    return np.concatenate([support_set, generated_images], axis=0)\n\n\n\n\n\n\n\n# Augment support sets with GAN-generated images\n\n\n\naugmented_episodes = []\n\n\n\nfor support_set, query_set in few_shot_episodes:\n\n\n\n    augmented_support_set = augment_with_gan(generator, support_set, k_shot=1, latent_dim=latent_dim)\n\n\n\n    augmented_episodes.append((augmented_support_set, query_set))\n\n\n\n\n\n\n\n# Train the Prototypical Network with augmented data\n\n\n\ntrain_prototypical_network(proto_net, augmented_episodes)\n\n\n\n\n\n\n\n# Explainability using saliency maps\n\n\n\ndef compute_saliency_maps(model, images, labels):\n\n\n\n    images = tf.convert_to_tensor(images)\n\n\n\n    with tf.GradientTape() as tape:\n\n\n\n        tape.watch(images)\n\n\n\n        predictions = model(images)\n\n\n\n        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n\n\n\n    gradients = tape.gradient(loss, images)\n\n\n\n    saliency_maps = tf.reduce_max(tf.abs(gradients), axis=-1)\n\n\n\n    return saliency_maps\n\n\n\n\n\n\n\n# Compute saliency maps for validation images\n\n\n\nval_images_tensor = tf.convert_to_tensor(val_images)\n\n\n\nval_labels_tensor = tf.convert_to_tensor(val_labels)\n\n\n\nsaliency_maps = compute_saliency_maps(proto_net.encoder, val_images_tensor, val_labels_tensor)\n\n\n\n\n\n\n\n# Display saliency maps\n\n\n\ndef display_saliency_maps(images, saliency_maps):\n\n\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n\n\n    axes[0].imshow(images[0].astype(np.uint8))\n\n\n\n    axes[0].set_title('Original Image')\n\n\n\n    axes[0].axis('off')\n\n\n\n    axes[1].imshow(saliency_maps[0], cmap='hot')\n\n\n\n    axes[1].set_title('Saliency Map')\n\n\n\n    axes[1].axis('off')\n\n\n\n    plt.show()\n\n\n\n\n\n\n\n# Display saliency map for a sample validation image\n\n\n\ndisplay_saliency_maps(val_images, saliency_maps)","metadata":{"execution":{"iopub.status.busy":"2024-12-03T05:32:57.569912Z","iopub.status.idle":"2024-12-03T05:32:57.570179Z","shell.execute_reply":"2024-12-03T05:32:57.570063Z","shell.execute_reply.started":"2024-12-03T05:32:57.570050Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saliency Maps and Explainability\n\n\n\nGenerate saliency maps using gradient-based methods to visualize model decisions.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n\n\nimport matplotlib.pyplot as plt\n\n\n\nimport numpy as np\n\n\n\n\n\n\n\n# Explainability using saliency maps\n\n\n\ndef compute_saliency_maps(model, images, labels):\n\n\n\n    images = tf.convert_to_tensor(images, dtype=tf.float32)\n\n\n\n    with tf.GradientTape() as tape:\n\n\n\n        tape.watch(images)\n\n\n\n        predictions = model(images)\n\n\n\n        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n\n\n\n    gradients = tape.gradient(loss, images)\n\n\n\n    saliency_maps = tf.reduce_max(tf.abs(gradients), axis=-1)\n\n\n\n    return saliency_maps\n\n\n\n\n\n\n\n# Compute saliency maps for validation images\n\n\n\nval_images_tensor = tf.convert_to_tensor(val_images, dtype=tf.float32)\n\n\n\nval_labels_tensor = tf.convert_to_tensor(val_labels, dtype=tf.int64)\n\n\n\nsaliency_maps = compute_saliency_maps(proto_net.encoder, val_images_tensor, val_labels_tensor)\n\n\n\n\n\n\n\n# Display saliency maps\n\n\n\ndef display_saliency_maps(images, saliency_maps):\n\n\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n\n\n    axes[0].imshow(images[0].astype(np.uint8))\n\n\n\n    axes[0].set_title('Original Image')\n\n\n\n    axes[0].axis('off')\n\n\n\n    axes[1].imshow(saliency_maps[0], cmap='hot')\n\n\n\n    axes[1].set_title('Saliency Map')\n\n\n\n    axes[1].axis('off')\n\n\n\n    plt.show()\n\n\n\n\n\n\n\n# Display saliency map for a sample validation image\n\n\n\ndisplay_saliency_maps(val_images, saliency_maps)","metadata":{"execution":{"iopub.status.busy":"2024-12-03T05:32:57.571629Z","iopub.status.idle":"2024-12-03T05:32:57.571928Z","shell.execute_reply":"2024-12-03T05:32:57.571812Z","shell.execute_reply.started":"2024-12-03T05:32:57.571798Z"},"trusted":true},"outputs":[],"execution_count":null}]}