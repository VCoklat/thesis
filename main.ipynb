{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:15.855257Z","iopub.execute_input":"2024-11-13T03:37:15.856177Z","iopub.status.idle":"2024-11-13T03:37:21.607703Z","shell.execute_reply.started":"2024-11-13T03:37:15.856134Z","shell.execute_reply":"2024-11-13T03:37:21.606712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class HAM10000Dataset(Dataset):\n    def __init__(self, csv_file, img_dirs, transform=None):\n        self.data = pd.read_csv(csv_file)\n        self.img_dirs = img_dirs  # List of directories\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Look up the image name\n        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n        \n        # Search for the image in the directories\n        for img_dir in self.img_dirs:\n            img_path = os.path.join(img_dir, img_name)\n            if os.path.exists(img_path):\n                image = Image.open(img_path).convert('RGB')\n                break\n        else:\n            raise FileNotFoundError(f\"Image {img_name} not found in specified directories.\")\n        \n        # Get the label\n        label = self.data.iloc[idx]['dx']  # Diagnosis column\n        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n        label = label_map[label]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:21.60933Z","iopub.execute_input":"2024-11-13T03:37:21.609762Z","iopub.status.idle":"2024-11-13T03:37:21.619788Z","shell.execute_reply.started":"2024-11-13T03:37:21.60973Z","shell.execute_reply":"2024-11-13T03:37:21.618742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\nmetadata = pd.read_csv(metadata_path)\n\n# Check the number of unique images in metadata\nprint(f\"Total images in metadata: {len(metadata)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:21.620913Z","iopub.execute_input":"2024-11-13T03:37:21.621193Z","iopub.status.idle":"2024-11-13T03:37:21.675607Z","shell.execute_reply.started":"2024-11-13T03:37:21.621164Z","shell.execute_reply":"2024-11-13T03:37:21.674708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split metadata into train and test sets\ntrain_metadata, test_metadata = train_test_split(metadata, test_size=0.99, random_state=42)\n\n# Save split metadata for easier loading\ntrain_metadata.to_csv(\"train_metadata.csv\", index=False)\ntest_metadata.to_csv(\"test_metadata.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:21.67821Z","iopub.execute_input":"2024-11-13T03:37:21.679097Z","iopub.status.idle":"2024-11-13T03:37:21.747117Z","shell.execute_reply.started":"2024-11-13T03:37:21.679046Z","shell.execute_reply":"2024-11-13T03:37:21.746374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directories containing images\nimage_dirs = [\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:21.748099Z","iopub.execute_input":"2024-11-13T03:37:21.748396Z","iopub.status.idle":"2024-11-13T03:37:21.752645Z","shell.execute_reply.started":"2024-11-13T03:37:21.748365Z","shell.execute_reply":"2024-11-13T03:37:21.751629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((64, 64)),  # Resize images to 256x256\n    transforms.ToTensor(),         # Convert to PyTorch tensor\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:21.753872Z","iopub.execute_input":"2024-11-13T03:37:21.754216Z","iopub.status.idle":"2024-11-13T03:37:21.763699Z","shell.execute_reply.started":"2024-11-13T03:37:21.754185Z","shell.execute_reply":"2024-11-13T03:37:21.762723Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Datasets\ntrain_dataset = HAM10000Dataset(csv_file=\"train_metadata.csv\", img_dirs=image_dirs, transform=transform)\ntest_dataset = HAM10000Dataset(csv_file=\"test_metadata.csv\", img_dirs=image_dirs, transform=transform)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n\n# Print dataset sizes\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of testing samples: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:22.155683Z","iopub.execute_input":"2024-11-13T03:37:22.156079Z","iopub.status.idle":"2024-11-13T03:37:22.189694Z","shell.execute_reply.started":"2024-11-13T03:37:22.156041Z","shell.execute_reply":"2024-11-13T03:37:22.188693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize one batch of images\nimages, labels = next(iter(train_loader))\nprint(f\"Image batch shape: {images.shape}\")\nprint(f\"Label batch shape: {labels.shape}\")\n\n# Display first 4 images\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 4, figsize=(12, 4))\nfor i in range(4):\n    axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Denormalize\n    axes[i].set_title(f\"Label: {labels[i].item()}\")\n    axes[i].axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:37:23.030695Z","iopub.execute_input":"2024-11-13T03:37:23.031142Z","iopub.status.idle":"2024-11-13T03:37:23.883755Z","shell.execute_reply.started":"2024-11-13T03:37:23.031103Z","shell.execute_reply":"2024-11-13T03:37:23.882658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_dim):\n        super(SelfAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        batch, channels, height, width = x.size()\n        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(batch, -1, width * height)\n        attention = torch.bmm(proj_query, proj_key)\n        attention = torch.softmax(attention, dim=-1)\n\n        proj_value = self.value_conv(x).view(batch, -1, width * height)\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(batch, channels, height, width)\n        out = self.gamma * out + x\n        return out\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_channels)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, img_channels, img_size=64):\n        super(Generator, self).__init__()\n        \n        self.latent_dim = latent_dim\n        self.img_channels = img_channels\n        self.img_size = img_size\n        self.init_size = img_size // 8  # Downsample by 8 (adjusted for 64x64 output)\n        self.fc = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)\n\n        self.upsample = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            ResidualBlock(128),\n            SelfAttention(128),  # Self-Attention after first upscale\n            \n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            ResidualBlock(64),\n\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            SelfAttention(32),  # Self-Attention in the middle layers\n        )\n\n        self.final_layer = nn.Sequential(\n            nn.Conv2d(32, img_channels, kernel_size=3, stride=1, padding=1),\n            nn.Tanh()  # Normalize output to [-1, 1]\n        )\n\n    def forward(self, z):\n        out = self.fc(z)\n        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n        out = self.upsample(out)\n        img = self.final_layer(out)\n        return img\n\n# Instantiate the generator\nlatent_dim = 100  # Size of latent vector\nimg_channels = 3  # RGB images\nimg_size = 64  # Output image size\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = Generator(latent_dim, img_channels, img_size).to(device)\n\n# Test the generator\nz = torch.randn(4, latent_dim).to(device)  # Random latent vector (batch size = 4)\ngenerated_images = generator(z)\n\nprint(f\"Generated image shape: {generated_images.shape}\")  # Should be [4, 3, 64, 64]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:40:01.296762Z","iopub.execute_input":"2024-11-13T03:40:01.297608Z","iopub.status.idle":"2024-11-13T03:40:02.565467Z","shell.execute_reply.started":"2024-11-13T03:40:01.297566Z","shell.execute_reply":"2024-11-13T03:40:02.564406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_dim):\n        super(SelfAttention, self).__init__()\n        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        batch, channels, height, width = x.size()\n        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(batch, -1, width * height)\n        attention = torch.bmm(proj_query, proj_key)\n        attention = torch.softmax(attention, dim=-1)\n\n        proj_value = self.value_conv(x).view(batch, -1, width * height)\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(batch, channels, height, width)\n        out = self.gamma * out + x\n        return out\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample=True):\n        super(ResidualBlock, self).__init__()\n        self.downsample = downsample\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if downsample else nn.Identity()\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.pool = nn.AvgPool2d(2) if downsample else nn.Identity()\n\n    def forward(self, x):\n        shortcut = self.shortcut(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += shortcut\n        out = self.relu(out)\n        out = self.pool(out)\n        return out\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels, img_size=64):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            ResidualBlock(img_channels, 64, downsample=True),            # 64x64 -> 32x32\n            SelfAttention(64),\n            ResidualBlock(64, 128, downsample=True),           # 32x32 -> 16x16\n            SelfAttention(128),\n            ResidualBlock(128, 256, downsample=True),          # 16x16 -> 8x8\n            ResidualBlock(256, 512, downsample=True)           # 8x8 -> 4x4\n        )\n\n        self.final_layer = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512 * 4 * 4, 1),  # Final score output\n            nn.Sigmoid()  # Outputs probability of being real or fake\n        )\n\n    def forward(self, img):\n        out = self.model(img)\n        out = self.final_layer(out)\n        return out\n\n# Instantiate the discriminator\nimg_channels = 3  # RGB images\nimg_size = 64  # Input image size\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndiscriminator = Discriminator(img_channels, img_size).to(device)\n\n# Test the discriminator\nbatch_size = 4\ntest_images = torch.randn(batch_size, img_channels, img_size, img_size).to(device)  # Fake images batch\noutput = discriminator(test_images)\n\nprint(f\"Discriminator output shape: {output.shape}\")  # Should be [4, 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:41:32.233316Z","iopub.execute_input":"2024-11-13T03:41:32.233766Z","iopub.status.idle":"2024-11-13T03:41:32.46463Z","shell.execute_reply.started":"2024-11-13T03:41:32.233725Z","shell.execute_reply":"2024-11-13T03:41:32.463541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:46:53.871058Z","iopub.execute_input":"2024-11-13T03:46:53.871919Z","iopub.status.idle":"2024-11-13T03:46:53.876521Z","shell.execute_reply.started":"2024-11-13T03:46:53.871864Z","shell.execute_reply":"2024-11-13T03:46:53.87544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import optim\nfrom tqdm import tqdm\n\n#def train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1000, lr=0.0002, beta1=0.5, beta2=0.999):\ndef train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1, lr=0.0002, beta1=0.5, beta2=0.999):\n    generator.to(device)\n    discriminator.to(device)\n\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n    \n    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n\n    for epoch in range(epochs):\n        generator.train()\n        discriminator.train()\n        epoch_loss_G = 0.0\n        epoch_loss_D = 0.0\n\n        for real_images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            batch_size = real_images.size(0)\n            real_images = real_images.to(device)\n\n            valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n            fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n\n            # Train Generator\n            optimizer_G.zero_grad()\n            z = torch.randn(batch_size, latent_dim).to(device)\n            \n            with torch.cuda.amp.autocast():  # Mixed precision training\n                generated_images = generator(z)\n                g_loss = criterion(discriminator(generated_images), valid)\n\n            scaler.scale(g_loss).backward()\n            scaler.step(optimizer_G)\n            scaler.update()\n            epoch_loss_G += g_loss.item()\n\n            # Train Discriminator\n            optimizer_D.zero_grad()\n            with torch.cuda.amp.autocast():\n                real_loss = criterion(discriminator(real_images), valid)\n                fake_loss = criterion(discriminator(generated_images.detach()), fake)\n                d_loss = (real_loss + fake_loss) / 2\n\n            scaler.scale(d_loss).backward()\n            scaler.step(optimizer_D)\n            scaler.update()\n            epoch_loss_D += d_loss.item()\n\n            # Clear cache to reduce memory fragmentation\n            torch.cuda.empty_cache()\n\n        print(f\"Epoch [{epoch+1}/{epochs}] | Generator Loss: {epoch_loss_G:.4f} | Discriminator Loss: {epoch_loss_D:.4f}\")\n\n    print(\"Training completed.\")\n\n\n# Call the train_gan function with the train_loader, generator, and discriminator\ntrain_gan(generator, discriminator, train_loader, latent_dim, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:47:38.58237Z","iopub.execute_input":"2024-11-13T03:47:38.582788Z","iopub.status.idle":"2024-11-13T03:47:38.671075Z","shell.execute_reply.started":"2024-11-13T03:47:38.582751Z","shell.execute_reply":"2024-11-13T03:47:38.67003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Define EfficientNetV2 model for HAM10000 with 7 classes\nclass EfficientNetV2Classifier(nn.Module):\n    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n        super(EfficientNetV2Classifier, self).__init__()\n        self.efficientnet_v2 = models.efficientnet_v2_s(pretrained=True)\n        \n        in_features = self.efficientnet_v2.classifier[1].in_features\n        self.efficientnet_v2.classifier = nn.Sequential(\n            nn.Dropout(p=0.3),\n            nn.Linear(in_features, num_classes)\n        )\n\n    def forward(self, x):\n        return self.efficientnet_v2(x)\n\n# Initialize the model\nmodel_EfficientNetV2 = EfficientNetV2Classifier(num_classes=7)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_EfficientNetV2.parameters(), lr=0.001)\nepochs = 1#20\n\n# Training loop\ndef train_model(model, train_loader, test_loader, criterion, optimizer, epochs=1):#20):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        total_correct = 0\n        \n        for data, labels in train_loader:\n            data, labels = data.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_correct += (predicted == labels).sum().item()\n            print(\"done\")\n        \n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = total_correct / len(train_loader.dataset)\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n        \n        # Validation after each epoch\n        validate_model(model, test_loader)\n\n# Validation loop\ndef validate_model(model, test_loader):\n    model.eval()\n    total_correct = 0\n    total_loss = 0.0\n    \n    with torch.no_grad():\n        for data, labels in test_loader:\n            data, labels = data.to(device), labels.to(device)\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total_correct += (predicted == labels).sum().item()\n    \n    avg_loss = total_loss / len(test_loader)\n    accuracy = total_correct / len(test_loader.dataset)\n    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n\nmodel_EfficientNetV2.to(device)\n\n# Train the model\ntrain_model(model_EfficientNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T03:55:49.314203Z","iopub.execute_input":"2024-11-13T03:55:49.315186Z","iopub.status.idle":"2024-11-13T03:55:49.3272Z","shell.execute_reply.started":"2024-11-13T03:55:49.315144Z","shell.execute_reply":"2024-11-13T03:55:49.326207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define ShuffleNetV2 model for HAM10000 with 7 classes\nclass ShuffleNetV2Classifier(nn.Module):\n    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n        super(ShuffleNetV2Classifier, self).__init__()\n        self.shufflenet_v2 = models.shufflenet_v2_x1_0(pretrained=True)\n        \n        # Modify the last fully connected layer to match the number of classes\n        in_features = self.shufflenet_v2.fc.in_features\n        self.shufflenet_v2.fc = nn.Sequential(\n            nn.Dropout(p=0.3),\n            nn.Linear(in_features, num_classes)\n        )\n\n    def forward(self, x):\n        return self.shufflenet_v2(x)\n\n# Initialize the model\nmodel_ShuffleNetV2 = ShuffleNetV2Classifier(num_classes=7)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_ShuffleNetV2.parameters(), lr=0.001)\nepochs = 1#20\n\n# Training loop\ndef train_model(model, train_loader, test_loader, criterion, optimizer, epochs=1):#20):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        total_correct = 0\n        \n        for data, labels in train_loader:\n            data, labels = data.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_correct += (predicted == labels).sum().item()\n        \n        epoch_loss = running_loss / len(train_loader)\n        epoch_accuracy = total_correct / len(train_loader.dataset)\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n        \n        # Validation after each epoch\n        validate_model(model, test_loader)\n\n# Validation loop\ndef validate_model(model, test_loader):\n    model.eval()\n    total_correct = 0\n    total_loss = 0.0\n    \n    with torch.no_grad():\n        for data, labels in test_loader:\n            data, labels = data.to(device), labels.to(device)\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total_correct += (predicted == labels).sum().item()\n    \n    avg_loss = total_loss / len(test_loader)\n    accuracy = total_correct / len(test_loader.dataset)\n    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n\nmodel_ShuffleNetV2.to(device)\n\n# Train the model\ntrain_model(model_ShuffleNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T04:00:06.452961Z","iopub.execute_input":"2024-11-13T04:00:06.453375Z","iopub.status.idle":"2024-11-13T04:00:06.464457Z","shell.execute_reply.started":"2024-11-13T04:00:06.453324Z","shell.execute_reply":"2024-11-13T04:00:06.463412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, labels, noise_dim=128):\n    noise = torch.randn(batch_size, noise_dim)  # Random noise for generator\n    created_imgs = generator(noise, labels) \n    EfficientNetV2Classifier_labels = model_EfficientNetV2(created_imgs)\n    ShuffleNetV2Classifier_labels = model_ShuffleNetV2(created_imgs)\n    if EfficientNetV2Classifier_labels == labels and ShuffleNetV2Classifier_labels == labels:\n        return created_imgs\n    else:\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T04:00:30.916705Z","iopub.execute_input":"2024-11-13T04:00:30.917234Z","iopub.status.idle":"2024-11-13T04:00:30.924325Z","shell.execute_reply.started":"2024-11-13T04:00:30.917194Z","shell.execute_reply":"2024-11-13T04:00:30.923203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CNNEncoder(nn.Module):\n    def __init__(self, in_channels=3, base_features=64):\n        super(CNNEncoder, self).__init__()\n        \n        # Encoder block 1\n        self.block1 = nn.Sequential(\n            nn.Conv2d(in_channels, base_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)  # Reduces 64x64 -> 32x32\n        )\n        \n        # Encoder block 2\n        self.block2 = nn.Sequential(\n            nn.Conv2d(base_features, base_features * 2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features * 2, base_features * 2, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)  # Reduces 32x32 -> 16x16\n        )\n        \n        # Encoder block 3\n        self.block3 = nn.Sequential(\n            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 4),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features * 4, base_features * 4, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 4),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)  # Reduces 16x16 -> 8x8\n        )\n        \n        # Encoder block 4\n        self.block4 = nn.Sequential(\n            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 8),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_features * 8, base_features * 8, kernel_size=3, padding=1),\n            nn.BatchNorm2d(base_features * 8),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, 2)  # Reduces 8x8 -> 4x4\n        )\n\n    def forward(self, x):\n        # Apply each encoder block to the input\n        x = self.block1(x)  # 64x64 -> 32x32\n        x = self.block2(x)  # 32x32 -> 16x16\n        x = self.block3(x)  # 16x16 -> 8x8\n        x = self.block4(x)  # 8x8 -> 4x4\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T04:10:51.369492Z","iopub.execute_input":"2024-11-13T04:10:51.370187Z","iopub.status.idle":"2024-11-13T04:10:51.385033Z","shell.execute_reply.started":"2024-11-13T04:10:51.370127Z","shell.execute_reply":"2024-11-13T04:10:51.383932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass AttentionModule(nn.Module):\n    def __init__(self, feature_dim, num_heads=4):\n        super(AttentionModule, self).__init__()\n        \n        self.num_heads = num_heads\n        self.head_dim = feature_dim // num_heads\n        \n        # Linear transformations for multi-head attention\n        self.query_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n        self.key_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n        self.value_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n        \n        # Multi-head attention mechanism\n        self.attn_heads = nn.ModuleList(\n            [nn.Sequential(\n                nn.Conv2d(self.head_dim, self.head_dim, kernel_size=1),\n                nn.Softmax(dim=-1)  # Softmax across the spatial dimension\n            ) for _ in range(num_heads)]\n        )\n        \n        # Channel attention to recalibrate feature maps\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(feature_dim, feature_dim // 16, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(feature_dim // 16, feature_dim, kernel_size=1),\n            nn.Sigmoid()\n        )\n        \n        # Spatial attention to emphasize important regions in the spatial dimension\n        self.spatial_attention = nn.Sequential(\n            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n            nn.Sigmoid()\n        )\n        \n        # Final 1x1 conv to combine outputs\n        self.output_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n    \n    def forward(self, features):\n        # Compute query, key, and value maps for multi-head attention\n        queries = self.query_conv(features)  # [B, C, H, W]\n        keys = self.key_conv(features)       # [B, C, H, W]\n        values = self.value_conv(features)   # [B, C, H, W]\n        \n        B, C, H, W = queries.size()\n        queries = queries.view(B, self.num_heads, self.head_dim, H * W)  # [B, heads, head_dim, H*W]\n        keys = keys.view(B, self.num_heads, self.head_dim, H * W)        # [B, heads, head_dim, H*W]\n        values = values.view(B, self.num_heads, self.head_dim, H * W)    # [B, heads, head_dim, H*W]\n        \n        # Multi-head attention\n        attention_outputs = []\n        for i in range(self.num_heads):\n            attn_weights = torch.bmm(queries[:, i], keys[:, i].transpose(1, 2))  # [B, head_dim, head_dim]\n            attn_weights = self.attn_heads[i](attn_weights.view(B, self.head_dim, H, W))  # Apply learned attention map\n            attn_output = torch.bmm(attn_weights.view(B, self.head_dim, H * W), values[:, i])  # [B, head_dim, H*W]\n            attention_outputs.append(attn_output.view(B, self.head_dim, H, W))\n        \n        # Concatenate all attention head outputs\n        multi_head_output = torch.cat(attention_outputs, dim=1)  # [B, C, H, W]\n        \n        # Channel Attention\n        channel_attn_weights = self.channel_attention(multi_head_output)\n        channel_attn_output = multi_head_output * channel_attn_weights  # Element-wise multiplication (recalibration)\n        \n        # Spatial Attention\n        avg_pool = torch.mean(channel_attn_output, dim=1, keepdim=True)  # Average pooling across channels\n        max_pool = torch.max(channel_attn_output, dim=1, keepdim=True)[0]  # Max pooling across channels\n        spatial_attn_weights = self.spatial_attention(torch.cat([avg_pool, max_pool], dim=1))\n        spatial_attn_output = channel_attn_output * spatial_attn_weights  # Element-wise multiplication (spatial recalibration)\n        \n        # Final 1x1 conv to produce the final attention output\n        output = self.output_conv(spatial_attn_output)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T04:10:51.652535Z","iopub.execute_input":"2024-11-13T04:10:51.652953Z","iopub.status.idle":"2024-11-13T04:10:51.672149Z","shell.execute_reply.started":"2024-11-13T04:10:51.652911Z","shell.execute_reply":"2024-11-13T04:10:51.671149Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MTUNet2(nn.Module):\n    def __init__(self, in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4):\n        super(MTUNet2, self).__init__()\n        \n        # Complex CNN Encoder shared by both query and support\n        self.encoder = CNNEncoder(in_channels, base_features)\n        \n        # Complex Attention mechanism\n        self.attn_module = AttentionModule(feature_dim, num_heads=num_heads)\n        \n        # Classification Decoder\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(base_features*16*8*8, 1024),  # Updated linear layer input size for complex encoder\n            nn.ReLU(),\n            nn.Linear(1024, num_classes)\n        )\n    \n    def forward(self, query, support):\n        # Step 1: Extract features from the query image using the updated CNNEncoder\n        query_features = self.encoder(query)  # Query features are [B, 1024, 8, 8] based on complex CNNEncoder\n        \n        # Step 2: Extract and aggregate features from the support set\n        N = support.size(0)  # Number of support images\n        support_features = []\n        for i in range(N):\n            support_feature = self.encoder(support[i].unsqueeze(0))  # Each support image's features\n            support_features.append(support_feature)\n        \n        # Aggregate support features (using average pooling for simplicity)\n        support_features = torch.mean(torch.stack(support_features), dim=0)  # [B, 1024, 8, 8]\n        \n        # Step 3: Apply complex attention to both query and support features\n        query_attn = self.attn_module(query_features)  # Attention on query\n        support_attn = self.attn_module(support_features)  # Attention on support\n        \n        # Step 4: Combine query and support features via one-to-one concatenation\n        combined_features = torch.cat((query_attn, support_attn), dim=1)  # Concatenate along the channel dimension\n        # Combined features will be [B, 1024 + 1024 = 2048, 8, 8]\n        \n        # Step 5: Classification Decoder (use the combined query-support features)\n        classification_output = self.classifier(combined_features)\n        \n        return classification_output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T04:10:52.046315Z","iopub.execute_input":"2024-11-13T04:10:52.047044Z","iopub.status.idle":"2024-11-13T04:10:52.058331Z","shell.execute_reply.started":"2024-11-13T04:10:52.047Z","shell.execute_reply":"2024-11-13T04:10:52.057386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Initialize the model, loss function, and optimizer\nmodel = MTUNet2(in_channels=3, base_features=64, num_classes=5)\ncriterion_cls = nn.CrossEntropyLoss()  # For classification output\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training function\ndef train(model, train_loader, criterion_cls, optimizer, epoch):\n    model.train()\n    running_loss = 0.0\n    \n    for data, target in enumerate(train_loader):\n        \n        # Clear gradients\n        optimizer.zero_grad()\n\n        # Creating support set\n        support = create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, target, noise_dim=128)\n\n        # Forward pass\n        classification_output = model(data, support)  # Assuming same data for support set in FSL\n        \n        # Compute loss\n        loss_cls = criterion_cls(classification_output, target)  # Assuming target is for classification\n        \n        # Backward pass\n        loss_cls.backward()\n        optimizer.step()\n\n        # Accumulate the running loss\n        running_loss += loss_cls.item()\n\n        # Compute accuracy for classification output\n        _, predicted = torch.max(classification_output.data, 1)\n        total += target.size(0)\n        correct_cls += (predicted == target).sum().item()\n\n    accuracy = 100 * correct_cls / total\n    \n    return running_loss / len(train_loader), accuracy\n\n\n# Evaluation function\ndef evaluate(model, test_loader, criterion_cls):\n    model.eval()\n    test_loss = 0.0\n    correct_cls = 0\n    total = 0\n\n    with torch.no_grad():\n        for data, target in test_loader:\n\n            # Forward pass\n            classification_output = model(data)\n            \n            # Compute loss\n            loss_cls = criterion_cls(classification_output, target)\n            \n            test_loss += loss_cls.item()\n\n            # Compute accuracy for classification output\n            _, predicted = torch.max(classification_output.data, 1)\n            total += target.size(0)\n            correct_cls += (predicted == target).sum().item()\n\n    accuracy = 100 * correct_cls / total\n    avg_loss = test_loss / len(test_loader)\n    \n    return avg_loss, accuracy\n\n\n# Main training loop\nnum_epochs = 1#500\nfor epoch in range(1, num_epochs + 1):\n    train_loss, train_accuracy = train(model, train_loader, criterion_cls, optimizer, epoch)\n    print(f'Epoch [{epoch}], Training Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n\n    test_loss, test_accuracy = evaluate(model, test_loader, criterion_cls)\n    print(f'Epoch [{epoch}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T04:16:30.480196Z","iopub.execute_input":"2024-11-13T04:16:30.480666Z","iopub.status.idle":"2024-11-13T04:16:30.551491Z","shell.execute_reply.started":"2024-11-13T04:16:30.480627Z","shell.execute_reply":"2024-11-13T04:16:30.550424Z"}},"outputs":[],"execution_count":null}]}