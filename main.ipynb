{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:15.856177Z",
     "iopub.status.busy": "2024-11-13T03:37:15.855257Z",
     "iopub.status.idle": "2024-11-13T03:37:21.607703Z",
     "shell.execute_reply": "2024-11-13T03:37:21.606712Z",
     "shell.execute_reply.started": "2024-11-13T03:37:15.856134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet imports several essential libraries and modules that are commonly used in data science and machine learning projects.\n",
    "\n",
    "1. **os**: This module provides a way of using operating system-dependent functionality like reading or writing to the file system. It is useful for handling file paths and directories.\n",
    "\n",
    "2. **pandas as pd**: Pandas is a powerful data manipulation and analysis library for Python. It provides data structures like DataFrames, which are essential for handling and analyzing structured data. The alias \n",
    "\n",
    "pd\n",
    "\n",
    " is a common convention to simplify the usage of the library.\n",
    "\n",
    "3. **train_test_split from sklearn.model_selection**: This function from the Scikit-learn library is used to split datasets into training and testing sets. It is crucial for evaluating the performance of machine learning models by training them on one subset of the data and testing them on another.\n",
    "\n",
    "4. **transforms and datasets from torchvision**: These modules are part of the Torchvision library, which is used in conjunction with PyTorch for computer vision tasks. \n",
    "\n",
    "transforms\n",
    "\n",
    " provides common image transformations for data augmentation and preprocessing, while \n",
    "\n",
    "datasets\n",
    "\n",
    " offers access to popular datasets like CIFAR-10 and ImageNet.\n",
    "\n",
    "5. **Dataset and DataLoader from torch.utils.data**: These classes are part of PyTorch's data loading utilities. \n",
    "\n",
    "Dataset\n",
    "\n",
    " is an abstract class representing a dataset, and \n",
    "\n",
    "DataLoader\n",
    "\n",
    " is used to load data in batches, shuffle it, and handle multiprocessing for efficient data loading.\n",
    "\n",
    "6. **Image from PIL**: The Python Imaging Library (PIL) is used for opening, manipulating, and saving many different image file formats. The \n",
    "\n",
    "Image\n",
    "\n",
    " class is specifically used to work with image data, which is often necessary in computer vision tasks.\n",
    "\n",
    "Together, these imports set up the environment for a machine learning workflow that involves data manipulation, dataset splitting, image processing, and efficient data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.609762Z",
     "iopub.status.busy": "2024-11-13T03:37:21.60933Z",
     "iopub.status.idle": "2024-11-13T03:37:21.619788Z",
     "shell.execute_reply": "2024-11-13T03:37:21.618742Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.60973Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dirs, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dirs = img_dirs  # List of directories\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Look up the image name\n",
    "        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n",
    "        \n",
    "        # Search for the image in the directories\n",
    "        for img_dir in self.img_dirs:\n",
    "            img_path = os.path.join(img_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Image {img_name} not found in specified directories.\")\n",
    "        \n",
    "        # Get the label\n",
    "        label = self.data.iloc[idx]['dx']  # Diagnosis column\n",
    "        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n",
    "        label = label_map[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines a custom dataset class named \n",
    "\n",
    "HAM10000Dataset\n",
    "\n",
    " that inherits from PyTorch's \n",
    "\n",
    "Dataset\n",
    "\n",
    " class. This class is designed to handle the HAM10000 dataset, which contains images of skin lesions along with their corresponding diagnoses.\n",
    "\n",
    "1. **Initialization (\n",
    "\n",
    "__init__\n",
    "\n",
    " method)**: The constructor takes three parameters: \n",
    "\n",
    "csv_file\n",
    "\n",
    ", \n",
    "\n",
    "img_dirs\n",
    "\n",
    ", and \n",
    "\n",
    "transform\n",
    "\n",
    ". The \n",
    "\n",
    "csv_file\n",
    "\n",
    " parameter is the path to a CSV file containing metadata about the images, such as their filenames and diagnoses. The \n",
    "\n",
    "img_dirs\n",
    "\n",
    " parameter is a list of directories where the images are stored. The \n",
    "\n",
    "transform\n",
    "\n",
    " parameter is optional and can be used to apply transformations to the images (e.g., data augmentation). The constructor reads the CSV file into a pandas DataFrame and stores the image directories and transform function as instance variables.\n",
    "\n",
    "2. **Length (\n",
    "\n",
    "__len__\n",
    "\n",
    " method)**: This method returns the number of samples in the dataset by returning the length of the DataFrame. This is a required method for PyTorch datasets, as it allows PyTorch to know how many samples are available.\n",
    "\n",
    "3. **Get Item (\n",
    "\n",
    "__getitem__\n",
    "\n",
    " method)**: This method retrieves a single sample from the dataset. It takes an index \n",
    "\n",
    "idx\n",
    "\n",
    " as input and performs the following steps:\n",
    "   - It constructs the image filename by appending '.jpg' to the `image_id` from the DataFrame.\n",
    "   - It searches for the image file in the specified directories. If the image is found, it is opened and converted to RGB format. If the image is not found in any directory, a \n",
    "\n",
    "FileNotFoundError\n",
    "\n",
    " is raised.\n",
    "   - It retrieves the label (diagnosis) for the image from the DataFrame. The labels are mapped to numerical values using a dictionary that assigns a unique index to each unique diagnosis.\n",
    "   - If a transform function is provided, it is applied to the image.\n",
    "   - The method returns a tuple containing the image and its corresponding label.\n",
    "\n",
    "This custom dataset class allows for efficient loading and preprocessing of the HAM10000 dataset, making it suitable for training machine learning models in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.621193Z",
     "iopub.status.busy": "2024-11-13T03:37:21.620913Z",
     "iopub.status.idle": "2024-11-13T03:37:21.675607Z",
     "shell.execute_reply": "2024-11-13T03:37:21.674708Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.621164Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Check the number of unique images in metadata\n",
    "print(f\"Total images in metadata: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for loading and inspecting metadata for the HAM10000 dataset, which is a collection of skin lesion images and their associated information.\n",
    "\n",
    "1. **Setting the Metadata Path**: The variable \n",
    "\n",
    "metadata_path\n",
    "\n",
    " is assigned the file path to the CSV file containing the metadata for the HAM10000 dataset. This path points to a file named `HAM10000_metadata.csv` located in the directory `../input/skin-cancer-mnist-ham10000/`.\n",
    "\n",
    "2. **Loading the Metadata**: The \n",
    "\n",
    "pd.read_csv(metadata_path)\n",
    "\n",
    " function is used to read the CSV file into a pandas DataFrame named \n",
    "\n",
    "metadata\n",
    "\n",
    ". This DataFrame will contain various columns with information about each image, such as the image ID, diagnosis, and other relevant details.\n",
    "\n",
    "3. **Checking the Number of Unique Images**: The \n",
    "\n",
    "print\n",
    "\n",
    " statement outputs the total number of images listed in the metadata. The \n",
    "\n",
    "len(metadata)\n",
    "\n",
    " function returns the number of rows in the DataFrame, which corresponds to the number of unique images described in the metadata file.\n",
    "\n",
    "This code is essential for verifying that the metadata has been loaded correctly and for understanding the size of the dataset, which is a crucial step before proceeding with further data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.679097Z",
     "iopub.status.busy": "2024-11-13T03:37:21.67821Z",
     "iopub.status.idle": "2024-11-13T03:37:21.747117Z",
     "shell.execute_reply": "2024-11-13T03:37:21.746374Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.679046Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split metadata into train and test sets\n",
    "train_metadata, test_metadata = train_test_split(metadata, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save split metadata for easier loading\n",
    "train_metadata.to_csv(\"train_metadata.csv\", index=False)\n",
    "test_metadata.to_csv(\"test_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is responsible for splitting the metadata of the HAM10000 dataset into training and testing sets and then saving these subsets to CSV files for easier future access.\n",
    "\n",
    "1. **Splitting the Metadata**: The \n",
    "\n",
    "train_test_split\n",
    "\n",
    " function from Scikit-learn is used to divide the \n",
    "\n",
    "metadata\n",
    "\n",
    " DataFrame into two subsets: \n",
    "\n",
    "train_metadata\n",
    "\n",
    " and \n",
    "\n",
    "test_metadata\n",
    "\n",
    ". The \n",
    "\n",
    "test_size=0.2\n",
    "\n",
    " parameter specifies that 20% of the data should be allocated to the test set, while the remaining 80% will be used for training. The \n",
    "\n",
    "random_state=42\n",
    "\n",
    " parameter ensures that the split is reproducible, meaning that the same split will be obtained each time the code is run with this seed value.\n",
    "\n",
    "2. **Saving the Split Metadata**: The \n",
    "\n",
    "to_csv\n",
    "\n",
    " method of the pandas DataFrame is used to save the training and testing metadata to CSV files named \n",
    "\n",
    "train_metadata.csv\n",
    "\n",
    " and \n",
    "\n",
    "test_metadata.csv\n",
    "\n",
    ", respectively. The \n",
    "\n",
    "index=False\n",
    "\n",
    " parameter ensures that the row indices are not included in the saved CSV files, keeping the files clean and focused on the actual data.\n",
    "\n",
    "By splitting the metadata into training and testing sets, this code prepares the dataset for model training and evaluation. Saving these subsets to CSV files allows for quick and easy loading in future steps of the workflow, ensuring that the same data split is used consistently throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.748396Z",
     "iopub.status.busy": "2024-11-13T03:37:21.748099Z",
     "iopub.status.idle": "2024-11-13T03:37:21.752645Z",
     "shell.execute_reply": "2024-11-13T03:37:21.751629Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.748365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Directories containing images\n",
    "image_dirs = [\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n",
    "    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a list of directories where the images for the HAM10000 dataset are stored.\n",
    "\n",
    "1. **Defining Image Directories**: The variable \n",
    "\n",
    "image_dirs\n",
    "\n",
    " is assigned a list containing two directory paths. These directories, `HAM10000_images_part_1` and `HAM10000_images_part_2`, are located within the `../input/skin-cancer-mnist-ham10000/` directory. Each directory contains a portion of the image files associated with the HAM10000 dataset.\n",
    "\n",
    "2. **Purpose of Image Directories**: By specifying these directories, the code sets up the locations from which the images will be loaded. This is crucial for any subsequent steps that involve accessing and processing the images, such as loading them into a dataset class, applying transformations, or feeding them into a machine learning model.\n",
    "\n",
    "This setup ensures that the code knows exactly where to find the image files, facilitating efficient data loading and management throughout the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:21.754216Z",
     "iopub.status.busy": "2024-11-13T03:37:21.753872Z",
     "iopub.status.idle": "2024-11-13T03:37:21.763699Z",
     "shell.execute_reply": "2024-11-13T03:37:21.762723Z",
     "shell.execute_reply.started": "2024-11-13T03:37:21.754185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64\n",
    "    transforms.ToTensor(),         # Convert to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines a series of image transformations that will be applied to the images in the dataset. These transformations are composed using the \n",
    "\n",
    "transforms.Compose\n",
    "\n",
    " function from the Torchvision library, which allows multiple transformations to be chained together.\n",
    "\n",
    "1. **Resizing Images**: The \n",
    "\n",
    "transforms.Resize((64, 64))\n",
    "\n",
    " transformation resizes each image to a fixed size of 64x64 pixels. This ensures that all images have the same dimensions, which is necessary for consistent input to a neural network. The comment incorrectly states \"Resize images to 256x256\"; it should be corrected to \"Resize images to 64x64\".\n",
    "\n",
    "2. **Converting to Tensor**: The \n",
    "\n",
    "transforms.ToTensor()\n",
    "\n",
    " transformation converts the image from a PIL Image or NumPy array to a PyTorch tensor. This conversion is essential because PyTorch models require input data to be in tensor format.\n",
    "\n",
    "3. **Normalizing**: The \n",
    "\n",
    "transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "\n",
    " transformation normalizes the image tensor. Each channel (Red, Green, Blue) is normalized by subtracting the mean value of 0.5 and dividing by the standard deviation of 0.5. This normalization scales the pixel values to the range [-1, 1], which can help improve the convergence of neural network training.\n",
    "\n",
    "By defining these transformations, the code ensures that all images are preprocessed in a consistent manner before being fed into a machine learning model. This preprocessing step is crucial for achieving good model performance and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:22.156079Z",
     "iopub.status.busy": "2024-11-13T03:37:22.155683Z",
     "iopub.status.idle": "2024-11-13T03:37:22.189694Z",
     "shell.execute_reply": "2024-11-13T03:37:22.188693Z",
     "shell.execute_reply.started": "2024-11-13T03:37:22.156041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = HAM10000Dataset(csv_file=\"train_metadata.csv\", img_dirs=image_dirs, transform=transform)\n",
    "test_dataset = HAM10000Dataset(csv_file=\"test_metadata.csv\", img_dirs=image_dirs, transform=transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet sets up the datasets and data loaders for training and testing a machine learning model using the HAM10000 dataset.\n",
    "\n",
    "1. **Creating Datasets**: \n",
    "   - \n",
    "\n",
    "train_dataset\n",
    "\n",
    " and \n",
    "\n",
    "test_dataset\n",
    "\n",
    " are instances of the \n",
    "\n",
    "HAM10000Dataset\n",
    "\n",
    " class, which is a custom dataset class designed to handle the HAM10000 dataset. \n",
    "   - The \n",
    "\n",
    "train_dataset\n",
    "\n",
    " is created using the metadata from `train_metadata.csv`, while the \n",
    "\n",
    "test_dataset\n",
    "\n",
    " is created using the metadata from `test_metadata.csv`. \n",
    "   - Both datasets use the same list of image directories (\n",
    "\n",
    "image_dirs\n",
    "\n",
    ") and the same set of transformations (\n",
    "\n",
    "transform\n",
    "\n",
    ") defined earlier. These transformations include resizing the images, converting them to tensors, and normalizing them.\n",
    "\n",
    "2. **Creating DataLoaders**: \n",
    "   - \n",
    "\n",
    "train_loader\n",
    "\n",
    " and \n",
    "\n",
    "test_loader\n",
    "\n",
    " are instances of the \n",
    "\n",
    "DataLoader\n",
    "\n",
    " class from PyTorch, which provides an efficient way to load data in batches.\n",
    "   - \n",
    "\n",
    "train_loader\n",
    "\n",
    " is created using the \n",
    "\n",
    "train_dataset\n",
    "\n",
    " and is configured with a batch size of 4, shuffling enabled (\n",
    "\n",
    "shuffle=True\n",
    "\n",
    "), and 2 worker threads (\n",
    "\n",
    "num_workers=2\n",
    "\n",
    ") for loading data in parallel. Shuffling the training data helps to ensure that the model does not learn the order of the data, which can improve generalization.\n",
    "   - \n",
    "\n",
    "test_loader\n",
    "\n",
    " is created using the \n",
    "\n",
    "test_dataset\n",
    "\n",
    " with the same batch size and number of worker threads, but shuffling is disabled (\n",
    "\n",
    "shuffle=False\n",
    "\n",
    "). This is because the order of the test data does not need to be randomized.\n",
    "\n",
    "3. **Printing Dataset Sizes**: Although the code to print the dataset sizes is not included in the snippet, it is implied that the sizes of the training and testing datasets will be printed. This is useful for verifying that the datasets have been loaded correctly and contain the expected number of samples.\n",
    "\n",
    "Overall, this setup prepares the data for training and evaluating a machine learning model by organizing it into manageable batches and ensuring consistent preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:37:23.031142Z",
     "iopub.status.busy": "2024-11-13T03:37:23.030695Z",
     "iopub.status.idle": "2024-11-13T03:37:23.883755Z",
     "shell.execute_reply": "2024-11-13T03:37:23.882658Z",
     "shell.execute_reply.started": "2024-11-13T03:37:23.031103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Visualize one batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Image batch shape: {images.shape}\")\n",
    "print(f\"Label batch shape: {labels.shape}\")\n",
    "\n",
    "# Display first 4 images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Denormalize\n",
    "    axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is designed to visualize a batch of images from the training dataset, which helps in understanding the data and verifying that the preprocessing steps have been applied correctly.\n",
    "\n",
    "1. **Loading a Batch of Images**: \n",
    "   - The line \n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    " retrieves the next batch of images and their corresponding labels from the \n",
    "\n",
    "train_loader\n",
    "\n",
    ". This batch is stored in the variables \n",
    "\n",
    "images\n",
    "\n",
    " and \n",
    "\n",
    "labels\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "print\n",
    "\n",
    " statements output the shapes of the image and label batches. This is useful for confirming that the batch size and image dimensions are as expected. The shape of \n",
    "\n",
    "images\n",
    "\n",
    " should be `(batch_size, channels, height, width)`, and the shape of \n",
    "\n",
    "labels\n",
    "\n",
    " should be `(batch_size,)`.\n",
    "\n",
    "2. **Importing Matplotlib**: \n",
    "   - The \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " statement imports the Matplotlib library, which is used for plotting and visualizing data.\n",
    "\n",
    "3. **Creating a Plot**: \n",
    "   - The \n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n",
    "\n",
    " line creates a figure with a 1x4 grid of subplots, each with a size of 12x4 inches. This layout is used to display the first four images in the batch.\n",
    "   - The `for` loop iterates over the first four images in the batch. For each image:\n",
    "     - \n",
    "\n",
    "axes[i].imshow(images[i].permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "\n",
    " displays the image. The \n",
    "\n",
    "permute(1, 2, 0)\n",
    "\n",
    " method rearranges the dimensions of the image tensor from (channels, height, width) to (height, width, channels), which is the format expected by Matplotlib. The \n",
    "\n",
    "numpy()\n",
    "\n",
    " method converts the tensor to a NumPy array, and the multiplication and addition operations denormalize the image (reversing the normalization applied during preprocessing).\n",
    "     - \n",
    "\n",
    "axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "\n",
    " sets the title of the subplot to the label of the image.\n",
    "     - \n",
    "\n",
    "axes[i].axis(\"off\")\n",
    "\n",
    " removes the axis ticks and labels for a cleaner display.\n",
    "\n",
    "4. **Displaying the Plot**: \n",
    "   - The \n",
    "\n",
    "plt.show()\n",
    "\n",
    " command renders the plot and displays the images.\n",
    "\n",
    "This visualization step is crucial for ensuring that the images are being loaded and preprocessed correctly, and it provides a quick way to inspect the data visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:40:01.297608Z",
     "iopub.status.busy": "2024-11-13T03:40:01.296762Z",
     "iopub.status.idle": "2024-11-13T03:40:02.565467Z",
     "shell.execute_reply": "2024-11-13T03:40:02.564406Z",
     "shell.execute_reply.started": "2024-11-13T03:40:01.297566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch, -1, width * height)\n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        proj_value = self.value_conv(x).view(batch, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels, img_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_channels = img_channels\n",
    "        self.img_size = img_size\n",
    "        self.init_size = img_size // 8  # Downsample by 8 (adjusted for 64x64 output)\n",
    "        self.fc = nn.Linear(latent_dim, 128 * self.init_size * self.init_size)\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128),\n",
    "            SelfAttention(128),  # Self-Attention after first upscale\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(64),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttention(32),  # Self-Attention in the middle layers\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Conv2d(32, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Normalize output to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc(z)\n",
    "        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n",
    "        out = self.upsample(out)\n",
    "        img = self.final_layer(out)\n",
    "        return img\n",
    "\n",
    "# Instantiate the generator\n",
    "latent_dim = 100  # Size of latent vector\n",
    "img_channels = 3  # RGB images\n",
    "img_size = 64  # Output image size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator(latent_dim, img_channels, img_size).to(device)\n",
    "\n",
    "# Test the generator\n",
    "z = torch.randn(4, latent_dim).to(device)  # Random latent vector (batch size = 4)\n",
    "generated_images = generator(z)\n",
    "\n",
    "print(f\"Generated image shape: {generated_images.shape}\")  # Should be [4, 3, 64, 64]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines and tests a Generative Adversarial Network (GAN) generator model that includes self-attention and residual blocks to enhance image generation quality.\n",
    "\n",
    "1. **Imports**: The code begins by importing the necessary PyTorch modules, including \n",
    "\n",
    "torch\n",
    "\n",
    " and \n",
    "\n",
    "torch.nn\n",
    "\n",
    ".\n",
    "\n",
    "2. **Self-Attention Class**: \n",
    "   - The \n",
    "\n",
    "SelfAttention\n",
    "\n",
    " class is defined to implement a self-attention mechanism. This mechanism allows the model to focus on different parts of the image, improving the generation of fine details.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes convolutional layers for query, key, and value projections, and a learnable parameter \n",
    "\n",
    "gamma\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method computes the attention map and applies it to the input feature map, enhancing the representation.\n",
    "\n",
    "3. **ResidualBlock Class**: \n",
    "   - The \n",
    "\n",
    "ResidualBlock\n",
    "\n",
    " class implements a residual block, which helps in training deep networks by allowing gradients to flow through skip connections.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method sets up two convolutional layers with batch normalization and ReLU activation.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method adds the input to the output of the block, creating a residual connection.\n",
    "\n",
    "4. **Generator Class**: \n",
    "   - The \n",
    "\n",
    "Generator\n",
    "\n",
    " class defines the architecture of the GAN generator.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes the generator with a fully connected layer to project the latent vector, followed by a series of upsampling layers, residual blocks, and self-attention layers.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method processes the latent vector through these layers to generate an image.\n",
    "   - The \n",
    "\n",
    "upsample\n",
    "\n",
    " sequence includes batch normalization, transposed convolutions for upsampling, residual blocks, and self-attention layers to refine the generated images.\n",
    "   - The \n",
    "\n",
    "final_layer\n",
    "\n",
    " normalizes the output to the range [-1, 1] using a Tanh activation function.\n",
    "\n",
    "5. **Instantiating and Testing the Generator**: \n",
    "   - The generator is instantiated with a latent dimension of 100, 3 image channels (for RGB images), and an output image size of 64x64 pixels.\n",
    "   - The generator is moved to the appropriate device (GPU if available, otherwise CPU).\n",
    "   - A random latent vector \n",
    "\n",
    "z\n",
    "\n",
    " is generated, and the generator produces a batch of images.\n",
    "   - The shape of the generated images is printed to verify that it matches the expected dimensions `[4, 3, 64, 64]`.\n",
    "\n",
    "This code sets up a sophisticated GAN generator that leverages self-attention and residual connections to produce high-quality images, and it includes a test to ensure the generator works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:41:32.233766Z",
     "iopub.status.busy": "2024-11-13T03:41:32.233316Z",
     "iopub.status.idle": "2024-11-13T03:41:32.46463Z",
     "shell.execute_reply": "2024-11-13T03:41:32.463541Z",
     "shell.execute_reply.started": "2024-11-13T03:41:32.233725Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channels, height, width = x.size()\n",
    "        proj_query = self.query_conv(x).view(batch, -1, width * height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(batch, -1, width * height)\n",
    "        attention = torch.bmm(proj_query, proj_key)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "        proj_value = self.value_conv(x).view(batch, -1, width * height)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        out = self.gamma * out + x\n",
    "        return out\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=True):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1) if downsample else nn.Identity()\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AvgPool2d(2) if downsample else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels, img_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ResidualBlock(img_channels, 64, downsample=True),            # 64x64 -> 32x32\n",
    "            SelfAttention(64),\n",
    "            ResidualBlock(64, 128, downsample=True),           # 32x32 -> 16x16\n",
    "            SelfAttention(128),\n",
    "            ResidualBlock(128, 256, downsample=True),          # 16x16 -> 8x8\n",
    "            ResidualBlock(256, 512, downsample=True)           # 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512 * 4 * 4, 1),  # Final score output\n",
    "            nn.Sigmoid()  # Outputs probability of being real or fake\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = self.final_layer(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate the discriminator\n",
    "img_channels = 3  # RGB images\n",
    "img_size = 64  # Input image size\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "discriminator = Discriminator(img_channels, img_size).to(device)\n",
    "\n",
    "# Test the discriminator\n",
    "batch_size = 4\n",
    "test_images = torch.randn(batch_size, img_channels, img_size, img_size).to(device)  # Fake images batch\n",
    "output = discriminator(test_images)\n",
    "\n",
    "print(f\"Discriminator output shape: {output.shape}\")  # Should be [4, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet defines and tests a Discriminator model for a Generative Adversarial Network (GAN) using PyTorch. The Discriminator is designed to distinguish between real and fake images, incorporating self-attention and residual blocks to enhance its performance.\n",
    "\n",
    "1. **Imports**: The code begins by importing the necessary PyTorch modules, including \n",
    "\n",
    "torch\n",
    "\n",
    " and \n",
    "\n",
    "torch.nn\n",
    "\n",
    ".\n",
    "\n",
    "2. **SelfAttention Class**:\n",
    "   - The \n",
    "\n",
    "SelfAttention\n",
    "\n",
    " class implements a self-attention mechanism, which allows the model to focus on different parts of the image, improving its ability to capture fine details.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes convolutional layers for query, key, and value projections, and a learnable parameter \n",
    "\n",
    "gamma\n",
    "\n",
    ".\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method computes the attention map and applies it to the input feature map, enhancing the representation.\n",
    "\n",
    "3. **ResidualBlock Class**:\n",
    "   - The \n",
    "\n",
    "ResidualBlock\n",
    "\n",
    " class implements a residual block, which helps in training deep networks by allowing gradients to flow through skip connections.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method sets up two convolutional layers with batch normalization and ReLU activation, along with a shortcut connection and optional downsampling.\n",
    "   - The \n",
    "\n",
    "forward\n",
    "\n",
    " method adds the input to the output of the block, creating a residual connection and optionally downsampling the feature map.\n",
    "\n",
    "4. **Discriminator Class**:\n",
    "   - The \n",
    "\n",
    "Discriminator\n",
    "\n",
    " class defines the architecture of the GAN discriminator.\n",
    "   - The \n",
    "\n",
    "__init__\n",
    "\n",
    " method initializes the discriminator with a series of residual blocks and self-attention layers, followed by a final layer that outputs a probability score.\n",
    "   - The \n",
    "\n",
    "model\n",
    "\n",
    " sequence includes residual blocks with downsampling, self-attention layers, and batch normalization to refine the feature maps.\n",
    "   - The \n",
    "\n",
    "final_layer\n",
    "\n",
    " flattens the feature maps and applies a linear layer followed by a sigmoid activation to output the probability of the image being real or fake.\n",
    "\n",
    "5. **Instantiating and Testing the Discriminator**:\n",
    "   - The discriminator is instantiated with 3 image channels (for RGB images) and an input image size of 64x64 pixels.\n",
    "   - The discriminator is moved to the appropriate device (GPU if available, otherwise CPU).\n",
    "   - A batch of random fake images is generated, and the discriminator produces an output for these images.\n",
    "   - The shape of the discriminator's output is printed to verify that it matches the expected dimensions `[4, 1]`, indicating the batch size and the probability score for each image.\n",
    "\n",
    "This code sets up a sophisticated GAN discriminator that leverages self-attention and residual connections to effectively distinguish between real and fake images, and it includes a test to ensure the discriminator works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:46:53.871919Z",
     "iopub.status.busy": "2024-11-13T03:46:53.871058Z",
     "iopub.status.idle": "2024-11-13T03:46:53.876521Z",
     "shell.execute_reply": "2024-11-13T03:46:53.87544Z",
     "shell.execute_reply.started": "2024-11-13T03:46:53.871864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code snippet is used to manage warning messages in Python.\n",
    "\n",
    "1. **Importing the Warnings Module**: The \n",
    "\n",
    "warnings\n",
    "\n",
    " module is imported, which is a built-in Python module used to handle warning messages. Warnings are typically issued to alert the user about potential issues in the code that do not necessarily stop the execution but might lead to unexpected behavior.\n",
    "\n",
    "2. **Filtering Warnings**: The \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    " function call is used to suppress all warning messages. By setting the filter to \"ignore\", the code instructs Python to ignore any warnings that would normally be printed to the console. This can be useful in scenarios where the developer is aware of certain non-critical warnings and wants to prevent them from cluttering the output.\n",
    "\n",
    "While suppressing warnings can make the output cleaner and easier to read, it is important to use this approach judiciously. Ignoring warnings without understanding their cause can sometimes hide underlying issues that might affect the program's correctness or performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:47:38.582788Z",
     "iopub.status.busy": "2024-11-13T03:47:38.58237Z",
     "iopub.status.idle": "2024-11-13T03:47:38.671075Z",
     "shell.execute_reply": "2024-11-13T03:47:38.67003Z",
     "shell.execute_reply.started": "2024-11-13T03:47:38.582751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_gan(generator, discriminator, train_loader, latent_dim, device, epochs=1000, lr=0.0002, beta1=0.5, beta2=0.999):\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        epoch_loss_G = 0.0\n",
    "        epoch_loss_D = 0.0\n",
    "\n",
    "        for real_images, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = real_images.to(device)\n",
    "\n",
    "            valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n",
    "            fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():  # Mixed precision training\n",
    "                generated_images = generator(z)\n",
    "                g_loss = criterion(discriminator(generated_images), valid)\n",
    "\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(optimizer_G)\n",
    "            scaler.update()\n",
    "            epoch_loss_G += g_loss.item()\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                real_loss = criterion(discriminator(real_images), valid)\n",
    "                fake_loss = criterion(discriminator(generated_images.detach()), fake)\n",
    "                d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(optimizer_D)\n",
    "            scaler.update()\n",
    "            epoch_loss_D += d_loss.item()\n",
    "\n",
    "            # Clear cache to reduce memory fragmentation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Generator Loss: {epoch_loss_G:.4f} | Discriminator Loss: {epoch_loss_D:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "\n",
    "# Call the train_gan function with the train_loader, generator, and discriminator\n",
    "train_gan(generator, discriminator, train_loader, latent_dim, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T03:55:49.315186Z",
     "iopub.status.busy": "2024-11-13T03:55:49.314203Z",
     "iopub.status.idle": "2024-11-13T03:55:49.3272Z",
     "shell.execute_reply": "2024-11-13T03:55:49.326207Z",
     "shell.execute_reply.started": "2024-11-13T03:55:49.315144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define EfficientNetV2 model for HAM10000 with 7 classes\n",
    "class EfficientNetV2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n",
    "        super(EfficientNetV2Classifier, self).__init__()\n",
    "        self.efficientnet_v2 = models.efficientnet_v2_s(pretrained=True)\n",
    "        \n",
    "        in_features = self.efficientnet_v2.classifier[1].in_features\n",
    "        self.efficientnet_v2.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet_v2(x)\n",
    "\n",
    "# Initialize the model\n",
    "model_EfficientNetV2 = EfficientNetV2Classifier(num_classes=7)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_EfficientNetV2.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            print(\"done\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = total_correct / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validation after each epoch\n",
    "        validate_model(model, test_loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / len(test_loader.dataset)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "model_EfficientNetV2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model_EfficientNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:00:06.453375Z",
     "iopub.status.busy": "2024-11-13T04:00:06.452961Z",
     "iopub.status.idle": "2024-11-13T04:00:06.464457Z",
     "shell.execute_reply": "2024-11-13T04:00:06.463412Z",
     "shell.execute_reply.started": "2024-11-13T04:00:06.453324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define ShuffleNetV2 model for HAM10000 with 7 classes\n",
    "class ShuffleNetV2Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # 7 classes for HAM10000\n",
    "        super(ShuffleNetV2Classifier, self).__init__()\n",
    "        self.shufflenet_v2 = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "        \n",
    "        # Modify the last fully connected layer to match the number of classes\n",
    "        in_features = self.shufflenet_v2.fc.in_features\n",
    "        self.shufflenet_v2.fc = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.shufflenet_v2(x)\n",
    "\n",
    "# Initialize the model\n",
    "model_ShuffleNetV2 = ShuffleNetV2Classifier(num_classes=7)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ShuffleNetV2.parameters(), lr=0.001)\n",
    "epochs = 20\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        \n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = total_correct / len(train_loader.dataset)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "        \n",
    "        # Validation after each epoch\n",
    "        validate_model(model, test_loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = total_correct / len(test_loader.dataset)\n",
    "    print(f'Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "model_ShuffleNetV2.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_model(model_ShuffleNetV2, train_loader, test_loader, criterion, optimizer, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:00:30.917234Z",
     "iopub.status.busy": "2024-11-13T04:00:30.916705Z",
     "iopub.status.idle": "2024-11-13T04:00:30.924325Z",
     "shell.execute_reply": "2024-11-13T04:00:30.923203Z",
     "shell.execute_reply.started": "2024-11-13T04:00:30.917194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, labels, noise_dim=128):\n",
    "    noise = torch.randn(batch_size, noise_dim)  # Random noise for generator\n",
    "    created_imgs = generator(noise, labels) \n",
    "    EfficientNetV2Classifier_labels = model_EfficientNetV2(created_imgs)\n",
    "    ShuffleNetV2Classifier_labels = model_ShuffleNetV2(created_imgs)\n",
    "    if EfficientNetV2Classifier_labels == labels and ShuffleNetV2Classifier_labels == labels:\n",
    "        return created_imgs\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:51.370187Z",
     "iopub.status.busy": "2024-11-13T04:10:51.369492Z",
     "iopub.status.idle": "2024-11-13T04:10:51.385033Z",
     "shell.execute_reply": "2024-11-13T04:10:51.383932Z",
     "shell.execute_reply.started": "2024-11-13T04:10:51.370127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder block 1\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features, base_features, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 64x64 -> 32x32\n",
    "        )\n",
    "        \n",
    "        # Encoder block 2\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(base_features, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 2, base_features * 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 32x32 -> 16x16\n",
    "        )\n",
    "        \n",
    "        # Encoder block 3\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 4, base_features * 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 16x16 -> 8x8\n",
    "        )\n",
    "        \n",
    "        # Encoder block 4\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_features * 8, base_features * 8, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # Reduces 8x8 -> 4x4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each encoder block to the input\n",
    "        x = self.block1(x)  # 64x64 -> 32x32\n",
    "        x = self.block2(x)  # 32x32 -> 16x16\n",
    "        x = self.block3(x)  # 16x16 -> 8x8\n",
    "        x = self.block4(x)  # 8x8 -> 4x4\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:51.652953Z",
     "iopub.status.busy": "2024-11-13T04:10:51.652535Z",
     "iopub.status.idle": "2024-11-13T04:10:51.672149Z",
     "shell.execute_reply": "2024-11-13T04:10:51.671149Z",
     "shell.execute_reply.started": "2024-11-13T04:10:51.652911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, feature_dim, num_heads=4):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = feature_dim // num_heads\n",
    "        \n",
    "        # Linear transformations for multi-head attention\n",
    "        self.query_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "        \n",
    "        # Multi-head attention mechanism\n",
    "        self.attn_heads = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Conv2d(self.head_dim, self.head_dim, kernel_size=1),\n",
    "                nn.Softmax(dim=-1)  # Softmax across the spatial dimension\n",
    "            ) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        # Channel attention to recalibrate feature maps\n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(feature_dim, feature_dim // 16, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(feature_dim // 16, feature_dim, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Spatial attention to emphasize important regions in the spatial dimension\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Final 1x1 conv to combine outputs\n",
    "        self.output_conv = nn.Conv2d(feature_dim, feature_dim, kernel_size=1)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Compute query, key, and value maps for multi-head attention\n",
    "        queries = self.query_conv(features)  # [B, C, H, W]\n",
    "        keys = self.key_conv(features)       # [B, C, H, W]\n",
    "        values = self.value_conv(features)   # [B, C, H, W]\n",
    "        \n",
    "        B, C, H, W = queries.size()\n",
    "        queries = queries.view(B, self.num_heads, self.head_dim, H * W)  # [B, heads, head_dim, H*W]\n",
    "        keys = keys.view(B, self.num_heads, self.head_dim, H * W)        # [B, heads, head_dim, H*W]\n",
    "        values = values.view(B, self.num_heads, self.head_dim, H * W)    # [B, heads, head_dim, H*W]\n",
    "        \n",
    "        # Multi-head attention\n",
    "        attention_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            attn_weights = torch.bmm(queries[:, i], keys[:, i].transpose(1, 2))  # [B, head_dim, head_dim]\n",
    "            attn_weights = self.attn_heads[i](attn_weights.view(B, self.head_dim, H, W))  # Apply learned attention map\n",
    "            attn_output = torch.bmm(attn_weights.view(B, self.head_dim, H * W), values[:, i])  # [B, head_dim, H*W]\n",
    "            attention_outputs.append(attn_output.view(B, self.head_dim, H, W))\n",
    "        \n",
    "        # Concatenate all attention head outputs\n",
    "        multi_head_output = torch.cat(attention_outputs, dim=1)  # [B, C, H, W]\n",
    "        \n",
    "        # Channel Attention\n",
    "        channel_attn_weights = self.channel_attention(multi_head_output)\n",
    "        channel_attn_output = multi_head_output * channel_attn_weights  # Element-wise multiplication (recalibration)\n",
    "        \n",
    "        # Spatial Attention\n",
    "        avg_pool = torch.mean(channel_attn_output, dim=1, keepdim=True)  # Average pooling across channels\n",
    "        max_pool = torch.max(channel_attn_output, dim=1, keepdim=True)[0]  # Max pooling across channels\n",
    "        spatial_attn_weights = self.spatial_attention(torch.cat([avg_pool, max_pool], dim=1))\n",
    "        spatial_attn_output = channel_attn_output * spatial_attn_weights  # Element-wise multiplication (spatial recalibration)\n",
    "        \n",
    "        # Final 1x1 conv to produce the final attention output\n",
    "        output = self.output_conv(spatial_attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:10:52.047044Z",
     "iopub.status.busy": "2024-11-13T04:10:52.046315Z",
     "iopub.status.idle": "2024-11-13T04:10:52.058331Z",
     "shell.execute_reply": "2024-11-13T04:10:52.057386Z",
     "shell.execute_reply.started": "2024-11-13T04:10:52.047Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MTUNet2(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_features=64, num_classes=5, feature_dim=512, num_heads=4):\n",
    "        super(MTUNet2, self).__init__()\n",
    "        \n",
    "        # Complex CNN Encoder shared by both query and support\n",
    "        self.encoder = CNNEncoder(in_channels, base_features)\n",
    "        \n",
    "        # Complex Attention mechanism\n",
    "        self.attn_module = AttentionModule(feature_dim, num_heads=num_heads)\n",
    "        \n",
    "        # Classification Decoder\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(base_features*16*8*8, 1024),  # Updated linear layer input size for complex encoder\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query, support):\n",
    "        # Step 1: Extract features from the query image using the updated CNNEncoder\n",
    "        query_features = self.encoder(query)  # Query features are [B, 1024, 8, 8] based on complex CNNEncoder\n",
    "        \n",
    "        # Step 2: Extract and aggregate features from the support set\n",
    "        N = support.size(0)  # Number of support images\n",
    "        support_features = []\n",
    "        for i in range(N):\n",
    "            support_feature = self.encoder(support[i].unsqueeze(0))  # Each support image's features\n",
    "            support_features.append(support_feature)\n",
    "        \n",
    "        # Aggregate support features (using average pooling for simplicity)\n",
    "        support_features = torch.mean(torch.stack(support_features), dim=0)  # [B, 1024, 8, 8]\n",
    "        \n",
    "        # Step 3: Apply complex attention to both query and support features\n",
    "        query_attn = self.attn_module(query_features)  # Attention on query\n",
    "        support_attn = self.attn_module(support_features)  # Attention on support\n",
    "        \n",
    "        # Step 4: Combine query and support features via one-to-one concatenation\n",
    "        combined_features = torch.cat((query_attn, support_attn), dim=1)  # Concatenate along the channel dimension\n",
    "        # Combined features will be [B, 1024 + 1024 = 2048, 8, 8]\n",
    "        \n",
    "        # Step 5: Classification Decoder (use the combined query-support features)\n",
    "        classification_output = self.classifier(combined_features)\n",
    "        \n",
    "        return classification_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-13T04:16:30.480666Z",
     "iopub.status.busy": "2024-11-13T04:16:30.480196Z",
     "iopub.status.idle": "2024-11-13T04:16:30.551491Z",
     "shell.execute_reply": "2024-11-13T04:16:30.550424Z",
     "shell.execute_reply.started": "2024-11-13T04:16:30.480627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = MTUNet2(in_channels=3, base_features=64, num_classes=5)\n",
    "criterion_cls = nn.CrossEntropyLoss()  # For classification output\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion_cls, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data, target in enumerate(train_loader):\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Creating support set\n",
    "        support = create_support_set(generator, model_EfficientNetV2, model_ShuffleNetV2, target, noise_dim=128)\n",
    "\n",
    "        # Forward pass\n",
    "        classification_output = model(data, support)  # Assuming same data for support set in FSL\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_cls = criterion_cls(classification_output, target)  # Assuming target is for classification\n",
    "        \n",
    "        # Backward pass\n",
    "        loss_cls.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss_cls.item()\n",
    "\n",
    "        # Compute accuracy for classification output\n",
    "        _, predicted = torch.max(classification_output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct_cls += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_cls / total\n",
    "    \n",
    "    return running_loss / len(train_loader), accuracy\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion_cls):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_cls = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            # Forward pass\n",
    "            classification_output = model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss_cls = criterion_cls(classification_output, target)\n",
    "            \n",
    "            test_loss += loss_cls.item()\n",
    "\n",
    "            # Compute accuracy for classification output\n",
    "            _, predicted = torch.max(classification_output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct_cls += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct_cls / total\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, train_accuracy = train(model, train_loader, criterion_cls, optimizer, epoch)\n",
    "    print(f'Epoch [{epoch}], Training Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, criterion_cls)\n",
    "    print(f'Epoch [{epoch}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 54339,
     "sourceId": 104884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
