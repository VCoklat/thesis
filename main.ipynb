{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install captum torch torchvision pandas pillow matplotlib scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:39:47.287255Z","iopub.execute_input":"2024-12-03T05:39:47.288239Z","iopub.status.idle":"2024-12-03T05:39:57.729130Z","shell.execute_reply.started":"2024-12-03T05:39:47.288180Z","shell.execute_reply":"2024-12-03T05:39:57.728011Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting captum\n  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (10.3.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from captum) (1.26.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from captum) (4.66.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nDownloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: captum\nSuccessfully installed captum-0.7.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom captum.attr import Saliency\n\nclass HAM10000Dataset(Dataset):\n    def __init__(self, csv_file, img_dirs, transform=None):\n        self.data = pd.read_csv(csv_file)\n        self.img_dirs = img_dirs  # List of directories\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # Look up the image name\n        img_name = self.data.iloc[idx]['image_id'] + '.jpg'\n        \n        # Search for the image in the directories\n        for img_dir in self.img_dirs:\n            img_path = os.path.join(img_dir, img_name)\n            if os.path.exists(img_path):\n                image = Image.open(img_path).convert('RGB')\n                break\n        else:\n            raise FileNotFoundError(f\"Image {img_name} not found in specified directories.\")\n        \n        # Get the label\n        label = self.data.iloc[idx]['dx']  # Diagnosis column\n        label_map = {label: idx for idx, label in enumerate(self.data['dx'].unique())}\n        label = label_map[label]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\nmetadata_path = \"../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\"\nmetadata = pd.read_csv(metadata_path)\n\n# Check the number of unique images in metadata\nprint(f\"Total images in metadata: {len(metadata)}\")\n\n# Split metadata into train and test sets\ntrain_metadata, test_metadata = train_test_split(metadata, test_size=0.001, random_state=42)\n\n# Save split metadata for easier loading\ntrain_metadata.to_csv(\"train_metadata.csv\", index=False)\ntest_metadata.to_csv(\"test_metadata.csv\", index=False)\n\n# Directories containing images\nimage_dirs = [\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1\",\n    \"../input/skin-cancer-mnist-ham10000/HAM10000_images_part_2\"\n]\n# Attention Module\nclass AttentionBlock(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.query = nn.Conv2d(in_channels, in_channels//8, 1)\n        self.key = nn.Conv2d(in_channels, in_channels//8, 1)\n        self.value = nn.Conv2d(in_channels, in_channels, 1)\n        \n    def forward(self, x):\n        batch, channels, height, width = x.size()\n        q = self.query(x).view(batch, -1, height*width).permute(0,2,1)\n        k = self.key(x).view(batch, -1, height*width)\n        v = self.value(x).view(batch, -1, height*width)\n        \n        attn = torch.bmm(q, k)\n        attn = F.softmax(attn, dim=2)\n        out = torch.bmm(v, attn.permute(0,2,1))\n        return out.view(batch, channels, height, width)\n\n# UNet blocks\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, x):\n        return self.conv(x)\n\n# UNet with Attention\nclass UNetWithAttention(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super().__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        \n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(64, 128)\n        )\n        self.attention1 = AttentionBlock(128)\n        self.down2 = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(128, 256)\n        )\n        self.attention2 = AttentionBlock(256)\n        \n        self.classifier = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1,1)),\n            nn.Flatten(),\n            nn.Linear(256, n_classes)\n        )\n        \n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x2 = self.attention1(x2)\n        x3 = self.down2(x2)\n        x3 = self.attention2(x3)\n        return self.classifier(x3)\n\n# FASTGAN Discriminator\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 1, 4, 1, 0),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x):\n        return self.main(x)\n\n# Training setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                       std=[0.229, 0.224, 0.225])\n])\n\n# Create datasets\ntrain_dataset = HAM10000Dataset(\"train_metadata.csv\", image_dirs, transform=transform)\ntest_dataset = HAM10000Dataset(\"test_metadata.csv\", image_dirs, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Initialize models\nmodel = UNetWithAttention(3, 7).to(device)\ndiscriminator = Discriminator().to(device)\n\n# Loss and optimizers\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nd_optimizer = torch.optim.Adam(discriminator.parameters())\n\n# Training loop\ndef train_epoch(model, discriminator, train_loader, criterion, optimizer, d_optimizer):\n    model.train()\n    discriminator.train()\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        # Train discriminator\n        d_optimizer.zero_grad()\n        real_labels = torch.ones(data.size(0), 1).to(device)\n        fake_labels = torch.zeros(data.size(0), 1).to(device)\n        \n        d_real = discriminator(data)\n        d_real_loss = F.binary_cross_entropy(d_real, real_labels)\n        \n        # Train classifier\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 100 == 0:\n            print(f'Batch [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.4f}')\n\n# Saliency map visualization\ndef generate_saliency_map(model, input_image):\n    saliency = Saliency(model)\n    input_image = input_image.unsqueeze(0).requires_grad_()\n    attribution = saliency.attribute(input_image)\n    \n    return attribution.squeeze().cpu().detach().numpy()\n\n# Training\nn_epochs = 10\nfor epoch in range(n_epochs):\n    print(f'Epoch {epoch+1}/{n_epochs}')\n    train_epoch(model, discriminator, train_loader, criterion, optimizer, d_optimizer)\n\n# Example of saliency map generation\nsample_data, _ = next(iter(test_loader))\nsample_image = sample_data[0].to(device)\nsaliency_map = generate_saliency_map(model, sample_image)\n\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.imshow(sample_image.cpu().permute(1,2,0))\nplt.title('Original Image')\nplt.subplot(1,2,2)\nplt.imshow(saliency_map.mean(axis=0), cmap='hot')\nplt.title('Saliency Map')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:42:09.775296Z","iopub.execute_input":"2024-12-03T05:42:09.775696Z","iopub.status.idle":"2024-12-03T05:42:12.212313Z","shell.execute_reply.started":"2024-12-03T05:42:09.775639Z","shell.execute_reply":"2024-12-03T05:42:12.211021Z"}},"outputs":[{"name":"stdout","text":"Total images in metadata: 10015\nEpoch 1/10\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 216\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Example of saliency map generation\u001b[39;00m\n\u001b[1;32m    219\u001b[0m sample_data, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n","Cell \u001b[0;32mIn[7], line 192\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, discriminator, train_loader, criterion, optimizer, d_optimizer)\u001b[0m\n\u001b[1;32m    189\u001b[0m fake_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    191\u001b[0m d_real \u001b[38;5;241m=\u001b[39m discriminator(data)\n\u001b[0;32m--> 192\u001b[0m d_real_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Train classifier\u001b[39;00m\n\u001b[1;32m    195\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3161\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3166\u001b[0m     )\n\u001b[1;32m   3168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3169\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n","\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 1, 25, 25])) is deprecated. Please ensure they have the same size."],"ename":"ValueError","evalue":"Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([32, 1, 25, 25])) is deprecated. Please ensure they have the same size.","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"# Data Loading and Preprocessing\n\nLoad HAM10000 dataset, implement data augmentation, and prepare few-shot episodes with support and query sets.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\n\nimport os\n\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n\n# Load the dataset\n\nmetadata = pd.read_csv('../input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv')\nimage_path = '../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1'\n\n\n\n# Function to load images\n\ndef load_images(df, image_path):\n\n    images = []\n\n    for img_id in df['image_id']:\n\n        img = cv2.imread(os.path.join(image_path, f'{img_id}.jpg'))\n\n        img = cv2.resize(img, (128, 128))\n\n        images.append(img)\n\n    return np.array(images)\n\n\n\n# Load images\n\nimages = load_images(metadata, image_path)\n\nlabels = metadata['dx'].values\n\n\n\n# Split the data into training and validation sets\n\ntrain_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n\n\n\n# Data augmentation\n\ndatagen = ImageDataGenerator(\n\n    rotation_range=20,\n\n    width_shift_range=0.2,\n\n    height_shift_range=0.2,\n\n    shear_range=0.2,\n\n    zoom_range=0.2,\n\n    horizontal_flip=True,\n\n    fill_mode='nearest'\n\n)\n\n\n\n# Prepare few-shot episodes\n\ndef create_few_shot_episodes(images, labels, n_way=5, k_shot=1, k_query=1):\n\n    unique_labels = np.unique(labels)\n\n    episodes = []\n\n    for _ in range(len(images) // (n_way * (k_shot + k_query))):\n\n        selected_labels = np.random.choice(unique_labels, n_way, replace=False)\n\n        support_set = []\n\n        query_set = []\n\n        for label in selected_labels:\n\n            label_indices = np.where(labels == label)[0]\n\n            selected_indices = np.random.choice(label_indices, k_shot + k_query, replace=False)\n\n            support_set.append(images[selected_indices[:k_shot]])\n\n            query_set.append(images[selected_indices[k_shot:]])\n\n        episodes.append((np.array(support_set), np.array(query_set)))\n\n    return episodes\n\n\n\n# Create few-shot episodes\n\nfew_shot_episodes = create_few_shot_episodes(train_images, train_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:32:57.353521Z","iopub.execute_input":"2024-12-03T05:32:57.353884Z","iopub.status.idle":"2024-12-03T05:32:57.563422Z","shell.execute_reply.started":"2024-12-03T05:32:57.353849Z","shell.execute_reply":"2024-12-03T05:32:57.562039Z"}},"outputs":[{"name":"stderr","text":"[ WARN:0@175.033] global loadsave.cpp:241 findDecoder imread_('../input/skin-cancer-mnist-ham10000/HAM10000_images_part_1/ISIC_0031633.jpg'): can't open/read file: check file path/integrity\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m labels \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Split the data into training and validation sets\u001b[39;00m\n","Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mload_images\u001b[0;34m(df, image_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_id \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     32\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 34\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(images)\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"],"ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/imgproc/src/resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"# Define UNet Architecture with Attention\n\nImplement UNet with skip connections and self-attention mechanism for better feature extraction.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Input, Activation, BatchNormalization, Add, Multiply\n\nfrom tensorflow.keras.models import Model\n\n\n\n# Attention block\n\ndef attention_block(x, g, inter_channel):\n\n    theta_x = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(x)\n\n    phi_g = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(g)\n\n    f = Activation('relu')(Add()([theta_x, phi_g]))\n\n    psi_f = Conv2D(1, (1, 1), strides=(1, 1), padding='same')(f)\n\n    rate = Activation('sigmoid')(psi_f)\n\n    att_x = Multiply()([x, rate])\n\n    return att_x\n\n\n\n# UNet with Attention\n\ndef unet_with_attention(input_shape=(128, 128, 3)):\n\n    inputs = Input(input_shape)\n\n    \n\n    # Encoder\n\n    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n\n    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    \n\n    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n\n    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    \n\n    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n\n    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    \n\n    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n\n    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    \n\n    # Bottleneck\n\n    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n\n    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)\n\n    \n\n    # Decoder\n\n    up6 = UpSampling2D(size=(2, 2))(conv5)\n\n    att6 = attention_block(conv4, up6, 512)\n\n    merge6 = Concatenate()([att6, up6])\n\n    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(merge6)\n\n    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n\n    \n\n    up7 = UpSampling2D(size=(2, 2))(conv6)\n\n    att7 = attention_block(conv3, up7, 256)\n\n    merge7 = Concatenate()([att7, up7])\n\n    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(merge7)\n\n    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n\n    \n\n    up8 = UpSampling2D(size=(2, 2))(conv7)\n\n    att8 = attention_block(conv2, up8, 128)\n\n    merge8 = Concatenate()([att8, up8])\n\n    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge8)\n\n    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n\n    \n\n    up9 = UpSampling2D(size=(2, 2))(conv8)\n\n    att9 = attention_block(conv1, up9, 64)\n\n    merge9 = Concatenate()([att9, up9])\n\n    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge9)\n\n    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n\n    \n\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n    \n\n    model = Model(inputs, conv10)\n\n    \n\n    return model\n\n\n\n# Create the model\n\nunet_model = unet_with_attention()\n\nunet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nunet_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:32:57.564222Z","iopub.status.idle":"2024-12-03T05:32:57.564526Z","shell.execute_reply.started":"2024-12-03T05:32:57.564382Z","shell.execute_reply":"2024-12-03T05:32:57.564398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Implement Few-Shot Learning Components\n\nCreate prototypical network components for few-shot learning, including support set embedding and query set comparison.","metadata":{}},{"cell_type":"code","source":"# Implement Few-Shot Learning Components\n\n\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Dense, Flatten\n\nfrom tensorflow.keras.models import Model\n\n\n\n# Prototypical Network Components\n\nclass PrototypicalNetwork(Model):\n\n    def __init__(self, encoder):\n\n        super(PrototypicalNetwork, self).__init__()\n\n        self.encoder = encoder\n\n\n\n    def call(self, support_set, query_set):\n\n        # Embed the support set\n\n        support_embeddings = self.encoder(support_set)\n\n        support_embeddings = tf.reduce_mean(support_embeddings, axis=1)  # Average embeddings for each class\n\n\n\n        # Embed the query set\n\n        query_embeddings = self.encoder(query_set)\n\n\n\n        return support_embeddings, query_embeddings\n\n\n\n# Encoder Model\n\ndef create_encoder(input_shape=(128, 128, 3)):\n\n    inputs = Input(input_shape)\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Flatten()(x)\n\n    x = Dense(1024, activation='relu')(x)\n\n    model = Model(inputs, x)\n\n    return model\n\n\n\n# Create encoder and prototypical network\n\nencoder = create_encoder()\n\nproto_net = PrototypicalNetwork(encoder)\n\n\n\n# Example usage with few-shot episodes\n\nsupport_set, query_set = few_shot_episodes[0]\n\nsupport_embeddings, query_embeddings = proto_net(support_set, query_set)\n\n\n\n# Calculate distances between support and query embeddings\n\ndef euclidean_distance(a, b):\n\n    return tf.sqrt(tf.reduce_sum(tf.square(a - b), axis=-1))\n\n\n\ndistances = euclidean_distance(tf.expand_dims(query_embeddings, 1), tf.expand_dims(support_embeddings, 0))\n\n\n\n# Predict class based on nearest prototype\n\npredictions = tf.argmin(distances, axis=-1)\n\n\n\n# Print predictions\n\nprint(predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:32:57.566343Z","iopub.status.idle":"2024-12-03T05:32:57.567058Z","shell.execute_reply.started":"2024-12-03T05:32:57.566811Z","shell.execute_reply":"2024-12-03T05:32:57.566835Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FASTGAN Implementation\n\nImplement FASTGAN for data augmentation to enhance few-shot learning performance.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, ReLU, Flatten, Dense, Reshape\n\nfrom tensorflow.keras.models import Model\n\n\n\n# Define the generator model for FASTGAN\n\ndef build_generator(latent_dim):\n\n    model = tf.keras.Sequential()\n\n    model.add(Dense(8 * 8 * 256, input_dim=latent_dim))\n\n    model.add(Reshape((8, 8, 256)))\n\n    model.add(BatchNormalization())\n\n    model.add(ReLU())\n\n\n\n    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n\n    model.add(BatchNormalization())\n\n    model.add(ReLU())\n\n\n\n    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n\n    model.add(BatchNormalization())\n\n    model.add(ReLU())\n\n\n\n    model.add(Conv2DTranspose(32, kernel_size=4, strides=2, padding='same'))\n\n    model.add(BatchNormalization())\n\n    model.add(ReLU())\n\n\n\n    model.add(Conv2D(3, kernel_size=3, padding='same', activation='tanh'))\n\n    return model\n\n\n\n# Define the discriminator model for FASTGAN\n\ndef build_discriminator(input_shape):\n\n    model = tf.keras.Sequential()\n\n    model.add(Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=input_shape))\n\n    model.add(LeakyReLU(alpha=0.2))\n\n\n\n    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n\n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha=0.2))\n\n\n\n    model.add(Conv2D(256, kernel_size=4, strides=2, padding='same'))\n\n    model.add(BatchNormalization())\n\n    model.add(LeakyReLU(alpha=0.2))\n\n\n\n    model.add(Flatten())\n\n    model.add(Dense(1, activation='sigmoid'))\n\n    return model\n\n\n\n# Define the GAN model combining generator and discriminator\n\ndef build_gan(generator, discriminator):\n\n    discriminator.trainable = False\n\n    model = tf.keras.Sequential([generator, discriminator])\n\n    return model\n\n\n\n# Set parameters\n\nlatent_dim = 100\n\ninput_shape = (128, 128, 3)\n\n\n\n# Build and compile the discriminator\n\ndiscriminator = build_discriminator(input_shape)\n\ndiscriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n\n\n# Build the generator\n\ngenerator = build_generator(latent_dim)\n\n\n\n# Build and compile the GAN\n\ngan = build_gan(generator, discriminator)\n\ngan.compile(optimizer='adam', loss='binary_crossentropy')\n\n\n\n# Function to generate and save images\n\ndef generate_and_save_images(model, epoch, test_input):\n\n    predictions = model(test_input, training=False)\n\n    fig = plt.figure(figsize=(4, 4))\n\n\n\n    for i in range(predictions.shape[0]):\n\n        plt.subplot(4, 4, i + 1)\n\n        plt.imshow((predictions[i] * 127.5 + 127.5).astype(np.uint8))\n\n        plt.axis('off')\n\n\n\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n\n    plt.show()\n\n\n\n# Training the GAN\n\ndef train_gan(generator, discriminator, gan, dataset, latent_dim, epochs=10000, batch_size=64, save_interval=200):\n\n    half_batch = batch_size // 2\n\n\n\n    for epoch in range(epochs):\n\n        # Train discriminator\n\n        idx = np.random.randint(0, dataset.shape[0], half_batch)\n\n        real_images = dataset[idx]\n\n        real_labels = np.ones((half_batch, 1))\n\n\n\n        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n\n        fake_images = generator.predict(noise)\n\n        fake_labels = np.zeros((half_batch, 1))\n\n\n\n        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n\n        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n\n\n        # Train generator\n\n        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n\n        valid_labels = np.ones((batch_size, 1))\n\n        g_loss = gan.train_on_batch(noise, valid_labels)\n\n\n\n        # Print progress\n\n        if epoch % save_interval == 0:\n\n            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n\n            generate_and_save_images(generator, epoch, np.random.normal(0, 1, (16, latent_dim)))\n\n\n\n# Prepare the dataset for GAN training\n\ntrain_images = (train_images.astype(np.float32) - 127.5) / 127.5\n\n\n\n# Train the GAN\n\ntrain_gan(generator, discriminator, gan, train_images, latent_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:32:57.568407Z","iopub.status.idle":"2024-12-03T05:32:57.568748Z","shell.execute_reply.started":"2024-12-03T05:32:57.568575Z","shell.execute_reply":"2024-12-03T05:32:57.568591Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training\n\nTrain the model using episodic training paradigm, combining few-shot learning with GAN-augmented data.","metadata":{}},{"cell_type":"code","source":"# Model Training\n\n\n\n# Define the episodic training function\n\ndef train_prototypical_network(proto_net, episodes, epochs=10):\n\n    optimizer = tf.keras.optimizers.Adam()\n\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n\n\n    for epoch in range(epochs):\n\n        epoch_loss = 0\n\n        for support_set, query_set in episodes:\n\n            with tf.GradientTape() as tape:\n\n                support_embeddings, query_embeddings = proto_net(support_set, query_set)\n\n                distances = euclidean_distance(tf.expand_dims(query_embeddings, 1), tf.expand_dims(support_embeddings, 0))\n\n                predictions = tf.argmin(distances, axis=-1)\n\n                loss = loss_fn(tf.range(len(predictions)), predictions)\n\n            gradients = tape.gradient(loss, proto_net.trainable_variables)\n\n            optimizer.apply_gradients(zip(gradients, proto_net.trainable_variables))\n\n            epoch_loss += loss\n\n\n\n        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(episodes)}')\n\n\n\n# Train the Prototypical Network\n\ntrain_prototypical_network(proto_net, few_shot_episodes)\n\n\n\n# Integrate GAN-augmented data into training\n\ndef augment_with_gan(generator, support_set, k_shot, latent_dim):\n\n    noise = np.random.normal(0, 1, (k_shot, latent_dim))\n\n    generated_images = generator.predict(noise)\n\n    return np.concatenate([support_set, generated_images], axis=0)\n\n\n\n# Augment support sets with GAN-generated images\n\naugmented_episodes = []\n\nfor support_set, query_set in few_shot_episodes:\n\n    augmented_support_set = augment_with_gan(generator, support_set, k_shot=1, latent_dim=latent_dim)\n\n    augmented_episodes.append((augmented_support_set, query_set))\n\n\n\n# Train the Prototypical Network with augmented data\n\ntrain_prototypical_network(proto_net, augmented_episodes)\n\n\n\n# Explainability using saliency maps\n\ndef compute_saliency_maps(model, images, labels):\n\n    images = tf.convert_to_tensor(images)\n\n    with tf.GradientTape() as tape:\n\n        tape.watch(images)\n\n        predictions = model(images)\n\n        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n\n    gradients = tape.gradient(loss, images)\n\n    saliency_maps = tf.reduce_max(tf.abs(gradients), axis=-1)\n\n    return saliency_maps\n\n\n\n# Compute saliency maps for validation images\n\nval_images_tensor = tf.convert_to_tensor(val_images)\n\nval_labels_tensor = tf.convert_to_tensor(val_labels)\n\nsaliency_maps = compute_saliency_maps(proto_net.encoder, val_images_tensor, val_labels_tensor)\n\n\n\n# Display saliency maps\n\ndef display_saliency_maps(images, saliency_maps):\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n    axes[0].imshow(images[0].astype(np.uint8))\n\n    axes[0].set_title('Original Image')\n\n    axes[0].axis('off')\n\n    axes[1].imshow(saliency_maps[0], cmap='hot')\n\n    axes[1].set_title('Saliency Map')\n\n    axes[1].axis('off')\n\n    plt.show()\n\n\n\n# Display saliency map for a sample validation image\n\ndisplay_saliency_maps(val_images, saliency_maps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:32:57.569912Z","iopub.status.idle":"2024-12-03T05:32:57.570179Z","shell.execute_reply.started":"2024-12-03T05:32:57.570050Z","shell.execute_reply":"2024-12-03T05:32:57.570063Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Saliency Maps and Explainability\n\nGenerate saliency maps using gradient-based methods to visualize model decisions.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\n\n\n# Explainability using saliency maps\n\ndef compute_saliency_maps(model, images, labels):\n\n    images = tf.convert_to_tensor(images, dtype=tf.float32)\n\n    with tf.GradientTape() as tape:\n\n        tape.watch(images)\n\n        predictions = model(images)\n\n        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n\n    gradients = tape.gradient(loss, images)\n\n    saliency_maps = tf.reduce_max(tf.abs(gradients), axis=-1)\n\n    return saliency_maps\n\n\n\n# Compute saliency maps for validation images\n\nval_images_tensor = tf.convert_to_tensor(val_images, dtype=tf.float32)\n\nval_labels_tensor = tf.convert_to_tensor(val_labels, dtype=tf.int64)\n\nsaliency_maps = compute_saliency_maps(proto_net.encoder, val_images_tensor, val_labels_tensor)\n\n\n\n# Display saliency maps\n\ndef display_saliency_maps(images, saliency_maps):\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n    axes[0].imshow(images[0].astype(np.uint8))\n\n    axes[0].set_title('Original Image')\n\n    axes[0].axis('off')\n\n    axes[1].imshow(saliency_maps[0], cmap='hot')\n\n    axes[1].set_title('Saliency Map')\n\n    axes[1].axis('off')\n\n    plt.show()\n\n\n\n# Display saliency map for a sample validation image\n\ndisplay_saliency_maps(val_images, saliency_maps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:32:57.571629Z","iopub.status.idle":"2024-12-03T05:32:57.571928Z","shell.execute_reply.started":"2024-12-03T05:32:57.571798Z","shell.execute_reply":"2024-12-03T05:32:57.571812Z"}},"outputs":[],"execution_count":null}]}