{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93f60b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T03:44:37.057262Z",
     "iopub.status.busy": "2025-01-05T03:44:37.056992Z",
     "iopub.status.idle": "2025-01-05T03:44:54.663582Z",
     "shell.execute_reply": "2025-01-05T03:44:54.662587Z"
    },
    "papermill": {
     "duration": 17.612386,
     "end_time": "2025-01-05T03:44:54.666014",
     "exception": false,
     "start_time": "2025-01-05T03:44:37.053628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Collecting adabelief-pytorch\r\n",
      "  Downloading adabelief_pytorch-0.2.1-py3-none-any.whl.metadata (616 bytes)\r\n",
      "Requirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (2.4.0)\r\n",
      "Requirement already satisfied: colorama>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (0.4.6)\r\n",
      "Requirement already satisfied: tabulate>=0.7 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (0.9.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (2024.6.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\r\n",
      "Downloading adabelief_pytorch-0.2.1-py3-none-any.whl (5.8 kB)\r\n",
      "Installing collected packages: adabelief-pytorch\r\n",
      "Successfully installed adabelief-pytorch-0.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision tqdm pillow\n",
    "!pip install adabelief-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76b970c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T03:44:54.674410Z",
     "iopub.status.busy": "2025-01-05T03:44:54.674085Z",
     "iopub.status.idle": "2025-01-05T08:13:48.274608Z",
     "shell.execute_reply": "2025-01-05T08:13:48.273582Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 16133.619329,
     "end_time": "2025-01-05T08:13:48.288545",
     "exception": false,
     "start_time": "2025-01-05T03:44:54.669216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 84.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training backbone...\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training attention module...\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Training few-shot classifier...\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [1/20], Average Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [2/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [3/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [4/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [5/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [6/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [7/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [8/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [9/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [10/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [11/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [12/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [13/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [14/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [15/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [16/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [17/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [18/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [19/20], Average Loss: 0.0000\n",
      "Error in episode 0: name 'compute_prototypes' is not defined\n",
      "Error in episode 1: name 'compute_prototypes' is not defined\n",
      "Error in episode 2: name 'compute_prototypes' is not defined\n",
      "Error in episode 3: name 'compute_prototypes' is not defined\n",
      "Error in episode 4: name 'compute_prototypes' is not defined\n",
      "Error in episode 5: name 'compute_prototypes' is not defined\n",
      "Error in episode 6: name 'compute_prototypes' is not defined\n",
      "Error in episode 7: name 'compute_prototypes' is not defined\n",
      "Error in episode 8: name 'compute_prototypes' is not defined\n",
      "Error in episode 9: name 'compute_prototypes' is not defined\n",
      "Epoch [20/20], Average Loss: 0.0000\n",
      "Evaluating model...\n",
      "Episode 100/2000, Running Avg Accuracy: 0.9787\n",
      "Episode 200/2000, Running Avg Accuracy: 0.9767\n",
      "Episode 300/2000, Running Avg Accuracy: 0.9760\n",
      "Episode 400/2000, Running Avg Accuracy: 0.9752\n",
      "Episode 500/2000, Running Avg Accuracy: 0.9741\n",
      "Episode 600/2000, Running Avg Accuracy: 0.9748\n",
      "Episode 700/2000, Running Avg Accuracy: 0.9747\n",
      "Episode 800/2000, Running Avg Accuracy: 0.9752\n",
      "Episode 900/2000, Running Avg Accuracy: 0.9756\n",
      "Episode 1000/2000, Running Avg Accuracy: 0.9760\n",
      "Episode 1100/2000, Running Avg Accuracy: 0.9765\n",
      "Episode 1200/2000, Running Avg Accuracy: 0.9768\n",
      "Episode 1300/2000, Running Avg Accuracy: 0.9766\n",
      "Episode 1400/2000, Running Avg Accuracy: 0.9764\n",
      "Episode 1500/2000, Running Avg Accuracy: 0.9762\n",
      "Episode 1600/2000, Running Avg Accuracy: 0.9764\n",
      "Episode 1700/2000, Running Avg Accuracy: 0.9766\n",
      "Episode 1800/2000, Running Avg Accuracy: 0.9766\n",
      "Episode 1900/2000, Running Avg Accuracy: 0.9764\n",
      "Episode 2000/2000, Running Avg Accuracy: 0.9764\n",
      "\n",
      "Final Average Accuracy over 2000 episodes: 0.9764\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Modified backbone to ensure consistent output size\n",
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        # Load pretrained ResNet-18\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify first conv layer to 3x3\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Remove first maxpool to maintain spatial dimensions\n",
    "        # Keep other layers but remove final FC\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        return x  # Output will be [batch_size, 512, H/16, W/16]\n",
    "\n",
    "def sample_episode(dataset, n_way=2, n_support=5, n_query=15):\n",
    "    \"\"\"\n",
    "    Samples an episode for few-shot learning\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset to sample from (can be Dataset or Subset)\n",
    "        n_way: Number of classes per episode\n",
    "        n_support: Number of support examples per class\n",
    "        n_query: Number of query examples per class\n",
    "    \n",
    "    Returns:\n",
    "        support_images: Tensor of support set images\n",
    "        support_labels: Tensor of support set labels\n",
    "        query_images: Tensor of query set images\n",
    "        query_labels: Tensor of query set labels\n",
    "    \"\"\"\n",
    "    # Handle both Dataset and Subset cases\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        original_dataset = dataset.dataset\n",
    "        indices = dataset.indices\n",
    "        # Get labels for the subset\n",
    "        labels = [original_dataset.labels[i] for i in indices]\n",
    "    else:\n",
    "        labels = dataset.labels\n",
    "        indices = range(len(dataset))\n",
    "    \n",
    "    # Get all available classes\n",
    "    all_classes = sorted(list(set(labels)))\n",
    "    \n",
    "    # Randomly sample n_way classes\n",
    "    selected_classes = random.sample(all_classes, n_way)\n",
    "    \n",
    "    # Initialize lists to store support and query examples\n",
    "    support_images = []\n",
    "    support_labels = []\n",
    "    query_images = []\n",
    "    query_labels = []\n",
    "    \n",
    "    # For each selected class\n",
    "    for label_idx, class_label in enumerate(selected_classes):\n",
    "        # Get all indices for this class\n",
    "        class_indices = [i for i, (idx, label) in enumerate(zip(indices, labels)) if label == class_label]\n",
    "        \n",
    "        # Ensure we have enough examples\n",
    "        if len(class_indices) < n_support + n_query:\n",
    "            n_query = max(1, len(class_indices) - n_support)  # Ensure at least 1 query example\n",
    "        \n",
    "        # Sample support and query indices\n",
    "        selected_indices = random.sample(class_indices, n_support + n_query)\n",
    "        support_indices = selected_indices[:n_support]\n",
    "        query_indices = selected_indices[n_support:n_support + n_query]\n",
    "        \n",
    "        # Get support examples\n",
    "        for idx in support_indices:\n",
    "            if isinstance(dataset, torch.utils.data.Subset):\n",
    "                image, _ = dataset[idx]\n",
    "            else:\n",
    "                image, _ = dataset[indices[idx]]\n",
    "            support_images.append(image)\n",
    "            support_labels.append(label_idx)\n",
    "        \n",
    "        # Get query examples\n",
    "        for idx in query_indices:\n",
    "            if isinstance(dataset, torch.utils.data.Subset):\n",
    "                image, _ = dataset[idx]\n",
    "            else:\n",
    "                image, _ = dataset[indices[idx]]\n",
    "            query_images.append(image)\n",
    "            query_labels.append(label_idx)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    support_images = torch.stack(support_images)\n",
    "    support_labels = torch.tensor(support_labels)\n",
    "    query_images = torch.stack(query_images)\n",
    "    query_labels = torch.tensor(query_labels)\n",
    "    \n",
    "    return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels // 8, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attention = F.relu(self.conv1(x))\n",
    "        attention = torch.sigmoid(self.conv2(attention))\n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class DynamicPatternExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=256, num_patterns=7, num_iterations=3):\n",
    "        super(DynamicPatternExtractor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patterns = num_patterns\n",
    "        self.num_iterations = num_iterations\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Adaptive pooling for handling variable input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dynamic pattern initialization network\n",
    "        self.pattern_init = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_patterns * input_dim)\n",
    "        )\n",
    "        \n",
    "        # Use PyTorch's built-in GRUCell instead of custom implementation\n",
    "        self.gru = nn.GRUCell(input_dim + input_dim, hidden_dim)  # Combined feature dimensions\n",
    "        \n",
    "        # Pattern attention network\n",
    "        self.pattern_attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),  # Single attention score per pattern\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Pattern update network\n",
    "        self.pattern_update = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "        # Complexity estimation network\n",
    "        self.complexity_estimator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def init_patterns(self, x):\n",
    "        \"\"\"Initialize patterns based on input features\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        init_features = self.pattern_init(x)\n",
    "        return init_features.view(batch_size, self.num_patterns, self.input_dim)\n",
    "    \n",
    "    def estimate_complexity(self, x):\n",
    "        \"\"\"Estimate image complexity to adjust pattern refinement\"\"\"\n",
    "        complexity = self.complexity_estimator(x)\n",
    "        return complexity.squeeze(-1)  # Remove last dimension for broadcasting\n",
    "    \n",
    "    def refine_patterns(self, patterns, features, complexity):\n",
    "        \"\"\"Refine patterns based on image complexity\"\"\"\n",
    "        batch_size = patterns.size(0)\n",
    "        h = None\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            # Calculate pattern attention weights\n",
    "            if h is not None:\n",
    "                # Reshape h for attention calculation\n",
    "                h_reshaped = h.view(batch_size * self.num_patterns, -1)\n",
    "                attention = self.pattern_attention(h_reshaped)\n",
    "                attention = attention.view(batch_size, self.num_patterns, 1)\n",
    "                \n",
    "                # Apply attention to patterns\n",
    "                attended_patterns = patterns * attention\n",
    "            else:\n",
    "                attended_patterns = patterns\n",
    "            \n",
    "            # Prepare features for combination\n",
    "            expanded_features = features.unsqueeze(1).expand(-1, self.num_patterns, -1)\n",
    "            \n",
    "            # Combine features and patterns\n",
    "            combined = torch.cat([expanded_features, attended_patterns], dim=-1)\n",
    "            \n",
    "            # Update hidden state\n",
    "            combined_flat = combined.view(batch_size * self.num_patterns, -1)\n",
    "            if h is None:\n",
    "                h = torch.zeros(batch_size * self.num_patterns, self.hidden_dim).to(patterns.device)\n",
    "            \n",
    "            h = self.gru(combined_flat, h)\n",
    "            \n",
    "            # Prepare inputs for pattern update\n",
    "            h_reshaped = h.view(batch_size, self.num_patterns, -1)\n",
    "            update_input = torch.cat([h_reshaped, attended_patterns], dim=-1)\n",
    "            \n",
    "            # Generate and apply updates\n",
    "            updates = self.pattern_update(update_input)\n",
    "            complexity_expanded = complexity.view(batch_size, 1, 1).expand(-1, self.num_patterns, self.input_dim)\n",
    "            patterns = patterns + complexity_expanded * updates\n",
    "            \n",
    "        return patterns\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Handle spatial dimensions\n",
    "        if len(x.shape) == 4:  # If input has spatial dimensions\n",
    "            x = self.adaptive_pool(x)\n",
    "            x = x.view(batch_size, -1)\n",
    "        \n",
    "        # Estimate image complexity\n",
    "        complexity = self.estimate_complexity(x)\n",
    "        \n",
    "        # Initialize patterns\n",
    "        patterns = self.init_patterns(x)\n",
    "        \n",
    "        # Refine patterns based on complexity\n",
    "        refined_patterns = self.refine_patterns(patterns, x, complexity)\n",
    "        \n",
    "        return refined_patterns, complexity\n",
    "\n",
    "class MTUNetPlusPlus(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MTUNetPlusPlus, self).__init__()\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        self.pattern_extractor = DynamicPatternExtractor(input_dim=512)\n",
    "        self.attention = AttentionModule(512)\n",
    "        \n",
    "        # Add global pooling before classifier\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Add complexity-aware feature fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512 * 2, 512),  # Double input for concatenated features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        # Extract features using modified ResNet-18\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Global average pooling for pattern extraction\n",
    "        pooled_features = self.global_pool(features)\n",
    "        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n",
    "        \n",
    "        # Extract patterns and complexity\n",
    "        patterns, complexity = self.pattern_extractor(pooled_features)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_features = self.attention(features)\n",
    "        \n",
    "        # Global average pooling for classification\n",
    "        final_features = self.global_pool(attended_features)\n",
    "        final_features = final_features.view(final_features.size(0), -1)\n",
    "        \n",
    "        # Complexity-aware feature fusion\n",
    "        pattern_features = torch.mean(patterns, dim=1)  # Average patterns\n",
    "        fused_features = torch.cat([final_features, pattern_features], dim=1)\n",
    "        fused_features = self.fusion(fused_features)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        if return_features:\n",
    "            return logits, fused_features, patterns, complexity\n",
    "        return logits\n",
    "\n",
    "def train_backbone(model, train_loader, num_epochs, device):\n",
    "    \"\"\"Train backbone CNN on the medical dataset\"\"\"\n",
    "    optimizer = AdaBelief(model.backbone.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "def train_attention(model, train_loader, num_epochs, device):\n",
    "    \"\"\"Train attention module independently\"\"\"\n",
    "    optimizer = AdaBelief(model.attention.parameters(), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    # Freeze backbone and pattern extractor\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.pattern_extractor.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(images)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "def train_fewshot(model, train_loader, num_epochs, device):\n",
    "    \"\"\"Train few-shot classifier with reduced episodes per epoch for quick testing\"\"\"\n",
    "    optimizer = AdaBelief([\n",
    "        {'params': model.backbone.parameters(), 'lr': 1e-5},\n",
    "        {'params': model.pattern_extractor.parameters(), 'lr': 1e-5},\n",
    "        {'params': model.attention.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "    ])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        episodes_per_epoch = 10  # Reduced from 500 for quick testing\n",
    "        total_loss = 0\n",
    "        \n",
    "        for episode in range(episodes_per_epoch):\n",
    "            try:\n",
    "                # Sample episode\n",
    "                support_images, support_labels, query_images, query_labels = sample_episode(\n",
    "                    train_loader.dataset, n_way=2, n_support=5, n_query=15\n",
    "                )\n",
    "                \n",
    "                # Move to device\n",
    "                support_images, support_labels = support_images.to(device), support_labels.to(device)\n",
    "                query_images, query_labels = query_images.to(device), query_labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Get features\n",
    "                support_features = model(support_images, return_features=True)[1]\n",
    "                query_features = model(query_images, return_features=True)[1]\n",
    "                \n",
    "                # Compute prototypes\n",
    "                prototypes = compute_prototypes(support_features, support_labels)\n",
    "                \n",
    "                # Compute distances\n",
    "                logits = compute_distances(query_features, prototypes)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = F.cross_entropy(logits, query_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if episode % 2 == 0:  # Print more frequently for testing\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Episode [{episode+1}/{episodes_per_epoch}], '\n",
    "                          f'Loss: {loss.item():.4f}')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in episode {episode}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / episodes_per_epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "        scheduler.step()\n",
    "\n",
    "class HAM10000Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images organized in class folders\n",
    "            transform (callable, optional): Optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))  # Get class folders\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load all image paths and labels\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.images.append(os.path.join(class_path, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def train_epoch(model, discriminator, train_loader, optimizer_G, optimizer_D, device):\n",
    "    model.train()\n",
    "    discriminator.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        features = model.feature_extractor(images)\n",
    "        d_real = discriminator(features.detach())\n",
    "        # Use proper target shape\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        d_loss_real = F.binary_cross_entropy(d_real, real_labels)\n",
    "        d_loss_real.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator (Feature Extractor) and Classifier\n",
    "        optimizer_G.zero_grad()\n",
    "        features = model.feature_extractor(images)\n",
    "        d_fake = discriminator(features)\n",
    "        g_loss = F.binary_cross_entropy(d_fake, real_labels)\n",
    "        \n",
    "        # Classification loss\n",
    "        logits = model(images)\n",
    "        cls_loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_g_loss = cls_loss + 0.1 * g_loss\n",
    "        total_g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        total_loss += total_g_loss.item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch [{batch_idx}/{len(train_loader)}], '\n",
    "                  f'Loss: {total_g_loss.item():.4f}, '\n",
    "                  f'Class Loss: {cls_loss.item():.4f}, '\n",
    "                  f'G Loss: {g_loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_episodes(model, dataset, num_episodes=2000, n_way=2, k_shot=1, n_query=15, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate model on n-way k-shot tasks\n",
    "    Args:\n",
    "        model: trained model\n",
    "        dataset: dataset to sample episodes from (can be Dataset or Subset)\n",
    "        num_episodes: number of episodes to evaluate\n",
    "        n_way: number of classes per episode\n",
    "        k_shot: number of support examples per class\n",
    "        n_query: number of query examples per class\n",
    "        device: device to run evaluation on\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    \n",
    "    # Handle both Dataset and Subset cases\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        original_dataset = dataset.dataset\n",
    "        indices = dataset.indices\n",
    "        # Get labels for the subset\n",
    "        labels = [original_dataset.labels[i] for i in indices]\n",
    "    else:\n",
    "        labels = dataset.labels\n",
    "        indices = range(len(dataset))\n",
    "    \n",
    "    # Get all available classes\n",
    "    all_classes = sorted(list(set(labels)))\n",
    "    \n",
    "    if len(all_classes) < n_way:\n",
    "        print(f\"Warning: Only {len(all_classes)} classes available, but {n_way} classes requested.\")\n",
    "        n_way = len(all_classes)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for episode in range(num_episodes):\n",
    "            try:\n",
    "                # Randomly sample n classes\n",
    "                episode_classes = random.sample(all_classes, n_way)\n",
    "                \n",
    "                # Get indices for each class\n",
    "                support_indices = []\n",
    "                query_indices = []\n",
    "                \n",
    "                for class_idx in episode_classes:\n",
    "                    # Get all indices for this class in the subset\n",
    "                    class_indices = [i for i, (idx, label) in enumerate(zip(indices, labels)) if label == class_idx]\n",
    "                    \n",
    "                    if len(class_indices) < k_shot + n_query:\n",
    "                        # If not enough examples, use what we have\n",
    "                        available = len(class_indices)\n",
    "                        k_shot_actual = min(k_shot, available - 1)\n",
    "                        n_query_actual = min(n_query, available - k_shot_actual)\n",
    "                    else:\n",
    "                        k_shot_actual = k_shot\n",
    "                        n_query_actual = n_query\n",
    "                    \n",
    "                    # Sample k examples for support set\n",
    "                    support = random.sample(class_indices, k_shot_actual)\n",
    "                    # Sample remaining examples for query set\n",
    "                    remaining = list(set(class_indices) - set(support))\n",
    "                    query = random.sample(remaining, n_query_actual)\n",
    "                    \n",
    "                    support_indices.extend(support)\n",
    "                    query_indices.extend(query)\n",
    "                \n",
    "                # Prepare support and query sets\n",
    "                support_images = torch.stack([dataset[idx][0] for idx in support_indices]).to(device)\n",
    "                support_labels = torch.tensor([labels[idx] for idx in support_indices]).to(device)\n",
    "                query_images = torch.stack([dataset[idx][0] for idx in query_indices]).to(device)\n",
    "                query_labels = torch.tensor([labels[idx] for idx in query_indices]).to(device)\n",
    "                \n",
    "                # Get model predictions\n",
    "                support_features = model(support_images, return_features=True)[1]\n",
    "                query_features = model(query_images, return_features=True)[1]\n",
    "                \n",
    "                # Calculate prototypes for each class\n",
    "                prototypes = {}\n",
    "                for cls in episode_classes:\n",
    "                    cls_mask = support_labels == cls\n",
    "                    cls_features = support_features[cls_mask]\n",
    "                    if len(cls_features) > 0:  # Ensure we have features for this class\n",
    "                        prototypes[cls] = cls_features.mean(0)\n",
    "                \n",
    "                # Calculate distances to prototypes\n",
    "                accuracies_episode = []\n",
    "                for i, query_feat in enumerate(query_features):\n",
    "                    distances = {cls: torch.norm(query_feat - proto) for cls, proto in prototypes.items()}\n",
    "                    if distances:  # Ensure we have distances to compute\n",
    "                        predicted_cls = min(distances, key=distances.get)\n",
    "                        correct = (predicted_cls == query_labels[i].item())\n",
    "                        accuracies_episode.append(correct)\n",
    "                \n",
    "                # Calculate accuracy for this episode\n",
    "                if accuracies_episode:  # Ensure we have accuracies to compute\n",
    "                    accuracy = sum(accuracies_episode) / len(accuracies_episode)\n",
    "                    accuracies.append(accuracy)\n",
    "                \n",
    "                if (episode + 1) % 100 == 0:\n",
    "                    print(f'Episode {episode + 1}/{num_episodes}, Running Avg Accuracy: {np.mean(accuracies):.4f}')\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in episode {episode}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if accuracies:\n",
    "        final_accuracy = np.mean(accuracies)\n",
    "        print(f'\\nFinal Average Accuracy over {len(accuracies)} episodes: {final_accuracy:.4f}')\n",
    "        return final_accuracy\n",
    "    else:\n",
    "        print(\"\\nNo valid episodes completed. Please check dataset size and parameters.\")\n",
    "        return 0.0\n",
    "\n",
    "def visualize_patterns_and_features(model, dataset, device, n_support=2, n_query=2, n_patterns=7, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize patterns and features for a sampled task\n",
    "    Args:\n",
    "        model: trained model\n",
    "        dataset: dataset to sample from\n",
    "        device: device to run model on\n",
    "        n_support: number of support images to show\n",
    "        n_query: number of query images to show\n",
    "        n_patterns: number of pattern slots (default 7)\n",
    "        save_path: path to save the visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Sample images\n",
    "    support_images, support_labels, query_images, query_labels = sample_episode(\n",
    "        dataset, n_way=2, n_support=n_support, n_query=n_query\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    support_images = support_images.to(device)\n",
    "    query_images = query_images.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get patterns and features for support set\n",
    "        _, support_features, support_patterns = model(support_images, return_features=True)\n",
    "        # Get patterns and features for query set\n",
    "        _, query_features, query_patterns = model(query_images, return_features=True)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(4, n_patterns + 1)\n",
    "    \n",
    "    # Plot support images and their patterns\n",
    "    for i in range(n_support):\n",
    "        # Original support image\n",
    "        ax = fig.add_subplot(gs[i, 0])\n",
    "        img = support_images[i].cpu().permute(1, 2, 0)\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Support Image {i+1}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Patterns for this support image\n",
    "        for j in range(n_patterns):\n",
    "            ax = fig.add_subplot(gs[i, j+1])\n",
    "            pattern = support_patterns[i, j].reshape(int(np.sqrt(support_patterns.size(-1))), -1)\n",
    "            pattern = pattern.cpu()\n",
    "            pattern = (pattern - pattern.min()) / (pattern.max() - pattern.min())\n",
    "            ax.imshow(pattern, cmap='viridis')\n",
    "            ax.set_title(f'Pattern {j+1}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    # Plot query images and their patterns\n",
    "    for i in range(n_query):\n",
    "        # Original query image\n",
    "        ax = fig.add_subplot(gs[i+n_support, 0])\n",
    "        img = query_images[i].cpu().permute(1, 2, 0)\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Query Image {i+1}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Patterns for this query image\n",
    "        for j in range(n_patterns):\n",
    "            ax = fig.add_subplot(gs[i+n_support, j+1])\n",
    "            pattern = query_patterns[i, j].reshape(int(np.sqrt(query_patterns.size(-1))), -1)\n",
    "            pattern = pattern.cpu()\n",
    "            pattern = (pattern - pattern.min()) / (pattern.max() - pattern.min())\n",
    "            ax.imshow(pattern, cmap='viridis')\n",
    "            ax.set_title(f'Pattern {j+1}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    # Plot average patterns\n",
    "    ax = fig.add_subplot(gs[3, :])\n",
    "    avg_patterns = torch.cat([support_patterns, query_patterns], dim=0).mean(dim=0)\n",
    "    avg_patterns = avg_patterns.reshape(n_patterns, int(np.sqrt(avg_patterns.size(-1))), -1)\n",
    "    avg_patterns = avg_patterns.cpu()\n",
    "    avg_patterns = (avg_patterns - avg_patterns.min()) / (avg_patterns.max() - avg_patterns.min())\n",
    "    \n",
    "    # Create a horizontal stack of average patterns\n",
    "    combined_patterns = torch.hstack([p for p in avg_patterns])\n",
    "    ax.imshow(combined_patterns, cmap='viridis')\n",
    "    ax.set_title('Overall Average Patterns')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_dataset_patterns(model, dataset_name, dataset, device, num_samples=3, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize patterns from multiple samples in a dataset\n",
    "    Args:\n",
    "        model: trained model\n",
    "        dataset_name: name of the dataset (for title)\n",
    "        dataset: dataset to sample from\n",
    "        device: device to run model on\n",
    "        num_samples: number of different samples to show\n",
    "        save_path: path to save the visualization\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5 * num_samples))\n",
    "    \n",
    "    for sample_idx in range(num_samples):\n",
    "        # Sample a single image\n",
    "        idx = np.random.randint(len(dataset))\n",
    "        image, label = dataset[idx]\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get patterns\n",
    "            _, _, patterns = model(image, return_features=True)\n",
    "        \n",
    "        # Plot original image\n",
    "        ax = plt.subplot(num_samples, 8, sample_idx * 8 + 1)\n",
    "        img = image[0].cpu().permute(1, 2, 0)\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f'Sample {sample_idx + 1}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Plot individual patterns\n",
    "        for j in range(7):\n",
    "            ax = plt.subplot(num_samples, 8, sample_idx * 8 + j + 2)\n",
    "            pattern = patterns[0, j].reshape(int(np.sqrt(patterns.size(-1))), -1)\n",
    "            pattern = pattern.cpu()\n",
    "            pattern = (pattern - pattern.min()) / (pattern.max() - pattern.min())\n",
    "            ax.imshow(pattern, cmap='viridis')\n",
    "            ax.set_title(f'Pattern {j + 1}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Pattern Visualization - {dataset_name} Dataset', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Visualization saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Function to test the visualizations\n",
    "def test_visualizations(model, dataset, device):\n",
    "    print(\"Generating pattern visualizations...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('visualizations', exist_ok=True)\n",
    "    \n",
    "    # Generate and save visualizations\n",
    "    visualize_patterns_and_features(\n",
    "        model, \n",
    "        dataset, \n",
    "        device,\n",
    "        save_path='visualizations/patterns_and_features.png'\n",
    "    )\n",
    "    \n",
    "    visualize_dataset_patterns(\n",
    "        model,\n",
    "        'HAM10000',\n",
    "        dataset,\n",
    "        device,\n",
    "        save_path='visualizations/dataset_patterns.png'\n",
    "    )\n",
    "    \n",
    "    print(\"Visualization complete! Check the 'visualizations' folder.\")\n",
    "    \n",
    "def main():\n",
    "    # Set device and data transforms\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((80, 80)),  # As per specifications\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = HAM10000Dataset(\n",
    "        root_dir='/kaggle/input/ham10000-and-gan/synthetic_images',\n",
    "        transform=transform\n",
    "    )\n",
    "    model = MTUNetPlusPlus(num_classes=7).to(device)\n",
    "    \n",
    "    # Sequential training\n",
    "    print(\"Training backbone...\")\n",
    "    train_backbone(model, DataLoader(dataset, batch_size=32, shuffle=True), num_epochs=150, device=device)\n",
    "    \n",
    "    print(\"Training attention module...\")\n",
    "    train_attention(model, DataLoader(dataset, batch_size=32, shuffle=True), num_epochs=20, device=device)\n",
    "    \n",
    "    print(\"Training few-shot classifier...\")\n",
    "    train_fewshot(model, DataLoader(dataset, batch_size=32, shuffle=True), num_epochs=20, device=device)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"Evaluating model...\")\n",
    "    evaluate_episodes(model, dataset, num_episodes=2000, n_way=2, k_shot=5, n_query=15, device=device)\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "   main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e474fe3",
   "metadata": {
    "papermill": {
     "duration": 0.011572,
     "end_time": "2025-01-05T08:13:48.311977",
     "exception": false,
     "start_time": "2025-01-05T08:13:48.300405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6329760,
     "sourceId": 10237231,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16154.935908,
   "end_time": "2025-01-05T08:13:49.544913",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-05T03:44:34.609005",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
