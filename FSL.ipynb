{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce9556a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-01-06T15:10:40.152068Z",
     "iopub.status.busy": "2025-01-06T15:10:40.151809Z",
     "iopub.status.idle": "2025-01-06T15:10:57.821206Z",
     "shell.execute_reply": "2025-01-06T15:10:57.820348Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 17.674438,
     "end_time": "2025-01-06T15:10:57.823291",
     "exception": false,
     "start_time": "2025-01-06T15:10:40.148853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (10.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Collecting adabelief-pytorch\r\n",
      "  Downloading adabelief_pytorch-0.2.1-py3-none-any.whl.metadata (616 bytes)\r\n",
      "Requirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (2.4.0)\r\n",
      "Requirement already satisfied: colorama>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (0.4.6)\r\n",
      "Requirement already satisfied: tabulate>=0.7 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (0.9.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (2024.6.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\r\n",
      "Downloading adabelief_pytorch-0.2.1-py3-none-any.whl (5.8 kB)\r\n",
      "Installing collected packages: adabelief-pytorch\r\n",
      "Successfully installed adabelief-pytorch-0.2.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision tqdm pillow\n",
    "!pip install adabelief-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e265d7e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-06T15:10:57.830107Z",
     "iopub.status.busy": "2025-01-06T15:10:57.829836Z",
     "iopub.status.idle": "2025-01-07T02:28:43.288105Z",
     "shell.execute_reply": "2025-01-07T02:28:43.287116Z"
    },
    "papermill": {
     "duration": 40665.510109,
     "end_time": "2025-01-07T02:28:43.336144",
     "exception": false,
     "start_time": "2025-01-06T15:10:57.826035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Number of training classes: 5\n",
      "Number of validation classes: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 196MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "Starting Phase 1: Initial Training\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "\n",
      "Initial Training - Epoch 1/150\n",
      "Epoch: 0, Batch: 0, Loss: 2.3110, Accuracy: 0.00%\n",
      "Epoch: 0, Batch: 100, Loss: 2.2860, Accuracy: 36.67%\n",
      "Epoch: 0, Batch: 200, Loss: 2.2604, Accuracy: 50.00%\n",
      "Epoch: 0, Batch: 300, Loss: 2.3259, Accuracy: 0.00%\n",
      "Epoch: 0, Batch: 400, Loss: 2.2560, Accuracy: 50.00%\n",
      "Training - Loss: 2.3053, Accuracy: 10.1867\n",
      "Validation - Loss: 2.3044, Accuracy: 7.9933\n",
      "Saved checkpoint: checkpoints/best_model_epoch_0.pt\n",
      "\n",
      "Initial Training - Epoch 2/150\n",
      "Epoch: 1, Batch: 0, Loss: 2.2726, Accuracy: 0.00%\n",
      "Epoch: 1, Batch: 100, Loss: 2.2718, Accuracy: 0.00%\n",
      "Epoch: 1, Batch: 200, Loss: 2.2873, Accuracy: 50.00%\n",
      "Epoch: 1, Batch: 300, Loss: 2.2962, Accuracy: 0.00%\n",
      "Epoch: 1, Batch: 400, Loss: 2.2990, Accuracy: 0.00%\n",
      "Training - Loss: 2.3049, Accuracy: 8.3600\n",
      "Validation - Loss: 2.3023, Accuracy: 10.4600\n",
      "Saved checkpoint: checkpoints/best_model_epoch_1.pt\n",
      "\n",
      "Initial Training - Epoch 3/150\n",
      "Epoch: 2, Batch: 0, Loss: 2.2811, Accuracy: 50.00%\n",
      "Epoch: 2, Batch: 100, Loss: 2.3040, Accuracy: 0.00%\n",
      "Epoch: 2, Batch: 200, Loss: 2.3064, Accuracy: 0.00%\n",
      "Epoch: 2, Batch: 300, Loss: 2.2941, Accuracy: 50.00%\n",
      "Epoch: 2, Batch: 400, Loss: 2.3368, Accuracy: 0.00%\n",
      "Training - Loss: 2.3036, Accuracy: 10.9933\n",
      "Validation - Loss: 2.3025, Accuracy: 12.4000\n",
      "Saved checkpoint: checkpoints/best_model_epoch_2.pt\n",
      "\n",
      "Initial Training - Epoch 4/150\n",
      "Epoch: 3, Batch: 0, Loss: 2.2943, Accuracy: 0.00%\n",
      "Epoch: 3, Batch: 100, Loss: 2.3158, Accuracy: 0.00%\n",
      "Epoch: 3, Batch: 200, Loss: 2.3000, Accuracy: 0.00%\n",
      "Epoch: 3, Batch: 300, Loss: 2.3015, Accuracy: 50.00%\n",
      "Epoch: 3, Batch: 400, Loss: 2.3033, Accuracy: 0.00%\n",
      "Training - Loss: 2.3031, Accuracy: 8.4000\n",
      "Validation - Loss: 2.3027, Accuracy: 9.3000\n",
      "\n",
      "Initial Training - Epoch 5/150\n",
      "Epoch: 4, Batch: 0, Loss: 2.3001, Accuracy: 0.00%\n",
      "Epoch: 4, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 4, Batch: 200, Loss: 2.3019, Accuracy: 0.00%\n",
      "Epoch: 4, Batch: 300, Loss: 2.3033, Accuracy: 0.00%\n",
      "Epoch: 4, Batch: 400, Loss: 2.3029, Accuracy: 0.00%\n",
      "Training - Loss: 2.3028, Accuracy: 10.7867\n",
      "Validation - Loss: 2.3034, Accuracy: 7.8800\n",
      "\n",
      "Initial Training - Epoch 6/150\n",
      "Epoch: 5, Batch: 0, Loss: 2.3041, Accuracy: 0.00%\n",
      "Epoch: 5, Batch: 100, Loss: 2.2978, Accuracy: 50.00%\n",
      "Epoch: 5, Batch: 200, Loss: 2.3016, Accuracy: 50.00%\n",
      "Epoch: 5, Batch: 300, Loss: 2.3030, Accuracy: 0.00%\n",
      "Epoch: 5, Batch: 400, Loss: 2.3013, Accuracy: 0.00%\n",
      "Training - Loss: 2.3035, Accuracy: 9.2067\n",
      "Validation - Loss: 2.3025, Accuracy: 10.6000\n",
      "\n",
      "Initial Training - Epoch 7/150\n",
      "Epoch: 6, Batch: 0, Loss: 2.2990, Accuracy: 0.00%\n",
      "Epoch: 6, Batch: 100, Loss: 2.2939, Accuracy: 50.00%\n",
      "Epoch: 6, Batch: 200, Loss: 2.3031, Accuracy: 0.00%\n",
      "Epoch: 6, Batch: 300, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 6, Batch: 400, Loss: 2.3009, Accuracy: 50.00%\n",
      "Training - Loss: 2.3028, Accuracy: 9.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.6000\n",
      "\n",
      "Initial Training - Epoch 8/150\n",
      "Epoch: 7, Batch: 0, Loss: 2.3036, Accuracy: 0.00%\n",
      "Epoch: 7, Batch: 100, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 7, Batch: 200, Loss: 2.3012, Accuracy: 0.00%\n",
      "Epoch: 7, Batch: 300, Loss: 2.3007, Accuracy: 0.00%\n",
      "Epoch: 7, Batch: 400, Loss: 2.3039, Accuracy: 0.00%\n",
      "Training - Loss: 2.3027, Accuracy: 9.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 9/150\n",
      "Epoch: 8, Batch: 0, Loss: 2.3020, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 100, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 200, Loss: 2.2992, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 300, Loss: 2.3091, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 400, Loss: 2.2523, Accuracy: 0.00%\n",
      "Training - Loss: 2.3029, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3107, Accuracy: 9.3133\n",
      "\n",
      "Initial Training - Epoch 10/150\n",
      "Epoch: 9, Batch: 0, Loss: 2.3591, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 100, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 200, Loss: 2.3036, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 300, Loss: 2.3040, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 400, Loss: 2.2988, Accuracy: 0.00%\n",
      "Training - Loss: 2.3044, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3032, Accuracy: 9.3000\n",
      "\n",
      "Initial Training - Epoch 11/150\n",
      "Epoch: 10, Batch: 0, Loss: 2.3207, Accuracy: 0.00%\n",
      "Epoch: 10, Batch: 100, Loss: 2.3266, Accuracy: 0.00%\n",
      "Epoch: 10, Batch: 200, Loss: 2.2992, Accuracy: 0.00%\n",
      "Epoch: 10, Batch: 300, Loss: 2.3010, Accuracy: 0.00%\n",
      "Epoch: 10, Batch: 400, Loss: 2.3021, Accuracy: 0.00%\n",
      "Training - Loss: 2.3043, Accuracy: 10.3000\n",
      "Validation - Loss: 2.3027, Accuracy: 10.5000\n",
      "\n",
      "Initial Training - Epoch 12/150\n",
      "Epoch: 11, Batch: 0, Loss: 2.3009, Accuracy: 50.00%\n",
      "Epoch: 11, Batch: 100, Loss: 2.4086, Accuracy: 0.00%\n",
      "Epoch: 11, Batch: 200, Loss: 2.3030, Accuracy: 0.00%\n",
      "Epoch: 11, Batch: 300, Loss: 2.3021, Accuracy: 0.00%\n",
      "Epoch: 11, Batch: 400, Loss: 2.3063, Accuracy: 0.00%\n",
      "Training - Loss: 2.3028, Accuracy: 11.0000\n",
      "Validation - Loss: 2.3027, Accuracy: 8.6000\n",
      "\n",
      "Initial Training - Epoch 13/150\n",
      "Epoch: 12, Batch: 0, Loss: 2.3057, Accuracy: 0.00%\n",
      "Epoch: 12, Batch: 100, Loss: 2.3020, Accuracy: 0.00%\n",
      "Epoch: 12, Batch: 200, Loss: 2.3010, Accuracy: 50.00%\n",
      "Epoch: 12, Batch: 300, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 12, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3027, Accuracy: 9.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.0000\n",
      "\n",
      "Initial Training - Epoch 14/150\n",
      "Epoch: 13, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 100, Loss: 2.3038, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 200, Loss: 2.2987, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 300, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 400, Loss: 2.3011, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.2000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.4000\n",
      "\n",
      "Initial Training - Epoch 15/150\n",
      "Epoch: 14, Batch: 0, Loss: 2.3035, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 100, Loss: 2.3049, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 200, Loss: 2.3014, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 400, Loss: 2.3028, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.9000\n",
      "\n",
      "Initial Training - Epoch 16/150\n",
      "Epoch: 15, Batch: 0, Loss: 2.3022, Accuracy: 50.00%\n",
      "Epoch: 15, Batch: 100, Loss: 2.3036, Accuracy: 0.00%\n",
      "Epoch: 15, Batch: 200, Loss: 2.3023, Accuracy: 0.00%\n",
      "Epoch: 15, Batch: 300, Loss: 2.3017, Accuracy: 50.00%\n",
      "Epoch: 15, Batch: 400, Loss: 2.3020, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.1067\n",
      "Validation - Loss: 2.3025, Accuracy: 10.5000\n",
      "\n",
      "Initial Training - Epoch 17/150\n",
      "Epoch: 16, Batch: 0, Loss: 2.3056, Accuracy: 0.00%\n",
      "Epoch: 16, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 16, Batch: 200, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 16, Batch: 300, Loss: 2.3020, Accuracy: 50.00%\n",
      "Epoch: 16, Batch: 400, Loss: 2.3017, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9000\n",
      "\n",
      "Initial Training - Epoch 18/150\n",
      "Epoch: 17, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 17, Batch: 100, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 17, Batch: 200, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 17, Batch: 300, Loss: 2.3023, Accuracy: 50.00%\n",
      "Epoch: 17, Batch: 400, Loss: 2.3029, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 19/150\n",
      "Epoch: 18, Batch: 0, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 18, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 18, Batch: 200, Loss: 2.3023, Accuracy: 0.00%\n",
      "Epoch: 18, Batch: 300, Loss: 2.1745, Accuracy: 50.00%\n",
      "Epoch: 18, Batch: 400, Loss: 2.3032, Accuracy: 0.00%\n",
      "Training - Loss: 2.3030, Accuracy: 9.8267\n",
      "Validation - Loss: 2.3026, Accuracy: 11.6533\n",
      "\n",
      "Initial Training - Epoch 20/150\n",
      "Epoch: 19, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 19, Batch: 100, Loss: 2.3086, Accuracy: 0.00%\n",
      "Epoch: 19, Batch: 200, Loss: 2.3100, Accuracy: 0.00%\n",
      "Epoch: 19, Batch: 300, Loss: 2.3049, Accuracy: 0.00%\n",
      "Epoch: 19, Batch: 400, Loss: 2.3338, Accuracy: 0.00%\n",
      "Training - Loss: 2.3042, Accuracy: 11.1933\n",
      "Validation - Loss: 2.3027, Accuracy: 9.9933\n",
      "\n",
      "Initial Training - Epoch 21/150\n",
      "Epoch: 20, Batch: 0, Loss: 2.3038, Accuracy: 0.00%\n",
      "Epoch: 20, Batch: 100, Loss: 2.3031, Accuracy: 0.00%\n",
      "Epoch: 20, Batch: 200, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 20, Batch: 300, Loss: 2.3021, Accuracy: 50.00%\n",
      "Epoch: 20, Batch: 400, Loss: 2.3035, Accuracy: 0.00%\n",
      "Training - Loss: 2.3027, Accuracy: 8.2000\n",
      "Validation - Loss: 2.3025, Accuracy: 11.1867\n",
      "\n",
      "Initial Training - Epoch 22/150\n",
      "Epoch: 21, Batch: 0, Loss: 2.3088, Accuracy: 50.00%\n",
      "Epoch: 21, Batch: 100, Loss: 2.3244, Accuracy: 0.00%\n",
      "Epoch: 21, Batch: 200, Loss: 2.3012, Accuracy: 0.00%\n",
      "Epoch: 21, Batch: 300, Loss: 2.3043, Accuracy: 0.00%\n",
      "Epoch: 21, Batch: 400, Loss: 2.3034, Accuracy: 0.00%\n",
      "Training - Loss: 2.3027, Accuracy: 9.3933\n",
      "Validation - Loss: 2.3026, Accuracy: 10.4000\n",
      "\n",
      "Initial Training - Epoch 23/150\n",
      "Epoch: 22, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 22, Batch: 100, Loss: 2.3023, Accuracy: 0.00%\n",
      "Epoch: 22, Batch: 200, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 22, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 22, Batch: 400, Loss: 2.3029, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0933\n",
      "Validation - Loss: 2.3026, Accuracy: 9.8000\n",
      "\n",
      "Initial Training - Epoch 24/150\n",
      "Epoch: 23, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 23, Batch: 100, Loss: 2.3023, Accuracy: 50.00%\n",
      "Epoch: 23, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 23, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 23, Batch: 400, Loss: 2.3024, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.4333\n",
      "\n",
      "Initial Training - Epoch 25/150\n",
      "Epoch: 24, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 24, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 24, Batch: 200, Loss: 2.3027, Accuracy: 50.00%\n",
      "Epoch: 24, Batch: 300, Loss: 2.3022, Accuracy: 50.00%\n",
      "Epoch: 24, Batch: 400, Loss: 2.2998, Accuracy: 0.00%\n",
      "Training - Loss: 2.3028, Accuracy: 9.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.6000\n",
      "\n",
      "Initial Training - Epoch 26/150\n",
      "Epoch: 25, Batch: 0, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 25, Batch: 100, Loss: 2.3048, Accuracy: 0.00%\n",
      "Epoch: 25, Batch: 200, Loss: 2.3045, Accuracy: 0.00%\n",
      "Epoch: 25, Batch: 300, Loss: 2.3046, Accuracy: 0.00%\n",
      "Epoch: 25, Batch: 400, Loss: 2.3024, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.5000\n",
      "\n",
      "Initial Training - Epoch 27/150\n",
      "Epoch: 26, Batch: 0, Loss: 2.3042, Accuracy: 0.00%\n",
      "Epoch: 26, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 26, Batch: 200, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 26, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 26, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.7000\n",
      "\n",
      "Initial Training - Epoch 28/150\n",
      "Epoch: 27, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 27, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 27, Batch: 200, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 27, Batch: 300, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 27, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.4067\n",
      "Validation - Loss: 2.3026, Accuracy: 9.1533\n",
      "\n",
      "Initial Training - Epoch 29/150\n",
      "Epoch: 28, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 28, Batch: 100, Loss: 2.3042, Accuracy: 0.00%\n",
      "Epoch: 28, Batch: 200, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 28, Batch: 300, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 28, Batch: 400, Loss: 2.3078, Accuracy: 0.00%\n",
      "Training - Loss: 2.3027, Accuracy: 11.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 30/150\n",
      "Epoch: 29, Batch: 0, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 29, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 29, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 29, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 29, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.2800\n",
      "\n",
      "Initial Training - Epoch 31/150\n",
      "Epoch: 30, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 30, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 30, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 30, Batch: 300, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 30, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.9200\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2933\n",
      "\n",
      "Initial Training - Epoch 32/150\n",
      "Epoch: 31, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 31, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 31, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 31, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 31, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.6267\n",
      "Validation - Loss: 2.3026, Accuracy: 9.3400\n",
      "\n",
      "Initial Training - Epoch 33/150\n",
      "Epoch: 32, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 32, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 32, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 32, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 32, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5333\n",
      "\n",
      "Initial Training - Epoch 34/150\n",
      "Epoch: 33, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 33, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 33, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 33, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 33, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.7800\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9000\n",
      "\n",
      "Initial Training - Epoch 35/150\n",
      "Epoch: 34, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 34, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 34, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 34, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 34, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.0933\n",
      "Validation - Loss: 2.3026, Accuracy: 11.7133\n",
      "\n",
      "Initial Training - Epoch 36/150\n",
      "Epoch: 35, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 35, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 35, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 35, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 35, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.2000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.2400\n",
      "\n",
      "Initial Training - Epoch 37/150\n",
      "Epoch: 36, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 36, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 36, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 36, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 36, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7533\n",
      "Validation - Loss: 2.3026, Accuracy: 8.9000\n",
      "\n",
      "Initial Training - Epoch 38/150\n",
      "Epoch: 37, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 37, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 37, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 37, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 37, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.8067\n",
      "Validation - Loss: 2.3026, Accuracy: 9.3267\n",
      "\n",
      "Initial Training - Epoch 39/150\n",
      "Epoch: 38, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 38, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 38, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 38, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 38, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7733\n",
      "Validation - Loss: 2.3026, Accuracy: 9.0333\n",
      "\n",
      "Initial Training - Epoch 40/150\n",
      "Epoch: 39, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 39, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 39, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 39, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 39, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.7067\n",
      "\n",
      "Initial Training - Epoch 41/150\n",
      "Epoch: 40, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 40, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 40, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 40, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 40, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.0000\n",
      "\n",
      "Initial Training - Epoch 42/150\n",
      "Epoch: 41, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 41, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 41, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 41, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 41, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7067\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5000\n",
      "\n",
      "Initial Training - Epoch 43/150\n",
      "Epoch: 42, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 42, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 42, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 42, Batch: 300, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 42, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 44/150\n",
      "Epoch: 43, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 43, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 43, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 43, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 43, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.4000\n",
      "\n",
      "Initial Training - Epoch 45/150\n",
      "Epoch: 44, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 44, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 44, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 44, Batch: 300, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 44, Batch: 400, Loss: 2.3024, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.9000\n",
      "\n",
      "Initial Training - Epoch 46/150\n",
      "Epoch: 45, Batch: 0, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 45, Batch: 100, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 45, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 45, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 45, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.5933\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7133\n",
      "\n",
      "Initial Training - Epoch 47/150\n",
      "Epoch: 46, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 46, Batch: 100, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 46, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 46, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 46, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 48/150\n",
      "Epoch: 47, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 47, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 47, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 47, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 47, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.1933\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 49/150\n",
      "Epoch: 48, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 48, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 48, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 48, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 48, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.4733\n",
      "Validation - Loss: 2.3026, Accuracy: 11.7000\n",
      "\n",
      "Initial Training - Epoch 50/150\n",
      "Epoch: 49, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 49, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 49, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 49, Batch: 300, Loss: 2.3026, Accuracy: 13.33%\n",
      "Epoch: 49, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.2133\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2200\n",
      "\n",
      "Initial Training - Epoch 51/150\n",
      "Epoch: 50, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 50, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 50, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 50, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 50, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.1067\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9000\n",
      "\n",
      "Initial Training - Epoch 52/150\n",
      "Epoch: 51, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 51, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 51, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 51, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 51, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.6133\n",
      "\n",
      "Initial Training - Epoch 53/150\n",
      "Epoch: 52, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 52, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 52, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 52, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 52, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7333\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5067\n",
      "\n",
      "Initial Training - Epoch 54/150\n",
      "Epoch: 53, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 53, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 53, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 53, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 53, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.0800\n",
      "Validation - Loss: 2.3026, Accuracy: 10.8933\n",
      "\n",
      "Initial Training - Epoch 55/150\n",
      "Epoch: 54, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 54, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 54, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 54, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 54, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.8933\n",
      "Validation - Loss: 2.3026, Accuracy: 10.8000\n",
      "\n",
      "Initial Training - Epoch 56/150\n",
      "Epoch: 55, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 55, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 55, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 55, Batch: 300, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 55, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.1200\n",
      "Validation - Loss: 2.3026, Accuracy: 8.9200\n",
      "\n",
      "Initial Training - Epoch 57/150\n",
      "Epoch: 56, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 56, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 56, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 56, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 56, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3533\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5067\n",
      "\n",
      "Initial Training - Epoch 58/150\n",
      "Epoch: 57, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 57, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 57, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 57, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 57, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.0000\n",
      "\n",
      "Initial Training - Epoch 59/150\n",
      "Epoch: 58, Batch: 0, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 58, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 58, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 58, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 58, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.0000\n",
      "\n",
      "Initial Training - Epoch 60/150\n",
      "Epoch: 59, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 59, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 59, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 59, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 59, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.2000\n",
      "\n",
      "Initial Training - Epoch 61/150\n",
      "Epoch: 60, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 60, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 60, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 60, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 60, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.6933\n",
      "Validation - Loss: 2.3026, Accuracy: 8.9800\n",
      "\n",
      "Initial Training - Epoch 62/150\n",
      "Epoch: 61, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 61, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 61, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 61, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 61, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.5000\n",
      "\n",
      "Initial Training - Epoch 63/150\n",
      "Epoch: 62, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 62, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 62, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 62, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 62, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.8800\n",
      "\n",
      "Initial Training - Epoch 64/150\n",
      "Epoch: 63, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 63, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 63, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 63, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 63, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7333\n",
      "Validation - Loss: 2.3026, Accuracy: 9.0000\n",
      "\n",
      "Initial Training - Epoch 65/150\n",
      "Epoch: 64, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 64, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 64, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 64, Batch: 300, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 64, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 66/150\n",
      "Epoch: 65, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 65, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 65, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 65, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 65, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.4933\n",
      "\n",
      "Initial Training - Epoch 67/150\n",
      "Epoch: 66, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 66, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 66, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 66, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 66, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.8000\n",
      "\n",
      "Initial Training - Epoch 68/150\n",
      "Epoch: 67, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 67, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 67, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 67, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 67, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0200\n",
      "Validation - Loss: 2.3026, Accuracy: 10.7200\n",
      "\n",
      "Initial Training - Epoch 69/150\n",
      "Epoch: 68, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 68, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 68, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 68, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 68, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7000\n",
      "\n",
      "Initial Training - Epoch 70/150\n",
      "Epoch: 69, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 69, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 69, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 69, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 69, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 71/150\n",
      "Epoch: 70, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 70, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 70, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 70, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 70, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 72/150\n",
      "Epoch: 71, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 71, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 71, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 71, Batch: 300, Loss: 2.3026, Accuracy: 10.00%\n",
      "Epoch: 71, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.1800\n",
      "Validation - Loss: 2.3026, Accuracy: 9.6000\n",
      "\n",
      "Initial Training - Epoch 73/150\n",
      "Epoch: 72, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 72, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 72, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 72, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 72, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.6733\n",
      "Validation - Loss: 2.3026, Accuracy: 10.9200\n",
      "\n",
      "Initial Training - Epoch 74/150\n",
      "Epoch: 73, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 73, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 73, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 73, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 73, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3667\n",
      "Validation - Loss: 2.3026, Accuracy: 10.9200\n",
      "\n",
      "Initial Training - Epoch 75/150\n",
      "Epoch: 74, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 74, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 74, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 74, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 74, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.9733\n",
      "Validation - Loss: 2.3026, Accuracy: 10.5000\n",
      "\n",
      "Initial Training - Epoch 76/150\n",
      "Epoch: 75, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 75, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 75, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 75, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 75, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.6133\n",
      "Validation - Loss: 2.3026, Accuracy: 9.8000\n",
      "\n",
      "Initial Training - Epoch 77/150\n",
      "Epoch: 76, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 76, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 76, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 76, Batch: 300, Loss: 2.3028, Accuracy: 50.00%\n",
      "Epoch: 76, Batch: 400, Loss: 2.3024, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.7000\n",
      "\n",
      "Initial Training - Epoch 78/150\n",
      "Epoch: 77, Batch: 0, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 77, Batch: 100, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 77, Batch: 200, Loss: 2.3023, Accuracy: 50.00%\n",
      "Epoch: 77, Batch: 300, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 77, Batch: 400, Loss: 2.3018, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2400\n",
      "\n",
      "Initial Training - Epoch 79/150\n",
      "Epoch: 78, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 78, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 78, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 78, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 78, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.7000\n",
      "\n",
      "Initial Training - Epoch 80/150\n",
      "Epoch: 79, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 79, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 79, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 79, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 79, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 81/150\n",
      "Epoch: 80, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 80, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 80, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 80, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 80, Batch: 400, Loss: 2.3028, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 12.0000\n",
      "\n",
      "Initial Training - Epoch 82/150\n",
      "Epoch: 81, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 81, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 81, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 81, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 81, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.8000\n",
      "\n",
      "Initial Training - Epoch 83/150\n",
      "Epoch: 82, Batch: 0, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 82, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 82, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 82, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 82, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.4000\n",
      "\n",
      "Initial Training - Epoch 84/150\n",
      "Epoch: 83, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 83, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 83, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 83, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 83, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7933\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7000\n",
      "\n",
      "Initial Training - Epoch 85/150\n",
      "Epoch: 84, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 84, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 84, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 84, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 84, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 12.2600\n",
      "\n",
      "Initial Training - Epoch 86/150\n",
      "Epoch: 85, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 85, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 85, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 85, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 85, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.2800\n",
      "Validation - Loss: 2.3026, Accuracy: 9.8067\n",
      "\n",
      "Initial Training - Epoch 87/150\n",
      "Epoch: 86, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 86, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 86, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 86, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 86, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.3133\n",
      "Validation - Loss: 2.3026, Accuracy: 11.4000\n",
      "\n",
      "Initial Training - Epoch 88/150\n",
      "Epoch: 87, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 87, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 87, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 87, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 87, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.3933\n",
      "Validation - Loss: 2.3026, Accuracy: 9.1000\n",
      "\n",
      "Initial Training - Epoch 89/150\n",
      "Epoch: 88, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 88, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 88, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 88, Batch: 300, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 88, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5000\n",
      "\n",
      "Initial Training - Epoch 90/150\n",
      "Epoch: 89, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 89, Batch: 100, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 89, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 89, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 89, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7467\n",
      "\n",
      "Initial Training - Epoch 91/150\n",
      "Epoch: 90, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 90, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 90, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 90, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 90, Batch: 400, Loss: 2.3024, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.0000\n",
      "\n",
      "Initial Training - Epoch 92/150\n",
      "Epoch: 91, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 91, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 91, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 91, Batch: 300, Loss: 2.3027, Accuracy: 50.00%\n",
      "Epoch: 91, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.9067\n",
      "Validation - Loss: 2.3026, Accuracy: 9.4000\n",
      "\n",
      "Initial Training - Epoch 93/150\n",
      "Epoch: 92, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 92, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 92, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 92, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 92, Batch: 400, Loss: 2.3028, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.8867\n",
      "\n",
      "Initial Training - Epoch 94/150\n",
      "Epoch: 93, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 93, Batch: 100, Loss: 2.3027, Accuracy: 50.00%\n",
      "Epoch: 93, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 93, Batch: 300, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 93, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.2000\n",
      "\n",
      "Initial Training - Epoch 95/150\n",
      "Epoch: 94, Batch: 0, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 94, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 94, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 94, Batch: 300, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 94, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.9067\n",
      "\n",
      "Initial Training - Epoch 96/150\n",
      "Epoch: 95, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 95, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 95, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 95, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 95, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.3067\n",
      "Validation - Loss: 2.3026, Accuracy: 10.0000\n",
      "\n",
      "Initial Training - Epoch 97/150\n",
      "Epoch: 96, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 96, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 96, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 96, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 96, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.6133\n",
      "\n",
      "Initial Training - Epoch 98/150\n",
      "Epoch: 97, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 97, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 97, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 97, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 97, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.7000\n",
      "\n",
      "Initial Training - Epoch 99/150\n",
      "Epoch: 98, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 98, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 98, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 98, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 98, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.4533\n",
      "\n",
      "Initial Training - Epoch 100/150\n",
      "Epoch: 99, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 99, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 99, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 99, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 99, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.2467\n",
      "\n",
      "Initial Training - Epoch 101/150\n",
      "Epoch: 100, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 100, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 100, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 100, Batch: 300, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 100, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 7.7933\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7000\n",
      "\n",
      "Initial Training - Epoch 102/150\n",
      "Epoch: 101, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 101, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 101, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 101, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 101, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.8667\n",
      "\n",
      "Initial Training - Epoch 103/150\n",
      "Epoch: 102, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 102, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 102, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 102, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 102, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.0000\n",
      "\n",
      "Initial Training - Epoch 104/150\n",
      "Epoch: 103, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 103, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 103, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 103, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 103, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.8667\n",
      "\n",
      "Initial Training - Epoch 105/150\n",
      "Epoch: 104, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 104, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 104, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 104, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 104, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 106/150\n",
      "Epoch: 105, Batch: 0, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 105, Batch: 100, Loss: 2.3034, Accuracy: 50.00%\n",
      "Epoch: 105, Batch: 200, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 105, Batch: 300, Loss: 2.3030, Accuracy: 0.00%\n",
      "Epoch: 105, Batch: 400, Loss: 2.3020, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.2000\n",
      "\n",
      "Initial Training - Epoch 107/150\n",
      "Epoch: 106, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 106, Batch: 100, Loss: 2.3019, Accuracy: 50.00%\n",
      "Epoch: 106, Batch: 200, Loss: 2.3032, Accuracy: 0.00%\n",
      "Epoch: 106, Batch: 300, Loss: 2.3021, Accuracy: 50.00%\n",
      "Epoch: 106, Batch: 400, Loss: 2.3019, Accuracy: 0.00%\n",
      "Training - Loss: 2.3025, Accuracy: 11.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7000\n",
      "\n",
      "Initial Training - Epoch 108/150\n",
      "Epoch: 107, Batch: 0, Loss: 2.3034, Accuracy: 0.00%\n",
      "Epoch: 107, Batch: 100, Loss: 2.3033, Accuracy: 0.00%\n",
      "Epoch: 107, Batch: 200, Loss: 2.3020, Accuracy: 0.00%\n",
      "Epoch: 107, Batch: 300, Loss: 2.3018, Accuracy: 0.00%\n",
      "Epoch: 107, Batch: 400, Loss: 2.3028, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.9000\n",
      "Validation - Loss: 2.3027, Accuracy: 10.8000\n",
      "\n",
      "Initial Training - Epoch 109/150\n",
      "Epoch: 108, Batch: 0, Loss: 2.3013, Accuracy: 0.00%\n",
      "Epoch: 108, Batch: 100, Loss: 2.3040, Accuracy: 0.00%\n",
      "Epoch: 108, Batch: 200, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 108, Batch: 300, Loss: 2.3015, Accuracy: 0.00%\n",
      "Epoch: 108, Batch: 400, Loss: 2.3028, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.1000\n",
      "Validation - Loss: 2.3025, Accuracy: 11.8000\n",
      "\n",
      "Initial Training - Epoch 110/150\n",
      "Epoch: 109, Batch: 0, Loss: 2.3021, Accuracy: 0.00%\n",
      "Epoch: 109, Batch: 100, Loss: 2.3030, Accuracy: 0.00%\n",
      "Epoch: 109, Batch: 200, Loss: 2.3013, Accuracy: 0.00%\n",
      "Epoch: 109, Batch: 300, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 109, Batch: 400, Loss: 2.3035, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5000\n",
      "\n",
      "Initial Training - Epoch 111/150\n",
      "Epoch: 110, Batch: 0, Loss: 2.3023, Accuracy: 0.00%\n",
      "Epoch: 110, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 110, Batch: 200, Loss: 2.3038, Accuracy: 0.00%\n",
      "Epoch: 110, Batch: 300, Loss: 2.3017, Accuracy: 50.00%\n",
      "Epoch: 110, Batch: 400, Loss: 2.3032, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 7.5000\n",
      "Validation - Loss: 2.3025, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 112/150\n",
      "Epoch: 111, Batch: 0, Loss: 2.3018, Accuracy: 50.00%\n",
      "Epoch: 111, Batch: 100, Loss: 2.3033, Accuracy: 50.00%\n",
      "Epoch: 111, Batch: 200, Loss: 2.3019, Accuracy: 0.00%\n",
      "Epoch: 111, Batch: 300, Loss: 2.3040, Accuracy: 0.00%\n",
      "Epoch: 111, Batch: 400, Loss: 2.3037, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.2000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.3000\n",
      "\n",
      "Initial Training - Epoch 113/150\n",
      "Epoch: 112, Batch: 0, Loss: 2.3016, Accuracy: 0.00%\n",
      "Epoch: 112, Batch: 100, Loss: 2.3032, Accuracy: 0.00%\n",
      "Epoch: 112, Batch: 200, Loss: 2.3042, Accuracy: 0.00%\n",
      "Epoch: 112, Batch: 300, Loss: 2.3018, Accuracy: 50.00%\n",
      "Epoch: 112, Batch: 400, Loss: 2.3024, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9667\n",
      "\n",
      "Initial Training - Epoch 114/150\n",
      "Epoch: 113, Batch: 0, Loss: 2.3014, Accuracy: 0.00%\n",
      "Epoch: 113, Batch: 100, Loss: 2.3037, Accuracy: 0.00%\n",
      "Epoch: 113, Batch: 200, Loss: 2.3033, Accuracy: 0.00%\n",
      "Epoch: 113, Batch: 300, Loss: 2.3015, Accuracy: 0.00%\n",
      "Epoch: 113, Batch: 400, Loss: 2.3006, Accuracy: 50.00%\n",
      "Training - Loss: 2.3025, Accuracy: 11.5000\n",
      "Validation - Loss: 2.3025, Accuracy: 9.7067\n",
      "\n",
      "Initial Training - Epoch 115/150\n",
      "Epoch: 114, Batch: 0, Loss: 2.3031, Accuracy: 0.00%\n",
      "Epoch: 114, Batch: 100, Loss: 2.3046, Accuracy: 0.00%\n",
      "Epoch: 114, Batch: 200, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 114, Batch: 300, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 114, Batch: 400, Loss: 2.3006, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3000\n",
      "Validation - Loss: 2.3025, Accuracy: 11.0000\n",
      "\n",
      "Initial Training - Epoch 116/150\n",
      "Epoch: 115, Batch: 0, Loss: 2.3013, Accuracy: 0.00%\n",
      "Epoch: 115, Batch: 100, Loss: 2.3017, Accuracy: 50.00%\n",
      "Epoch: 115, Batch: 200, Loss: 2.3040, Accuracy: 0.00%\n",
      "Epoch: 115, Batch: 300, Loss: 2.3005, Accuracy: 0.00%\n",
      "Epoch: 115, Batch: 400, Loss: 2.3023, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.2000\n",
      "Validation - Loss: 2.3027, Accuracy: 8.7000\n",
      "\n",
      "Initial Training - Epoch 117/150\n",
      "Epoch: 116, Batch: 0, Loss: 2.3007, Accuracy: 50.00%\n",
      "Epoch: 116, Batch: 100, Loss: 2.3012, Accuracy: 0.00%\n",
      "Epoch: 116, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 116, Batch: 300, Loss: 2.3016, Accuracy: 50.00%\n",
      "Epoch: 116, Batch: 400, Loss: 2.3019, Accuracy: 0.00%\n",
      "Training - Loss: 2.3027, Accuracy: 11.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.3000\n",
      "\n",
      "Initial Training - Epoch 118/150\n",
      "Epoch: 117, Batch: 0, Loss: 2.3021, Accuracy: 0.00%\n",
      "Epoch: 117, Batch: 100, Loss: 2.3033, Accuracy: 50.00%\n",
      "Epoch: 117, Batch: 200, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 117, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 117, Batch: 400, Loss: 2.3029, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.8000\n",
      "\n",
      "Initial Training - Epoch 119/150\n",
      "Epoch: 118, Batch: 0, Loss: 2.3021, Accuracy: 0.00%\n",
      "Epoch: 118, Batch: 100, Loss: 2.3035, Accuracy: 0.00%\n",
      "Epoch: 118, Batch: 200, Loss: 2.3032, Accuracy: 0.00%\n",
      "Epoch: 118, Batch: 300, Loss: 2.3014, Accuracy: 50.00%\n",
      "Epoch: 118, Batch: 400, Loss: 2.3031, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5000\n",
      "\n",
      "Initial Training - Epoch 120/150\n",
      "Epoch: 119, Batch: 0, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 119, Batch: 100, Loss: 2.3034, Accuracy: 0.00%\n",
      "Epoch: 119, Batch: 200, Loss: 2.3023, Accuracy: 50.00%\n",
      "Epoch: 119, Batch: 300, Loss: 2.3030, Accuracy: 0.00%\n",
      "Epoch: 119, Batch: 400, Loss: 2.3023, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 121/150\n",
      "Epoch: 120, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 120, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 120, Batch: 200, Loss: 2.3016, Accuracy: 50.00%\n",
      "Epoch: 120, Batch: 300, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 120, Batch: 400, Loss: 2.3029, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9000\n",
      "\n",
      "Initial Training - Epoch 122/150\n",
      "Epoch: 121, Batch: 0, Loss: 2.3022, Accuracy: 50.00%\n",
      "Epoch: 121, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 121, Batch: 200, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 121, Batch: 300, Loss: 2.3020, Accuracy: 50.00%\n",
      "Epoch: 121, Batch: 400, Loss: 2.3022, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.2000\n",
      "\n",
      "Initial Training - Epoch 123/150\n",
      "Epoch: 122, Batch: 0, Loss: 2.3041, Accuracy: 0.00%\n",
      "Epoch: 122, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 122, Batch: 200, Loss: 2.3020, Accuracy: 50.00%\n",
      "Epoch: 122, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 122, Batch: 400, Loss: 2.3024, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.8000\n",
      "\n",
      "Initial Training - Epoch 124/150\n",
      "Epoch: 123, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 123, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 123, Batch: 200, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 123, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 123, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.9000\n",
      "\n",
      "Initial Training - Epoch 125/150\n",
      "Epoch: 124, Batch: 0, Loss: 2.3023, Accuracy: 50.00%\n",
      "Epoch: 124, Batch: 100, Loss: 2.3023, Accuracy: 0.00%\n",
      "Epoch: 124, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 124, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 124, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 126/150\n",
      "Epoch: 125, Batch: 0, Loss: 2.3028, Accuracy: 50.00%\n",
      "Epoch: 125, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 125, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 125, Batch: 300, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 125, Batch: 400, Loss: 2.3024, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 127/150\n",
      "Epoch: 126, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 126, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 126, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 126, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 126, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 12.0000\n",
      "\n",
      "Initial Training - Epoch 128/150\n",
      "Epoch: 127, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 127, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 127, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 127, Batch: 300, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 127, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.2000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.2000\n",
      "\n",
      "Initial Training - Epoch 129/150\n",
      "Epoch: 128, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 128, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 128, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 128, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 128, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.4933\n",
      "Validation - Loss: 2.3026, Accuracy: 10.7000\n",
      "\n",
      "Initial Training - Epoch 130/150\n",
      "Epoch: 129, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 129, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 129, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 129, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 129, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 12.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.1200\n",
      "\n",
      "Initial Training - Epoch 131/150\n",
      "Epoch: 130, Batch: 0, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 130, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 130, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 130, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 130, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.2000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.6000\n",
      "\n",
      "Initial Training - Epoch 132/150\n",
      "Epoch: 131, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 131, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 131, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 131, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 131, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.1133\n",
      "Validation - Loss: 2.3026, Accuracy: 10.6000\n",
      "\n",
      "Initial Training - Epoch 133/150\n",
      "Epoch: 132, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 132, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 132, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 132, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 132, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 134/150\n",
      "Epoch: 133, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 133, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 133, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 133, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 133, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.4000\n",
      "\n",
      "Initial Training - Epoch 135/150\n",
      "Epoch: 134, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 134, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 134, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 134, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 134, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.2000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.4000\n",
      "\n",
      "Initial Training - Epoch 136/150\n",
      "Epoch: 135, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 135, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 135, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 135, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 135, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.3933\n",
      "\n",
      "Initial Training - Epoch 137/150\n",
      "Epoch: 136, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 136, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 136, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 136, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 136, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.1600\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9000\n",
      "\n",
      "Initial Training - Epoch 138/150\n",
      "Epoch: 137, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 137, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 137, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 137, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 137, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.0067\n",
      "\n",
      "Initial Training - Epoch 139/150\n",
      "Epoch: 138, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 138, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 138, Batch: 200, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 138, Batch: 300, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 138, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Initial Training - Epoch 140/150\n",
      "Epoch: 139, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 139, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 139, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 139, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 139, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Initial Training - Epoch 141/150\n",
      "Epoch: 140, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 140, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 140, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 140, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 140, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.1067\n",
      "Validation - Loss: 2.3026, Accuracy: 8.9933\n",
      "\n",
      "Initial Training - Epoch 142/150\n",
      "Epoch: 141, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 141, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 141, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 141, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 141, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0400\n",
      "Validation - Loss: 2.3026, Accuracy: 8.7333\n",
      "\n",
      "Initial Training - Epoch 143/150\n",
      "Epoch: 142, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 142, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 142, Batch: 200, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 142, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 142, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 7.6000\n",
      "\n",
      "Initial Training - Epoch 144/150\n",
      "Epoch: 143, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 143, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 143, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 143, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 143, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.8533\n",
      "Validation - Loss: 2.3026, Accuracy: 9.5867\n",
      "\n",
      "Initial Training - Epoch 145/150\n",
      "Epoch: 144, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 144, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 144, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 144, Batch: 300, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 144, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.2000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.3000\n",
      "\n",
      "Initial Training - Epoch 146/150\n",
      "Epoch: 145, Batch: 0, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 145, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 145, Batch: 200, Loss: 2.3023, Accuracy: 0.00%\n",
      "Epoch: 145, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 145, Batch: 400, Loss: 2.3024, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.5000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.1000\n",
      "\n",
      "Initial Training - Epoch 147/150\n",
      "Epoch: 146, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 146, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 146, Batch: 200, Loss: 2.3022, Accuracy: 50.00%\n",
      "Epoch: 146, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 146, Batch: 400, Loss: 2.3023, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1200\n",
      "\n",
      "Initial Training - Epoch 148/150\n",
      "Epoch: 147, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 147, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 147, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 147, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 147, Batch: 400, Loss: 2.3023, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.2000\n",
      "\n",
      "Initial Training - Epoch 149/150\n",
      "Epoch: 148, Batch: 0, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 148, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 148, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 148, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 148, Batch: 400, Loss: 2.3027, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.7000\n",
      "\n",
      "Initial Training - Epoch 150/150\n",
      "Epoch: 149, Batch: 0, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 149, Batch: 100, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 149, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 149, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 149, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.3867\n",
      "\n",
      "Starting Phase 2: Fine-tuning\n",
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n",
      "Rectification enabled in AdaBelief\n",
      "\n",
      "Fine-tuning - Epoch 1/20\n",
      "Epoch: 0, Batch: 0, Loss: 2.3023, Accuracy: 50.00%\n",
      "Epoch: 0, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 0, Batch: 200, Loss: 2.3024, Accuracy: 0.00%\n",
      "Epoch: 0, Batch: 300, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 0, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.4000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7067\n",
      "\n",
      "Fine-tuning - Epoch 2/20\n",
      "Epoch: 1, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 1, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 1, Batch: 200, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 1, Batch: 300, Loss: 2.3029, Accuracy: 0.00%\n",
      "Epoch: 1, Batch: 400, Loss: 2.3020, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.7000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9000\n",
      "\n",
      "Fine-tuning - Epoch 3/20\n",
      "Epoch: 2, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 2, Batch: 100, Loss: 2.3023, Accuracy: 0.00%\n",
      "Epoch: 2, Batch: 200, Loss: 2.3022, Accuracy: 0.00%\n",
      "Epoch: 2, Batch: 300, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 2, Batch: 400, Loss: 2.3022, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.6267\n",
      "\n",
      "Fine-tuning - Epoch 4/20\n",
      "Epoch: 3, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 3, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 3, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 3, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 3, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Fine-tuning - Epoch 5/20\n",
      "Epoch: 4, Batch: 0, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 4, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 4, Batch: 200, Loss: 2.3024, Accuracy: 50.00%\n",
      "Epoch: 4, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 4, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3267\n",
      "Validation - Loss: 2.3026, Accuracy: 9.1200\n",
      "\n",
      "Fine-tuning - Epoch 6/20\n",
      "Epoch: 5, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 5, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 5, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 5, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 5, Batch: 400, Loss: 2.3028, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.7000\n",
      "\n",
      "Fine-tuning - Epoch 7/20\n",
      "Epoch: 6, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 6, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 6, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 6, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 6, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.2000\n",
      "\n",
      "Fine-tuning - Epoch 8/20\n",
      "Epoch: 7, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 7, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 7, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 7, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 7, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.3000\n",
      "Validation - Loss: 2.3026, Accuracy: 8.8000\n",
      "\n",
      "Fine-tuning - Epoch 9/20\n",
      "Epoch: 8, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 100, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 8, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.7600\n",
      "Validation - Loss: 2.3026, Accuracy: 10.7133\n",
      "\n",
      "Fine-tuning - Epoch 10/20\n",
      "Epoch: 9, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 200, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 9, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 11.1000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.8067\n",
      "\n",
      "Fine-tuning - Epoch 11/20\n",
      "Epoch: 10, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 10, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 10, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 10, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 10, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.8000\n",
      "Validation - Loss: 2.3026, Accuracy: 10.7000\n",
      "\n",
      "Fine-tuning - Epoch 12/20\n",
      "Epoch: 11, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 11, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 11, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 11, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 11, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.9000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.6000\n",
      "\n",
      "Fine-tuning - Epoch 13/20\n",
      "Epoch: 12, Batch: 0, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 12, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 12, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 12, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 12, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.0000\n",
      "Validation - Loss: 2.3026, Accuracy: 9.2000\n",
      "\n",
      "Fine-tuning - Epoch 14/20\n",
      "Epoch: 13, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 100, Loss: 2.3027, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 13, Batch: 400, Loss: 2.3027, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.1933\n",
      "Validation - Loss: 2.3026, Accuracy: 9.7000\n",
      "\n",
      "Fine-tuning - Epoch 15/20\n",
      "Epoch: 14, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 14, Batch: 400, Loss: 2.3026, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.3867\n",
      "Validation - Loss: 2.3026, Accuracy: 9.9000\n",
      "\n",
      "Fine-tuning - Epoch 16/20\n",
      "Epoch: 15, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 15, Batch: 100, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 15, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 15, Batch: 300, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 15, Batch: 400, Loss: 2.3025, Accuracy: 50.00%\n",
      "Training - Loss: 2.3026, Accuracy: 12.6000\n",
      "Validation - Loss: 2.3026, Accuracy: 11.0000\n",
      "\n",
      "Fine-tuning - Epoch 17/20\n",
      "Epoch: 16, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 16, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 16, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 16, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 16, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.7667\n",
      "Validation - Loss: 2.3026, Accuracy: 9.6000\n",
      "\n",
      "Fine-tuning - Epoch 18/20\n",
      "Epoch: 17, Batch: 0, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 17, Batch: 100, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 17, Batch: 200, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 17, Batch: 300, Loss: 2.3028, Accuracy: 0.00%\n",
      "Epoch: 17, Batch: 400, Loss: 2.3025, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 9.4067\n",
      "Validation - Loss: 2.3026, Accuracy: 9.4000\n",
      "\n",
      "Fine-tuning - Epoch 19/20\n",
      "Epoch: 18, Batch: 0, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 18, Batch: 100, Loss: 2.3025, Accuracy: 0.00%\n",
      "Epoch: 18, Batch: 200, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 18, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 18, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 10.2467\n",
      "Validation - Loss: 2.3026, Accuracy: 10.1000\n",
      "\n",
      "Fine-tuning - Epoch 20/20\n",
      "Epoch: 19, Batch: 0, Loss: 2.3026, Accuracy: 50.00%\n",
      "Epoch: 19, Batch: 100, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 19, Batch: 200, Loss: 2.3025, Accuracy: 50.00%\n",
      "Epoch: 19, Batch: 300, Loss: 2.3026, Accuracy: 0.00%\n",
      "Epoch: 19, Batch: 400, Loss: 2.3026, Accuracy: 0.00%\n",
      "Training - Loss: 2.3026, Accuracy: 8.9067\n",
      "Validation - Loss: 2.3026, Accuracy: 9.3000\n",
      "\n",
      "Training completed! Best accuracy: 12.4000 at epoch 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Replace 7x7 conv with 3x3 conv as per specs\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        \n",
    "        # Remove maxpool to prevent downsampling\n",
    "        self.maxpool = nn.Identity()\n",
    "        \n",
    "        # Modify first two layers to remove downsampling\n",
    "        self.layer1 = self._modify_layer(resnet.layer1, stride=1)\n",
    "        self.layer2 = self._modify_layer(resnet.layer2, stride=1)\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        \n",
    "        self.is_pretraining = True\n",
    "    \n",
    "    def _modify_layer(self, layer, stride):\n",
    "        for block in layer:\n",
    "            block.conv1.stride = (stride, stride)\n",
    "            block.conv2.stride = (1, 1)\n",
    "            if block.downsample is not None:\n",
    "                block.downsample[0].stride = (stride, stride)\n",
    "        return layer\n",
    "    \n",
    "    def freeze_parameters(self):\n",
    "        \"\"\"Freeze parameters after pretraining\"\"\"\n",
    "        self.is_pretraining = False\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # Now using 3x3 conv\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PretrainingModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PretrainingModel, self).__init__()\n",
    "        self.backbone = ModifiedResNet18(pretrained=True)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class PretrainingTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = AdaBelief(self.model.parameters(), lr=1e-4)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.1, patience=5\n",
    "        )\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, targets in tqdm(self.train_loader):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        return total_loss / len(self.train_loader), 100. * correct / total\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.val_loader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        return 100. * correct / total\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        best_acc = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            val_acc = self.validate()\n",
    "            \n",
    "            print(f'Epoch: {epoch}')\n",
    "            print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.3f}%')\n",
    "            print(f'Val Acc: {val_acc:.3f}%')\n",
    "            \n",
    "            self.scheduler.step(val_acc)\n",
    "            \n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(self.model.backbone.state_dict(), 'best_backbone.pt')\n",
    "        \n",
    "        return best_acc\n",
    "\n",
    "class GRUWithSkipConnection(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GRUWithSkipConnection, self).__init__()\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim)\n",
    "        self.skip_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        h_new = self.gru(x, h)\n",
    "        skip = self.skip_proj(x)\n",
    "        return h_new + skip\n",
    "\n",
    "class PatternExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=512, hidden_dim=256, num_patterns=7, num_iterations=3):\n",
    "        super(PatternExtractor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_patterns = num_patterns\n",
    "        self.num_iterations = num_iterations\n",
    "        \n",
    "        # Initial feature processing with 1x1 conv\n",
    "        self.conv1x1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, hidden_dim, 1, 1))\n",
    "        \n",
    "        # Attention gating parameter\n",
    "        self.attention_gate = nn.Parameter(torch.ones(1))\n",
    "        \n",
    "        # GRU with skip connections\n",
    "        self.grusc = GRUWithSkipConnection(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Pattern networks\n",
    "        self.pattern_init = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_patterns * hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Networks gq and gM (3 FC layers with ReLU as per specs)\n",
    "        self.gq = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.gM = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "    def compute_attention(self, q, k):\n",
    "        \"\"\"Compute attention scores with proper normalization\"\"\"\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1))\n",
    "        attn = attn / math.sqrt(self.hidden_dim)\n",
    "        return F.softmax(attn, dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initial feature processing\n",
    "        x = self.conv1x1(x)  # [B, hidden_dim, H, W]\n",
    "        x = x + self.positional_embedding\n",
    "        h, w = x.shape[-2:]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, H*W, hidden_dim]\n",
    "        \n",
    "        # Initialize patterns\n",
    "        patterns = self.pattern_init(x.mean(1))\n",
    "        patterns = patterns.view(batch_size, self.num_patterns, self.hidden_dim)\n",
    "        \n",
    "        # Iterative pattern refinement\n",
    "        h_state = torch.zeros(batch_size * self.num_patterns, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        for _ in range(self.num_iterations):\n",
    "            # Apply gq and gM networks\n",
    "            q = self.gq(patterns.reshape(-1, self.hidden_dim))\n",
    "            k = self.gM(x.reshape(-1, self.hidden_dim))\n",
    "            \n",
    "            # Compute attention scores with proper normalization\n",
    "            attn = self.compute_attention(\n",
    "                q.view(batch_size, self.num_patterns, -1),\n",
    "                k.view(batch_size, -1, self.hidden_dim)\n",
    "            )\n",
    "            \n",
    "            # Apply attention gating\n",
    "            attn = attn * torch.sigmoid(self.attention_gate)\n",
    "            \n",
    "            # Update patterns using attention and GRU\n",
    "            context = torch.bmm(attn, x)\n",
    "            context_flat = context.reshape(-1, self.hidden_dim)\n",
    "            h_state = self.grusc(context_flat, h_state)\n",
    "            patterns = h_state.view(batch_size, self.num_patterns, self.hidden_dim)\n",
    "        \n",
    "        return patterns, attn.view(batch_size, self.num_patterns, h, w)\n",
    "\n",
    "\n",
    "class PairwiseMatchingModule(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(PairwiseMatchingModule, self).__init__()\n",
    "        self.matching_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query_features, support_features):\n",
    "        if query_features.dim() > 2:\n",
    "            query_features = query_features.mean(1)\n",
    "        if support_features.dim() > 2:\n",
    "            support_features = support_features.mean(1)\n",
    "            \n",
    "        batch_size = query_features.size(0)\n",
    "        \n",
    "        query_expanded = query_features.unsqueeze(1)\n",
    "        \n",
    "        combined = torch.cat([\n",
    "            query_expanded.expand(-1, support_features.size(0), -1),\n",
    "            support_features.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        scores = self.matching_net(combined.view(-1, combined.size(-1)))\n",
    "        return scores.view(batch_size, -1)\n",
    "\n",
    "class MTUNetPlusPlus(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(MTUNetPlusPlus, self).__init__()\n",
    "        self.backbone = ModifiedResNet18()\n",
    "        self.pattern_extractor = PatternExtractor(in_channels=512, hidden_dim=hidden_dim)\n",
    "        self.matching_module = PairwiseMatchingModule(hidden_dim)\n",
    "        \n",
    "    def forward(self, query_img, support_imgs=None, return_features=False):\n",
    "        query_features = self.backbone(query_img)\n",
    "        query_patterns, query_attn = self.pattern_extractor(query_features)\n",
    "        \n",
    "        if support_imgs is None or return_features:\n",
    "            return query_patterns, query_attn\n",
    "        \n",
    "        support_features = self.backbone(support_imgs)\n",
    "        support_patterns, _ = self.pattern_extractor(support_features)\n",
    "        \n",
    "        query_features = query_patterns.mean(1)\n",
    "        support_features = support_patterns.mean(1)\n",
    "        \n",
    "        scores = self.matching_module(query_features, support_features)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, device,\n",
    "                 episodes_per_epoch=500, checkpoint_dir='checkpoints'):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.best_accuracy = 0.0\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (support_imgs, support_labels, query_imgs, query_labels) in enumerate(self.train_loader):\n",
    "            # Move data to device\n",
    "            support_imgs = support_imgs.squeeze(0).to(self.device)\n",
    "            support_labels = support_labels.squeeze(0).to(self.device)\n",
    "            query_imgs = query_imgs.squeeze(0).to(self.device)\n",
    "            query_labels = query_labels.squeeze(0).to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Get similarity scores\n",
    "            scores = self.model(query_imgs, support_imgs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.criterion(scores, query_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = scores.max(1)\n",
    "            correct = predicted.eq(query_labels).sum().item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_correct += correct\n",
    "            total_samples += query_labels.size(0)\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item():.4f}, '\n",
    "                      f'Accuracy: {100.0 * correct / query_labels.size(0):.2f}%')\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        avg_acc = 100.0 * total_correct / total_samples\n",
    "        \n",
    "        return avg_loss, avg_acc\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for support_imgs, support_labels, query_imgs, query_labels in self.val_loader:\n",
    "                # Move data to device\n",
    "                support_imgs = support_imgs.squeeze(0).to(self.device)\n",
    "                support_labels = support_labels.squeeze(0).to(self.device)\n",
    "                query_imgs = query_imgs.squeeze(0).to(self.device)\n",
    "                query_labels = query_labels.squeeze(0).to(self.device)\n",
    "                \n",
    "                # Get similarity scores\n",
    "                scores = self.model(query_imgs, support_imgs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self.criterion(scores, query_labels)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = scores.max(1)\n",
    "                correct = predicted.eq(query_labels).sum().item()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += correct\n",
    "                total_samples += query_labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_acc = 100.0 * total_correct / total_samples\n",
    "        \n",
    "        return avg_loss, avg_acc\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_acc):\n",
    "        \"\"\"Save model checkpoint if validation accuracy improves\"\"\"\n",
    "        if val_acc > self.best_accuracy:\n",
    "            self.best_accuracy = val_acc\n",
    "            self.best_epoch = epoch\n",
    "            \n",
    "            if not os.path.exists(self.checkpoint_dir):\n",
    "                os.makedirs(self.checkpoint_dir)\n",
    "            \n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, f'best_model_epoch_{epoch}.pt')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'val_accuracy': val_acc,\n",
    "            }, checkpoint_path)\n",
    "            print(f'Saved checkpoint: {checkpoint_path}')\n",
    "    \n",
    "    def train_phase(self, num_epochs, phase_name=\"Training\"):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n{phase_name} - Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Learning rate schedules as per specs\n",
    "            if phase_name == \"Initial Training\" and epoch == 40:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.1\n",
    "            elif phase_name == \"Fine-tuning\" and epoch == 10:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] *= 0.1\n",
    "            \n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            val_loss, val_acc = self.validate()\n",
    "            \n",
    "            print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            self.save_checkpoint(epoch, val_acc)\n",
    "    \n",
    "    def train_full(self):\n",
    "        \"\"\"Complete training process according to specifications\"\"\"\n",
    "        # Phase 1: Initial Training (150 epochs)\n",
    "        print(\"Starting Phase 1: Initial Training\")\n",
    "        # All components start with lr=1e-4\n",
    "        param_groups = [\n",
    "            {'params': self.model.backbone.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.model.pattern_extractor.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.model.matching_module.parameters(), 'lr': 1e-4}\n",
    "        ]\n",
    "        self.optimizer = AdaBelief(param_groups)\n",
    "        self.train_phase(num_epochs=150, phase_name=\"Initial Training\")\n",
    "        \n",
    "        # Phase 2: Fine-tuning (20 epochs)\n",
    "        print(\"\\nStarting Phase 2: Fine-tuning\")\n",
    "        # Adjust learning rates for fine-tuning phase\n",
    "        param_groups = [\n",
    "            {'params': self.model.backbone.parameters(), 'lr': 1e-5},  # Lower lr for backbone\n",
    "            {'params': self.model.pattern_extractor.parameters(), 'lr': 1e-5},  # Lower lr for pattern extractor\n",
    "            {'params': self.model.matching_module.parameters(), 'lr': 1e-4}  # Original lr for other components\n",
    "        ]\n",
    "        self.optimizer = AdaBelief(param_groups)\n",
    "        \n",
    "        self.train_phase(num_epochs=20, phase_name=\"Fine-tuning\")\n",
    "        \n",
    "        print(f\"\\nTraining completed! Best accuracy: {self.best_accuracy:.4f} at epoch {self.best_epoch}\")\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    def __init__(self, root_dir, allowed_classes, transform=None, n_way=2, n_support=5, n_query=15,\n",
    "                 episodes_per_epoch=500):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = n_query\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "        \n",
    "        self.classes = allowed_classes\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        \n",
    "        self.images_by_class = {}\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(root_dir, cls)\n",
    "            self.images_by_class[cls] = [\n",
    "                os.path.join(class_path, img) \n",
    "                for img in os.listdir(class_path) \n",
    "                if img.endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.episodes_per_epoch  # Configurable episodes per epoch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        episode_classes = random.sample(self.classes, self.n_way)\n",
    "        \n",
    "        support_images = []\n",
    "        support_labels = []\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "        \n",
    "        for label, cls in enumerate(episode_classes):\n",
    "            class_images = self.images_by_class[cls]\n",
    "            selected_images = random.sample(class_images, self.n_support + self.n_query)\n",
    "            \n",
    "            for img_path in selected_images[:self.n_support]:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                support_images.append(image)\n",
    "                support_labels.append(label)\n",
    "            \n",
    "            for img_path in selected_images[self.n_support:]:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                query_images.append(image)\n",
    "                query_labels.append(label)\n",
    "        \n",
    "        support_images = torch.stack(support_images)\n",
    "        support_labels = torch.tensor(support_labels)\n",
    "        query_images = torch.stack(query_images)\n",
    "        query_labels = torch.tensor(query_labels)\n",
    "        \n",
    "        return support_images, support_labels, query_images, query_labels\n",
    "\n",
    "def split_classes(root_dir, val_split=0.2, random_seed=42):\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "    \n",
    "    random.shuffle(classes)\n",
    "    \n",
    "    split_idx = int(len(classes) * (1 - val_split))\n",
    "    train_classes = classes[:split_idx]\n",
    "    val_classes = classes[split_idx:]\n",
    "    \n",
    "    return train_classes, val_classes\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((80, 80)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Split classes\n",
    "    data_path = '/kaggle/input/ham10000-and-gan/synthetic_images'  # Update this path\n",
    "    train_classes, val_classes = split_classes(data_path)\n",
    "    \n",
    "    print(f\"Number of training classes: {len(train_classes)}\")\n",
    "    print(f\"Number of validation classes: {len(val_classes)}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EpisodeDataset(\n",
    "        root_dir=data_path,\n",
    "        allowed_classes=train_classes,\n",
    "        transform=transform,\n",
    "        n_way=2,\n",
    "        n_support=5,\n",
    "        n_query=15\n",
    "    )\n",
    "    \n",
    "    val_dataset = EpisodeDataset(\n",
    "        root_dir=data_path,\n",
    "        allowed_classes=val_classes,\n",
    "        transform=transform,\n",
    "        n_way=2,\n",
    "        n_support=5,\n",
    "        n_query=15\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    model = MTUNetPlusPlus(hidden_dim=256).to(device)\n",
    "    \n",
    "    # Define parameter groups with different learning rates\n",
    "    param_groups = [\n",
    "        {'params': model.backbone.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.pattern_extractor.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.matching_module.parameters(), 'lr': 1e-4}\n",
    "    ]\n",
    "    \n",
    "    # Initialize optimizer with parameter groups\n",
    "    optimizer = AdaBelief(param_groups)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ModelTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        episodes_per_epoch=500\n",
    "    )\n",
    "    \n",
    "    # The trainer will automatically adjust learning rates during training phases\n",
    "    trainer.train_full()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6329760,
     "sourceId": 10384525,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40686.807804,
   "end_time": "2025-01-07T02:28:44.603151",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-06T15:10:37.795347",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
