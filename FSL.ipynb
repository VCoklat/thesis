{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10384525,"sourceType":"datasetVersion","datasetId":6329760}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision tqdm pillow\n!pip install adabelief-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:24:18.784173Z","iopub.execute_input":"2025-01-06T14:24:18.784804Z","iopub.status.idle":"2025-01-06T14:24:35.438956Z","shell.execute_reply.started":"2025-01-06T14:24:18.784770Z","shell.execute_reply":"2025-01-06T14:24:35.438052Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.19.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: adabelief-pytorch in /opt/conda/lib/python3.10/site-packages (0.2.1)\nRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (2.4.0)\nRequirement already satisfied: colorama>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (0.4.6)\nRequirement already satisfied: tabulate>=0.7 in /opt/conda/lib/python3.10/site-packages (from adabelief-pytorch) (0.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief-pytorch) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->adabelief-pytorch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->adabelief-pytorch) (1.3.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom PIL import Image\nimport os\nimport random\nfrom tqdm import tqdm\nfrom adabelief_pytorch import AdaBelief\n\nclass ModifiedResNet18(nn.Module):\n    def __init__(self):\n        super(ModifiedResNet18, self).__init__()\n        resnet = models.resnet18(pretrained=True)\n        \n        # Modified first conv with stride 2\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = resnet.bn1\n        self.relu = resnet.relu\n        \n        # Modify layer1 and layer2 to remove downsampling\n        self.layer1 = self._modify_layer(resnet.layer1, stride=1)\n        self.layer2 = self._modify_layer(resnet.layer2, stride=1)\n        self.layer3 = resnet.layer3\n        self.layer4 = resnet.layer4\n        \n        # Freeze parameters\n        for param in self.parameters():\n            param.requires_grad = False\n    \n    def _modify_layer(self, layer, stride):\n        for block in layer:\n            block.conv1.stride = (stride, stride)\n            block.conv2.stride = (1, 1)\n            if block.downsample is not None:\n                block.downsample[0].stride = (stride, stride)\n        return layer\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        return x  # Output: [B, 512, H/16, W/16]\n\nclass GRUWithSkipConnection(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(GRUWithSkipConnection, self).__init__()\n        self.gru = nn.GRUCell(input_dim, hidden_dim)\n        self.skip_proj = nn.Linear(input_dim, hidden_dim)\n        \n    def forward(self, x, h):\n        h_new = self.gru(x, h)\n        skip = self.skip_proj(x)\n        return h_new + skip\n\nclass PatternExtractor(nn.Module):\n    def __init__(self, in_channels=512, hidden_dim=256, num_patterns=7, num_iterations=3):\n        super(PatternExtractor, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_patterns = num_patterns\n        self.num_iterations = num_iterations\n        \n        # Initial feature processing\n        self.conv1x1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=1)\n        self.positional_embedding = nn.Parameter(torch.randn(1, hidden_dim, 1, 1))\n        \n        # Attention gating parameter\n        self.attention_gate = nn.Parameter(torch.ones(1))\n        \n        # GRU with skip connections\n        self.grusc = GRUWithSkipConnection(hidden_dim, hidden_dim)\n        \n        # Pattern networks\n        self.pattern_init = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, num_patterns * hidden_dim)\n        )\n        \n        # Attention networks\n        self.query_net = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        \n        self.key_net = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        # Initial feature processing\n        x = self.conv1x1(x)  # [B, hidden_dim, H, W]\n        x = x + self.positional_embedding\n        h, w = x.shape[-2:]\n        x = x.flatten(2).transpose(1, 2)  # [B, H*W, hidden_dim]\n        \n        # Initialize patterns\n        patterns = self.pattern_init(x.mean(1))  # [B, num_patterns * hidden_dim]\n        patterns = patterns.view(batch_size, self.num_patterns, self.hidden_dim)\n        \n        # Iterative pattern refinement\n        h_state = torch.zeros(batch_size * self.num_patterns, self.hidden_dim).to(x.device)\n        \n        for _ in range(self.num_iterations):\n            # Self-attention\n            q = self.query_net(patterns.reshape(-1, self.hidden_dim))\n            k = self.key_net(x.reshape(-1, self.hidden_dim))\n            \n            # Compute attention scores\n            attn = torch.matmul(q.view(batch_size, self.num_patterns, -1),\n                              k.view(batch_size, -1, self.hidden_dim).transpose(1, 2))\n            attn = F.softmax(attn / (self.hidden_dim ** 0.5), dim=-1)\n            \n            # Apply attention gating\n            attn = attn * torch.sigmoid(self.attention_gate)\n            \n            # Update patterns\n            context = torch.bmm(attn, x)  # [B, num_patterns, hidden_dim]\n            \n            # GRU update\n            context_flat = context.reshape(-1, self.hidden_dim)\n            h_state = self.grusc(context_flat, h_state)\n            patterns = h_state.view(batch_size, self.num_patterns, self.hidden_dim)\n        \n        return patterns, attn.view(batch_size, self.num_patterns, h, w)\n\nclass PairwiseMatchingModule(nn.Module):\n    def __init__(self, hidden_dim):\n        super(PairwiseMatchingModule, self).__init__()\n        self.matching_net = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    \n    def forward(self, query_features, support_features):\n        # Ensure both inputs are 2D\n        if query_features.dim() > 2:\n            query_features = query_features.mean(1)  # Average across any extra dimensions\n        if support_features.dim() > 2:\n            support_features = support_features.mean(1)  # Average across any extra dimensions\n            \n        batch_size = query_features.size(0)\n        \n        # Reshape for pairwise comparison\n        query_expanded = query_features.unsqueeze(1)  # [B, 1, H]\n        \n        # Combine features\n        combined = torch.cat([\n            query_expanded.expand(-1, support_features.size(0), -1),  # [B, S, H]\n            support_features.unsqueeze(0).expand(batch_size, -1, -1)  # [B, S, H]\n        ], dim=-1)\n        \n        # Get scores\n        scores = self.matching_net(combined.view(-1, combined.size(-1)))\n        return scores.view(batch_size, -1)\n\nclass MTUNetPlusPlus(nn.Module):\n    def __init__(self, hidden_dim=256):\n        super(MTUNetPlusPlus, self).__init__()\n        self.backbone = ModifiedResNet18()\n        self.pattern_extractor = PatternExtractor(in_channels=512, hidden_dim=hidden_dim)\n        self.matching_module = PairwiseMatchingModule(hidden_dim)\n        \n    def forward(self, query_img, support_imgs=None, return_features=False):\n        query_features = self.backbone(query_img)\n        query_patterns, query_attn = self.pattern_extractor(query_features)\n        \n        if support_imgs is None or return_features:\n            return query_patterns, query_attn\n        \n        support_features = self.backbone(support_imgs)\n        support_patterns, _ = self.pattern_extractor(support_features)\n        \n        # Ensure patterns are properly averaged before matching\n        query_features = query_patterns.mean(1)   # Average across patterns\n        support_features = support_patterns.mean(1)  # Average across patterns\n        \n        scores = self.matching_module(query_features, support_features)\n        \n        return scores\n\nclass ModelTrainer:\n    def __init__(self, model, train_loader, val_loader, optimizer, device,\n                 checkpoint_dir='checkpoints'):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.optimizer = optimizer\n        self.device = device\n        self.checkpoint_dir = checkpoint_dir\n        \n        os.makedirs(checkpoint_dir, exist_ok=True)\n        \n        self.best_accuracy = 0.0\n        self.best_epoch = 0\n    \n    # Modify train_episode method\n    def train_episode(self, support_images, support_labels, query_images, query_labels):\n        self.model.train()\n        \n        # Reshape tensors to correct dimensions\n        support_images = support_images.squeeze(0)  # Remove extra batch dimension\n        query_images = query_images.squeeze(0)\n        support_labels = support_labels.squeeze(0)\n        query_labels = query_labels.squeeze(0)\n        \n        # Move to device\n        support_images = support_images.to(self.device)\n        support_labels = support_labels.to(self.device)\n        query_images = query_images.to(self.device)\n        query_labels = query_labels.to(self.device)\n        \n        self.optimizer.zero_grad()\n        scores = self.model(query_images, support_images)\n        loss = F.cross_entropy(scores, query_labels)\n        \n        loss.backward()\n        self.optimizer.step()\n        \n        predictions = scores.max(1)[1]\n        accuracy = (predictions == query_labels).float().mean()\n        \n        return loss.item(), accuracy.item()\n        \n    \n    def validate_episode(self, support_images, support_labels, query_images, query_labels):\n        self.model.eval()\n        \n        with torch.no_grad():\n            support_images = support_images.to(self.device)\n            support_labels = support_labels.to(self.device)\n            query_images = query_images.to(self.device)\n            query_labels = query_labels.to(self.device)\n            \n            scores = self.model(query_images, support_images)\n            loss = F.cross_entropy(scores, query_labels)\n            \n            predictions = scores.max(1)[1]\n            accuracy = (predictions == query_labels).float().mean()\n        \n        return loss.item(), accuracy.item()\n    \n    def train_epoch(self, epoch):\n        total_loss = 0\n        total_accuracy = 0\n        \n        pbar = tqdm(enumerate(self.train_loader), total=len(self.train_loader))\n        for batch_idx, (support_imgs, support_labels, query_imgs, query_labels) in pbar:\n            loss, accuracy = self.train_episode(support_imgs, support_labels, query_imgs, query_labels)\n            \n            total_loss += loss\n            total_accuracy += accuracy\n            \n            pbar.set_description(f'Epoch {epoch} | Loss: {loss:.4f} | Acc: {accuracy:.4f}')\n        \n        return total_loss / len(self.train_loader), total_accuracy / len(self.train_loader)\n    \n    def validate(self):\n        total_loss = 0\n        total_accuracy = 0\n        \n        for support_imgs, support_labels, query_imgs, query_labels in tqdm(self.val_loader):\n            loss, accuracy = self.validate_episode(support_imgs, support_labels, query_imgs, query_labels)\n            total_loss += loss\n            total_accuracy += accuracy\n        \n        return total_loss / len(self.val_loader), total_accuracy / len(self.val_loader)\n    \n    def save_checkpoint(self, epoch, accuracy):\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'accuracy': accuracy\n        }\n        \n        path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n        torch.save(checkpoint, path)\n        \n        if accuracy > self.best_accuracy:\n            self.best_accuracy = accuracy\n            self.best_epoch = epoch\n            best_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n            torch.save(checkpoint, best_path)\n    \n    def _adjust_learning_rates(self):\n        for param_group in self.optimizer.param_groups:\n            if any(p in self.model.backbone.parameters() for p in param_group['params']):\n                param_group['lr'] = 1e-5\n            else:\n                param_group['lr'] = 1e-4\n    \n    def train_phase(self, num_epochs, phase_name=\"Training\"):\n        for epoch in range(num_epochs):\n            print(f\"\\n{phase_name} - Epoch {epoch+1}/{num_epochs}\")\n            \n            train_loss, train_acc = self.train_epoch(epoch)\n            print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n            \n            val_loss, val_acc = self.validate()\n            print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n            \n            self.save_checkpoint(epoch, val_acc)\n            \n            if phase_name == \"Initial Training\" and epoch == 40:\n                for param_group in self.optimizer.param_groups:\n                    param_group['lr'] *= 0.1\n       \n    def train_full(self):\n        \"\"\"Complete two-phase training process\"\"\"\n        # Phase 1: Initial training\n        print(\"Starting Phase 1: Initial Training\")\n        self.train_phase(num_epochs=150, phase_name=\"Initial Training\")\n        \n        # Phase 2: Fine-tuning\n        print(\"\\nStarting Phase 2: Fine-tuning\")\n        self._adjust_learning_rates()\n        self.train_phase(num_epochs=20, phase_name=\"Fine-tuning\")\n        \n        print(f\"\\nTraining completed! Best accuracy: {self.best_accuracy:.4f} at epoch {self.best_epoch}\")\n\ndef split_classes(root_dir, val_split=0.2, random_seed=42):\n    \"\"\"\n    Split classes into training and validation sets.\n    \n    Args:\n        root_dir: Path to the data directory\n        val_split: Proportion of classes to use for validation\n        random_seed: Random seed for reproducibility\n    \n    Returns:\n        train_classes: List of class names for training\n        val_classes: List of class names for validation\n    \"\"\"\n    random.seed(random_seed)\n    \n    # Get all class folders\n    classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n    \n    # Randomly shuffle classes\n    random.shuffle(classes)\n    \n    # Split classes\n    split_idx = int(len(classes) * (1 - val_split))\n    train_classes = classes[:split_idx]\n    val_classes = classes[split_idx:]\n    \n    return train_classes, val_classes\n\nclass EpisodeDataset(Dataset):\n    def __init__(self, root_dir, allowed_classes, transform=None, n_way=2, n_support=5, n_query=15):\n        \"\"\"\n        Dataset class for few-shot learning episodes\n        \n        Args:\n            root_dir: Root directory containing class folders\n            allowed_classes: List of class names this dataset can use\n            transform: Image transformations\n            n_way: Number of classes per episode\n            n_support: Number of support examples per class\n            n_query: Number of query examples per class\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.n_way = n_way\n        self.n_support = n_support\n        self.n_query = n_query\n        \n        # Only use the allowed classes\n        self.classes = allowed_classes\n        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n        \n        # Build image paths dictionary\n        self.images_by_class = {}\n        for cls in self.classes:\n            class_path = os.path.join(root_dir, cls)\n            self.images_by_class[cls] = [\n                os.path.join(class_path, img) \n                for img in os.listdir(class_path) \n                if img.endswith(('.jpg', '.jpeg', '.png'))\n            ]\n    \n    def __len__(self):\n        return 1000  # Number of episodes per epoch\n    \n    def __getitem__(self, idx):\n        # Rest of the __getitem__ method remains the same\n        episode_classes = random.sample(self.classes, self.n_way)\n        \n        support_images = []\n        support_labels = []\n        query_images = []\n        query_labels = []\n        \n        for label, cls in enumerate(episode_classes):\n            class_images = self.images_by_class[cls]\n            selected_images = random.sample(class_images, self.n_support + self.n_query)\n            \n            for img_path in selected_images[:self.n_support]:\n                image = Image.open(img_path).convert('RGB')\n                if self.transform:\n                    image = self.transform(image)\n                support_images.append(image)\n                support_labels.append(label)\n            \n            for img_path in selected_images[self.n_support:]:\n                image = Image.open(img_path).convert('RGB')\n                if self.transform:\n                    image = self.transform(image)\n                query_images.append(image)\n                query_labels.append(label)\n        \n        support_images = torch.stack(support_images)\n        support_labels = torch.tensor(support_labels)\n        query_images = torch.stack(query_images)\n        query_labels = torch.tensor(query_labels)\n        \n        return support_images, support_labels, query_images, query_labels\n\ndef main():\n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    random.seed(42)\n    np.random.seed(42)\n    \n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Define transforms\n    transform = transforms.Compose([\n        transforms.Resize((80, 80)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Split classes into train and validation sets\n    data_path = '/kaggle/input/ham10000-and-gan/synthetic_images'  # Update this to your actual data path\n    train_classes, val_classes = split_classes(data_path)\n    \n    print(f\"Number of training classes: {len(train_classes)}\")\n    print(f\"Number of validation classes: {len(val_classes)}\")\n    \n    # Create datasets with respective class splits\n    train_dataset = EpisodeDataset(\n        root_dir=data_path,\n        allowed_classes=train_classes,\n        transform=transform,\n        n_way=2,\n        n_support=5,\n        n_query=15\n    )\n    \n    val_dataset = EpisodeDataset(\n        root_dir=data_path,\n        allowed_classes=val_classes,\n        transform=transform,\n        n_way=2,\n        n_support=5,\n        n_query=15\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n    \n    # Rest of the main function remains the same\n    model = MTUNetPlusPlus(hidden_dim=256).to(device)\n    optimizer = AdaBelief(model.parameters(), lr=1e-4)\n    \n    trainer = ModelTrainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        optimizer=optimizer,\n        device=device\n    )\n    \n    trainer.train_full()\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-06T14:24:35.441193Z","iopub.execute_input":"2025-01-06T14:24:35.441524Z","execution_failed":"2025-01-06T14:24:58.151Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nNumber of training classes: 5\nNumber of validation classes: 2\n\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n\u001b[31mModifications to default arguments:\n\u001b[31m                           eps  weight_decouple    rectify\n-----------------------  -----  -----------------  ---------\nadabelief-pytorch=0.0.5  1e-08  False              False\n>=0.1.0 (Current 0.2.0)  1e-16  True               True\n\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n----------------------------------------------------------  ----------------------------------------------\nRecommended eps = 1e-8                                      Recommended eps = 1e-16\n\u001b[34mFor a complete table of recommended hyperparameters, see\n\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n\u001b[0m\nWeight decoupling enabled in AdaBelief\nRectification enabled in AdaBelief\nStarting Phase 1: Initial Training\n\nInitial Training - Epoch 1/150\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0 | Loss: 2.2855 | Acc: 0.5000:  10%|â–ˆ         | 100/1000 [00:21<02:36,  5.76it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}