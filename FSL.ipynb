{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10237231,"sourceType":"datasetVersion","datasetId":6329760}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision tqdm pillow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T07:57:53.767490Z","iopub.execute_input":"2024-12-24T07:57:53.767756Z","iopub.status.idle":"2024-12-24T07:58:03.503022Z","shell.execute_reply.started":"2024-12-24T07:57:53.767728Z","shell.execute_reply":"2024-12-24T07:58:03.501955Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport random\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport os\nfrom PIL import Image\nfrom tqdm import tqdm\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self, pretrained=True):\n        super(FeatureExtractor, self).__init__()\n        # Using ResNet as base feature extractor\n        resnet = models.resnet50(pretrained=pretrained)\n        self.features = nn.Sequential(*list(resnet.children())[:-2])\n        \n    def forward(self, x):\n        return self.features(x)\n\nclass AttentionModule(nn.Module):\n    def __init__(self, in_channels):\n        super(AttentionModule, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n        self.conv2 = nn.Conv2d(in_channels // 8, in_channels, kernel_size=1)\n        \n    def forward(self, x):\n        attention = F.relu(self.conv1(x))\n        attention = torch.sigmoid(self.conv2(attention))\n        return x * attention\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels):\n        super(Discriminator, self).__init__()\n        # Adjust the architecture to handle the feature map size from ResNet50\n        # ResNet50 outputs 2048 x 7 x 7 for 224x224 input\n        self.conv1 = nn.Conv2d(input_channels, 512, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n        # Adjust the final linear layer input size based on the conv output\n        self.fc = nn.Linear(128 * 7 * 7, 1)\n        \n        # Add batch normalization for better training stability\n        self.bn1 = nn.BatchNorm2d(512)\n        self.bn2 = nn.BatchNorm2d(256)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n    def forward(self, x):\n        # Add shape checks for debugging\n        x = self.bn1(F.leaky_relu(self.conv1(x), 0.2))\n        x = self.bn2(F.leaky_relu(self.conv2(x), 0.2))\n        x = self.bn3(F.leaky_relu(self.conv3(x), 0.2))\n        x = x.view(x.size(0), -1)  # Flatten\n        return torch.sigmoid(self.fc(x))\n\nclass MTUNetPlusPlus(nn.Module):\n    def __init__(self, num_classes, feature_dim=2048):\n        super(MTUNetPlusPlus, self).__init__()\n        self.feature_extractor = FeatureExtractor()\n        self.attention = AttentionModule(feature_dim)\n        \n        # Prototype learning\n        self.prototype_vectors = nn.Parameter(torch.randn(num_classes, feature_dim))\n        \n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(feature_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, x, return_features=False):\n        # Extract features\n        features = self.feature_extractor(x)\n        \n        # Apply attention\n        attended_features = self.attention(features)\n        \n        # Global average pooling\n        pooled_features = F.adaptive_avg_pool2d(attended_features, (1, 1))\n        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n        \n        # Prototype matching\n        prototype_distances = torch.cdist(pooled_features, self.prototype_vectors)\n        \n        # Classification\n        logits = self.classifier(pooled_features)\n        \n        if return_features:\n            return logits, pooled_features, prototype_distances, attended_features\n        return logits\n\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.features = None\n        \n        # Register hooks\n        target_layer.register_forward_hook(self._save_features)\n        target_layer.register_backward_hook(self._save_gradients)\n    \n    def _save_features(self, module, input, output):\n        self.features = output\n    \n    def _save_gradients(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0]\n    \n    def generate_cam(self, input_image, target_class):\n        # Forward pass\n        model_output = self.model(input_image)\n        \n        # Zero gradients\n        self.model.zero_grad()\n        \n        # Backward pass for target class\n        one_hot = torch.zeros_like(model_output)\n        one_hot[0][target_class] = 1\n        model_output.backward(gradient=one_hot, retain_graph=True)\n        \n        # Generate CAM\n        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n        for i in range(self.features.shape[1]):\n            self.features[:, i, :, :] *= pooled_gradients[i]\n        \n        cam = torch.mean(self.features, dim=1, keepdim=True)\n        cam = F.relu(cam)\n        cam = F.interpolate(cam, size=input_image.shape[2:], mode='bilinear', align_corners=False)\n        \n        # Normalize\n        cam = (cam - cam.min()) / (cam.max() - cam.min())\n        \n        return cam\n\ndef train_step(model, discriminator, batch, optimizer_G, optimizer_D):\n    # Training logic for one step\n    real_images, labels = batch\n    \n    # Train Discriminator\n    optimizer_D.zero_grad()\n    features = model.feature_extractor(real_images)\n    d_loss_real = F.binary_cross_entropy(discriminator(features), torch.ones_like(features))\n    d_loss_real.backward()\n    optimizer_D.step()\n    \n    # Train Generator (Feature Extractor)\n    optimizer_G.zero_grad()\n    features = model.feature_extractor(real_images)\n    g_loss = F.binary_cross_entropy(discriminator(features), torch.ones_like(features))\n    \n    # Classification loss\n    logits = model(real_images)\n    cls_loss = F.cross_entropy(logits, labels)\n    \n    # Combined loss\n    total_loss = cls_loss + 0.1 * g_loss\n    total_loss.backward()\n    optimizer_G.step()\n    \n    return total_loss.item()\n\nclass HAM10000Dataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        Args:\n            root_dir (string): Directory with all the images organized in class folders\n            transform (callable, optional): Optional transform to be applied on a sample\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.classes = sorted(os.listdir(root_dir))  # Get class folders\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        \n        self.images = []\n        self.labels = []\n        \n        # Load all image paths and labels\n        for class_name in self.classes:\n            class_path = os.path.join(root_dir, class_name)\n            if os.path.isdir(class_path):\n                for img_name in os.listdir(class_path):\n                    if img_name.endswith(('.jpg', '.jpeg', '.png')):\n                        self.images.append(os.path.join(class_path, img_name))\n                        self.labels.append(self.class_to_idx[class_name])\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        image = Image.open(img_path).convert('RGB')\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\ndef train_epoch(model, discriminator, train_loader, optimizer_G, optimizer_D, device):\n    model.train()\n    discriminator.train()\n    total_loss = 0\n    \n    for batch_idx, (images, labels) in enumerate(tqdm(train_loader)):\n        images, labels = images.to(device), labels.to(device)\n        batch_size = images.size(0)\n        \n        # Train Discriminator\n        optimizer_D.zero_grad()\n        features = model.feature_extractor(images)\n        d_real = discriminator(features.detach())\n        # Use proper target shape\n        real_labels = torch.ones(batch_size, 1).to(device)\n        d_loss_real = F.binary_cross_entropy(d_real, real_labels)\n        d_loss_real.backward()\n        optimizer_D.step()\n        \n        # Train Generator (Feature Extractor) and Classifier\n        optimizer_G.zero_grad()\n        features = model.feature_extractor(images)\n        d_fake = discriminator(features)\n        g_loss = F.binary_cross_entropy(d_fake, real_labels)\n        \n        # Classification loss\n        logits = model(images)\n        cls_loss = F.cross_entropy(logits, labels)\n        \n        # Combined loss\n        total_g_loss = cls_loss + 0.1 * g_loss\n        total_g_loss.backward()\n        optimizer_G.step()\n        \n        total_loss += total_g_loss.item()\n        \n        if batch_idx % 100 == 0:\n            print(f'Batch [{batch_idx}/{len(train_loader)}], '\n                  f'Loss: {total_g_loss.item():.4f}, '\n                  f'Class Loss: {cls_loss.item():.4f}, '\n                  f'G Loss: {g_loss.item():.4f}')\n    \n    return total_loss / len(train_loader)\n\ndef evaluate_episodes(model, dataset, num_episodes=2000, n_way=2, k_shot=1, device='cuda'):\n    \"\"\"\n    Evaluate model on n-way k-shot tasks\n    Args:\n        model: trained model\n        dataset: dataset to sample episodes from\n        num_episodes: number of episodes to evaluate\n        n_way: number of classes per episode (2 for 2-way tasks)\n        k_shot: number of support examples per class\n        device: device to run evaluation on\n    \"\"\"\n    model.eval()\n    accuracies = []\n    \n    # Get all available classes\n    all_classes = list(set(dataset.labels))\n    \n    with torch.no_grad():\n        for episode in range(num_episodes):\n            # Randomly sample n classes\n            episode_classes = random.sample(all_classes, n_way)\n            \n            # Get indices for each class\n            support_indices = []\n            query_indices = []\n            \n            for class_idx in episode_classes:\n                class_indices = [i for i, label in enumerate(dataset.labels) if label == class_idx]\n                # Sample k examples for support set\n                support = random.sample(class_indices, k_shot)\n                # Sample 1 example for query set (from remaining examples)\n                remaining = list(set(class_indices) - set(support))\n                query = random.sample(remaining, 1)\n                \n                support_indices.extend(support)\n                query_indices.extend(query)\n            \n            # Prepare support and query sets\n            support_images = torch.stack([dataset[idx][0] for idx in support_indices]).to(device)\n            support_labels = torch.tensor([dataset[idx][1] for idx in support_indices]).to(device)\n            query_images = torch.stack([dataset[idx][0] for idx in query_indices]).to(device)\n            query_labels = torch.tensor([dataset[idx][1] for idx in query_indices]).to(device)\n            \n            # Get model predictions\n            support_features = model(support_images, return_features=True)[1]  # Get pooled features\n            query_features = model(query_images, return_features=True)[1]\n            \n            # Calculate prototypes for each class\n            prototypes = {}\n            for cls in episode_classes:\n                cls_mask = support_labels == cls\n                cls_features = support_features[cls_mask]\n                prototypes[cls] = cls_features.mean(0)\n            \n            # Calculate distances to prototypes\n            accuracies_episode = []\n            for i, query_feat in enumerate(query_features):\n                distances = {cls: torch.norm(query_feat - proto) for cls, proto in prototypes.items()}\n                predicted_cls = min(distances, key=distances.get)\n                correct = (predicted_cls == query_labels[i].item())\n                accuracies_episode.append(correct)\n            \n            # Calculate accuracy for this episode\n            accuracy = sum(accuracies_episode) / len(accuracies_episode)\n            accuracies.append(accuracy)\n            \n            if (episode + 1) % 100 == 0:\n                print(f'Episode {episode + 1}/{num_episodes}, Running Avg Accuracy: {np.mean(accuracies):.4f}')\n    \n    final_accuracy = np.mean(accuracies)\n    print(f'\\nFinal Average Accuracy over {num_episodes} episodes: {final_accuracy:.4f}')\n    return final_accuracy\n\ndef main():\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Define transforms\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Create dataset and dataloader\n    dataset = HAM10000Dataset(\n        root_dir='/kaggle/input/ham10000-and-gan/synthetic_images',\n        transform=transform\n    )\n    \n    train_loader = DataLoader(\n        dataset,\n        batch_size=32,\n        shuffle=True,\n        num_workers=4\n    )\n\n    # Initialize models\n    model = MTUNetPlusPlus(num_classes=7).to(device)\n    discriminator = Discriminator(input_channels=2048).to(device)\n\n    # Initialize optimizers\n    optimizer_G = torch.optim.Adam(model.parameters(), lr=0.0001)\n    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0001)\n\n    # Training loop\n    num_epochs = 50\n    for epoch in range(num_epochs):\n        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n        avg_loss = train_epoch(\n            model, discriminator, train_loader,\n            optimizer_G, optimizer_D, device\n        )\n        print(f'Average loss: {avg_loss:.4f}')\n        \n        # Evaluate episodes after each epoch\n        print(\"\\nEvaluating 2-way tasks...\")\n        avg_accuracy = evaluate_episodes(\n            model,\n            dataset,\n            num_episodes=2000,\n            n_way=2,\n            k_shot=1,\n            device=device\n        )\n        print(f'Average accuracy over 2000 episodes: {avg_accuracy:.4f}')\n\n        # Save checkpoint\n        if (epoch + 1) % 5 == 0:\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'discriminator_state_dict': discriminator.state_dict(),\n                'optimizer_G_state_dict': optimizer_G.state_dict(),\n                'optimizer_D_state_dict': optimizer_D.state_dict(),\n                'episode_accuracy': avg_accuracy\n            }, f'checkpoint_epoch_{epoch+1}.pth')\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T08:19:46.914321Z","iopub.execute_input":"2024-12-24T08:19:46.914790Z","iopub.status.idle":"2024-12-24T08:24:42.180690Z","shell.execute_reply.started":"2024-12-24T08:19:46.914745Z","shell.execute_reply":"2024-12-24T08:24:42.179644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}