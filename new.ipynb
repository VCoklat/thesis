{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "Load HAM10000 dataset, implement data augmentation, and prepare few-shot episodes with support and query sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the dataset\n",
    "metadata = pd.read_csv('../input/ham10000/HAM10000_metadata.csv')\n",
    "image_path = '../input/ham10000/'\n",
    "\n",
    "# Function to load images\n",
    "def load_images(df, image_path):\n",
    "    images = []\n",
    "    for img_id in df['image_id']:\n",
    "        img = cv2.imread(os.path.join(image_path, f'{img_id}.jpg'))\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load images\n",
    "images = load_images(metadata, image_path)\n",
    "labels = metadata['dx'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Prepare few-shot episodes\n",
    "def create_few_shot_episodes(images, labels, n_way=5, k_shot=1, k_query=1):\n",
    "    unique_labels = np.unique(labels)\n",
    "    episodes = []\n",
    "    for _ in range(len(images) // (n_way * (k_shot + k_query))):\n",
    "        selected_labels = np.random.choice(unique_labels, n_way, replace=False)\n",
    "        support_set = []\n",
    "        query_set = []\n",
    "        for label in selected_labels:\n",
    "            label_indices = np.where(labels == label)[0]\n",
    "            selected_indices = np.random.choice(label_indices, k_shot + k_query, replace=False)\n",
    "            support_set.append(images[selected_indices[:k_shot]])\n",
    "            query_set.append(images[selected_indices[k_shot:]])\n",
    "        episodes.append((np.array(support_set), np.array(query_set)))\n",
    "    return episodes\n",
    "\n",
    "# Create few-shot episodes\n",
    "few_shot_episodes = create_few_shot_episodes(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define UNet Architecture with Attention\n",
    "Implement UNet with skip connections and self-attention mechanism for better feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Input, Activation, BatchNormalization, Add, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Attention block\n",
    "def attention_block(x, g, inter_channel):\n",
    "    theta_x = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(x)\n",
    "    phi_g = Conv2D(inter_channel, (1, 1), strides=(1, 1), padding='same')(g)\n",
    "    f = Activation('relu')(Add()([theta_x, phi_g]))\n",
    "    psi_f = Conv2D(1, (1, 1), strides=(1, 1), padding='same')(f)\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "    att_x = Multiply()([x, rate])\n",
    "    return att_x\n",
    "\n",
    "# UNet with Attention\n",
    "def unet_with_attention(input_shape=(128, 128, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(pool4)\n",
    "    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Decoder\n",
    "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
    "    att6 = attention_block(conv4, up6, 512)\n",
    "    merge6 = Concatenate()([att6, up6])\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(merge6)\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    \n",
    "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    att7 = attention_block(conv3, up7, 256)\n",
    "    merge7 = Concatenate()([att7, up7])\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(merge7)\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    \n",
    "    up8 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    att8 = attention_block(conv2, up8, 128)\n",
    "    merge8 = Concatenate()([att8, up8])\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge8)\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    \n",
    "    up9 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    att9 = attention_block(conv1, up9, 64)\n",
    "    merge9 = Concatenate()([att9, up9])\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge9)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
    "    \n",
    "    model = Model(inputs, conv10)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "unet_model = unet_with_attention()\n",
    "unet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Few-Shot Learning Components\n",
    "Create prototypical network components for few-shot learning, including support set embedding and query set comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Few-Shot Learning Components\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Prototypical Network Components\n",
    "class PrototypicalNetwork(Model):\n",
    "    def __init__(self, encoder):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def call(self, support_set, query_set):\n",
    "        # Embed the support set\n",
    "        support_embeddings = self.encoder(support_set)\n",
    "        support_embeddings = tf.reduce_mean(support_embeddings, axis=1)  # Average embeddings for each class\n",
    "\n",
    "        # Embed the query set\n",
    "        query_embeddings = self.encoder(query_set)\n",
    "\n",
    "        return support_embeddings, query_embeddings\n",
    "\n",
    "# Encoder Model\n",
    "def create_encoder(input_shape=(128, 128, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    model = Model(inputs, x)\n",
    "    return model\n",
    "\n",
    "# Create encoder and prototypical network\n",
    "encoder = create_encoder()\n",
    "proto_net = PrototypicalNetwork(encoder)\n",
    "\n",
    "# Example usage with few-shot episodes\n",
    "support_set, query_set = few_shot_episodes[0]\n",
    "support_embeddings, query_embeddings = proto_net(support_set, query_set)\n",
    "\n",
    "# Calculate distances between support and query embeddings\n",
    "def euclidean_distance(a, b):\n",
    "    return tf.sqrt(tf.reduce_sum(tf.square(a - b), axis=-1))\n",
    "\n",
    "distances = euclidean_distance(tf.expand_dims(query_embeddings, 1), tf.expand_dims(support_embeddings, 0))\n",
    "\n",
    "# Predict class based on nearest prototype\n",
    "predictions = tf.argmin(distances, axis=-1)\n",
    "\n",
    "# Print predictions\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASTGAN Implementation\n",
    "Implement FASTGAN for data augmentation to enhance few-shot learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, ReLU, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define the generator model for FASTGAN\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(8 * 8 * 256, input_dim=latent_dim))\n",
    "    model.add(Reshape((8, 8, 256)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Conv2DTranspose(32, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "\n",
    "    model.add(Conv2D(3, kernel_size=3, padding='same', activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model for FASTGAN\n",
    "def build_discriminator(input_shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Conv2D(256, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define the GAN model combining generator and discriminator\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    model = tf.keras.Sequential([generator, discriminator])\n",
    "    return model\n",
    "\n",
    "# Set parameters\n",
    "latent_dim = 100\n",
    "input_shape = (128, 128, 3)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(input_shape)\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(latent_dim)\n",
    "\n",
    "# Build and compile the GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow((predictions[i] * 127.5 + 127.5).astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "# Training the GAN\n",
    "def train_gan(generator, discriminator, gan, dataset, latent_dim, epochs=10000, batch_size=64, save_interval=200):\n",
    "    half_batch = batch_size // 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Train discriminator\n",
    "        idx = np.random.randint(0, dataset.shape[0], half_batch)\n",
    "        real_images = dataset[idx]\n",
    "        real_labels = np.ones((half_batch, 1))\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
    "        fake_images = generator.predict(noise)\n",
    "        fake_labels = np.zeros((half_batch, 1))\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % save_interval == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n",
    "            generate_and_save_images(generator, epoch, np.random.normal(0, 1, (16, latent_dim)))\n",
    "\n",
    "# Prepare the dataset for GAN training\n",
    "train_images = (train_images.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, gan, train_images, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Train the model using episodic training paradigm, combining few-shot learning with GAN-augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "# Define the episodic training function\n",
    "def train_prototypical_network(proto_net, episodes, epochs=10):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for support_set, query_set in episodes:\n",
    "            with tf.GradientTape() as tape:\n",
    "                support_embeddings, query_embeddings = proto_net(support_set, query_set)\n",
    "                distances = euclidean_distance(tf.expand_dims(query_embeddings, 1), tf.expand_dims(support_embeddings, 0))\n",
    "                predictions = tf.argmin(distances, axis=-1)\n",
    "                loss = loss_fn(tf.range(len(predictions)), predictions)\n",
    "            gradients = tape.gradient(loss, proto_net.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, proto_net.trainable_variables))\n",
    "            epoch_loss += loss\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss / len(episodes)}')\n",
    "\n",
    "# Train the Prototypical Network\n",
    "train_prototypical_network(proto_net, few_shot_episodes)\n",
    "\n",
    "# Integrate GAN-augmented data into training\n",
    "def augment_with_gan(generator, support_set, k_shot, latent_dim):\n",
    "    noise = np.random.normal(0, 1, (k_shot, latent_dim))\n",
    "    generated_images = generator.predict(noise)\n",
    "    return np.concatenate([support_set, generated_images], axis=0)\n",
    "\n",
    "# Augment support sets with GAN-generated images\n",
    "augmented_episodes = []\n",
    "for support_set, query_set in few_shot_episodes:\n",
    "    augmented_support_set = augment_with_gan(generator, support_set, k_shot=1, latent_dim=latent_dim)\n",
    "    augmented_episodes.append((augmented_support_set, query_set))\n",
    "\n",
    "# Train the Prototypical Network with augmented data\n",
    "train_prototypical_network(proto_net, augmented_episodes)\n",
    "\n",
    "# Explainability using saliency maps\n",
    "def compute_saliency_maps(model, images, labels):\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        predictions = model(images)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "    gradients = tape.gradient(loss, images)\n",
    "    saliency_maps = tf.reduce_max(tf.abs(gradients), axis=-1)\n",
    "    return saliency_maps\n",
    "\n",
    "# Compute saliency maps for validation images\n",
    "val_images_tensor = tf.convert_to_tensor(val_images)\n",
    "val_labels_tensor = tf.convert_to_tensor(val_labels)\n",
    "saliency_maps = compute_saliency_maps(proto_net.encoder, val_images_tensor, val_labels_tensor)\n",
    "\n",
    "# Display saliency maps\n",
    "def display_saliency_maps(images, saliency_maps):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(images[0].astype(np.uint8))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(saliency_maps[0], cmap='hot')\n",
    "    axes[1].set_title('Saliency Map')\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display saliency map for a sample validation image\n",
    "display_saliency_maps(val_images, saliency_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency Maps and Explainability\n",
    "Generate saliency maps using gradient-based methods to visualize model decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Explainability using saliency maps\n",
    "def compute_saliency_maps(model, images, labels):\n",
    "    images = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(images)\n",
    "        predictions = model(images)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)\n",
    "    gradients = tape.gradient(loss, images)\n",
    "    saliency_maps = tf.reduce_max(tf.abs(gradients), axis=-1)\n",
    "    return saliency_maps\n",
    "\n",
    "# Compute saliency maps for validation images\n",
    "val_images_tensor = tf.convert_to_tensor(val_images, dtype=tf.float32)\n",
    "val_labels_tensor = tf.convert_to_tensor(val_labels, dtype=tf.int64)\n",
    "saliency_maps = compute_saliency_maps(proto_net.encoder, val_images_tensor, val_labels_tensor)\n",
    "\n",
    "# Display saliency maps\n",
    "def display_saliency_maps(images, saliency_maps):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(images[0].astype(np.uint8))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(saliency_maps[0], cmap='hot')\n",
    "    axes[1].set_title('Saliency Map')\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display saliency map for a sample validation image\n",
    "display_saliency_maps(val_images, saliency_maps)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
